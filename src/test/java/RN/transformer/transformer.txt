package RN.transformer;

import java.util.Arrays;
import java.util.List;

import org.junit.Assert;
import org.junit.Test;

public class BatchTest {
    
    @Test
    public void testDataAndTargetAssignment() {
        List<String> data = Arrays.asList("data1", "data2");
        List<String> target = Arrays.asList("target1", "target2");
        Batch batch = new Batch(data, target);
        
        Assert.assertEquals("Data should match input list", data, batch.getData());
        Assert.assertEquals("Target should match input list", target, batch.getTarget());
    }
}
package RN.transformer;

import java.util.ArrayList;
import java.util.List;
import java.util.Random;

import org.junit.Assert;
import org.junit.Before;
import org.junit.Test;
import org.nd4j.linalg.api.buffer.DataType;
import org.nd4j.linalg.api.ndarray.INDArray;
import org.nd4j.linalg.factory.Nd4j;

public class CustomAdamOptimizerTest {

    private CustomAdamOptimizer optimizer;
    private List<INDArray> parameters;
    private List<INDArray> gradients;
    private final float initialLr = 0.001f;
    private final int warmupSteps = 1000;
    private final int dModel = 512;
    

    @Before
    public void setUp() {

        
        // Ensuite, initialiser 'parameters' avec les valeurs de test
        parameters = new ArrayList<>();
        parameters.add(Nd4j.create(new float[]{0.1f, -0.2f}, new int[]{1, 2}));
        gradients = new ArrayList<>();
        gradients.add(Nd4j.create(new float[]{0.01f, -0.01f}, new int[]{1, 2}));
        
        optimizer = new CustomAdamOptimizer(initialLr, dModel, warmupSteps, parameters);

    }


    @Test
    public void testUpdate() {
        INDArray originalParams = parameters.get(0).dup();
        optimizer.update(parameters, gradients);
        INDArray updatedParams = parameters.get(0);

        Assert.assertTrue("Parameters should be updated after optimizer update call",
                !originalParams.equalsWithEps(updatedParams, 1e-7)); // Utiliser equalsWithEps pour une comparaison avec une tolérance

    }

    @Test
    public void testLearningRateAdjustment() {
        // Simulate a few steps to trigger learning rate adjustments
        for (int i = 0; i < warmupSteps / 2; i++) {
            optimizer.update(parameters, gradients);
        }

        double learningRateMidway = optimizer.getLearningRate();
        
        // Verify that learning rate during warmup is less than initial and decreases with steps
        Assert.assertTrue("Learning rate should increase during warmup",
                          learningRateMidway > 0 && learningRateMidway <= initialLr);

        // Simulate more steps to complete warmup
        for (int i = warmupSteps / 2; i < warmupSteps * 2; i++) {
            optimizer.update(parameters, gradients);
        }

        float learningRateAfterWarmup = optimizer.getLearningRate();

        // Verify learning rate after warmup is less than during warmup
        Assert.assertTrue("Learning rate should decrease after warmup",
                          learningRateAfterWarmup < initialLr);
    }
    
    @Test
    public void testLearningRateSetter() {
        float newLearningRate = 0.0001f;
        optimizer.setLearningRate(newLearningRate);
        Assert.assertEquals("Learning rate setter should update the learning rate",
                            newLearningRate, optimizer.getLearningRate(), 0.0);
    }
}
package RN.transformer;

import java.io.IOException;
import java.nio.file.Files;
import java.nio.file.Path;
import java.util.List;

import org.junit.After;
import org.junit.Assert;
import org.junit.Before;
import org.junit.Test;
import org.mockito.Mockito;

public class DataGeneratorTest {

    private Path dataFile;
    private Path targetFile;
    private DataGenerator generator;
    private Tokenizer tokenizer;

    @Before
    public void setUp() throws IOException {
        // Create temporary files for data and target
        dataFile = Files.createTempFile("testData", ".txt");
        targetFile = Files.createTempFile("testTarget", ".txt");
        
        // Write some dummy data to files
        Files.write(dataFile, "data sample".getBytes());
        Files.write(targetFile, "target sample".getBytes());
        
        // Mocking the tokenizer
        tokenizer = Mockito.mock(Tokenizer.class);
        int maxTokenPerBatch = 13;
        Mockito.when(tokenizer.tokenize("data sample")).thenReturn(List.of("data", "sample"));
        Mockito.when(tokenizer.tokenize("target sample")).thenReturn(List.of("target", "sample"));

        // Initialize DataGenerator
        generator = new DataGenerator(dataFile.toString(), targetFile.toString(), tokenizer, 1, maxTokenPerBatch);
    }

    @Test
    public void testNextBatch() throws IOException {
        Batch batch = generator.nextBatch();

        Assert.assertNotNull("Batch should not be null", batch);
        Assert.assertFalse("Data batch should not be empty", batch.getData().isEmpty());
        Assert.assertFalse("Target batch should not be empty", batch.getTarget().isEmpty());
        Assert.assertEquals("Data batch should contain expected tokens", "data sample", batch.getData().get(0));
        Assert.assertEquals("Target batch should contain expected tokens", "target sample", batch.getTarget().get(0));
    }

    @After
    public void tearDown() throws IOException {
        Files.deleteIfExists(dataFile);
        Files.deleteIfExists(targetFile);
    }
}
package RN.transformer;

import java.io.IOException;
import java.util.Arrays;
import java.util.List;

class DummyDataGenerator extends DataGenerator {
    
    private int maxBatchesPerEpoch;
    private int currentBatch;

    /**
     * Constructeur de DummyDataGenerator avec un nombre limité de batches par epoch.
     *
     * @param dataPath         Chemin vers les données (non utilisé dans ce dummy).
     * @param targetPath       Chemin vers les cibles (non utilisé dans ce dummy).
     * @param tokenizer        Instance du tokenizer.
     * @param batchSize        Taille du batch.
     * @param maxTokensPerBatch Nombre maximum de tokens par batch.
     * @param maxBatchesPerEpoch Nombre maximum de batches par epoch.
     * @throws IOException Si une erreur d'entrée/sortie survient.
     */
    public DummyDataGenerator(String dataPath, String targetPath, Tokenizer tokenizer, int batchSize, int maxTokensPerBatch, int maxBatchesPerEpoch) throws IOException {
        super(dataPath, targetPath, tokenizer, batchSize, maxTokensPerBatch);
        this.maxBatchesPerEpoch = maxBatchesPerEpoch;
        this.currentBatch = 0;
    }

    @Override
    public boolean hasNextBatch() {
        return currentBatch < maxBatchesPerEpoch;
    }

    @Override
    public Batch nextBatch() {
        currentBatch++;
        List<String> dummyData = Arrays.asList("This is a dummy sentence.");
        List<String> dummyTarget = Arrays.asList("Ceci est une phrase fictive.");
        return new Batch(dummyData, dummyTarget);
    }

    @Override
    public void init() {
        currentBatch = 0;
    }
}
package RN.transformer;

import static org.junit.jupiter.api.Assertions.*;
import static org.junit.jupiter.api.Assertions.assertEquals;
import static org.junit.jupiter.api.Assertions.assertFalse;
import static org.junit.jupiter.api.Assertions.assertNotNull;
import static org.junit.jupiter.api.Assertions.assertTrue;
import org.junit.jupiter.api.Test;
import org.nd4j.linalg.api.ndarray.INDArray;
import org.nd4j.linalg.factory.Nd4j;
import java.util.Map;

public class LayerNormTest {

    @Test
    public void testLayerNormBackward() {
        int batchSize = 1;
        int seqLength = 2;
        int dModel = 3;
    
        // Création d'un LayerNorm
        LayerNorm layerNorm = new LayerNorm(dModel);
    
        // Entrée fictive avec trois dimensions [batchSize, seqLength, dModel]
        INDArray input = Nd4j.create(new float[][][]{
            {
                {1.0f, 2.0f, 3.0f},
                {4.0f, 5.0f, 6.0f}
            }
        });
    
        // Passe forward
        INDArray output = layerNorm.forward(input);
    
        // Création d'un gradOutput fictif avec trois dimensions [batchSize, seqLength, dModel]
        INDArray gradOutput = Nd4j.create(new float[][][]{
            {
                {0.1f, 0.2f, 0.3f},
                {0.4f, 0.5f, 0.6f}
            }
        });
    
        // Passe backward
        Map<String, INDArray> gradients = layerNorm.backward(gradOutput);
    
        // Assertions
        assertNotNull(gradients, "Les gradients ne devraient pas être null");
        assertFalse(gradients.isEmpty(), "Le map des gradients ne devrait pas être vide");
        assertTrue(gradients.containsKey("gamma"), "Les gradients devraient contenir la clé 'gamma'");
        assertTrue(gradients.containsKey("beta"), "Les gradients devraient contenir la clé 'beta'");
        assertTrue(gradients.containsKey("input"), "Les gradients devraient contenir la clé 'input'");
    
        // Vérifier les formes des gradients
        INDArray gradGamma = gradients.get("gamma");
        INDArray gradBeta = gradients.get("beta");
        INDArray gradInput = gradients.get("input");
    
        assertEquals(dModel, gradGamma.length(), "La longueur de gradGamma devrait être égale à dModel");
        assertEquals(dModel, gradBeta.length(), "La longueur de gradBeta devrait être égale à dModel");
        assertEquals(batchSize, gradInput.size(0), "Le batchSize de gradInput devrait correspondre");
        assertEquals(seqLength, gradInput.size(1), "Le nombre de séquences de gradInput devrait correspondre");
        assertEquals(dModel, gradInput.size(2), "Le nombre de dimensions de gradInput devrait correspondre");
    
        // Optionnel : Vérifier que les gradients ne contiennent pas de NaN ou d'Inf
        assertFalse(gradGamma.isNaN().any(), "gradGamma ne devrait pas contenir de NaN");
        assertFalse(gradGamma.isInfinite().any(), "gradGamma ne devrait pas contenir d'Inf");
        assertFalse(gradBeta.isNaN().any(), "gradBeta ne devrait pas contenir de NaN");
        assertFalse(gradBeta.isInfinite().any(), "gradBeta ne devrait pas contenir d'Inf");
        assertFalse(gradInput.isNaN().any(), "gradInput ne devrait pas contenir de NaN");
        assertFalse(gradInput.isInfinite().any(), "gradInput ne devrait pas contenir d'Inf");
    }
    


}
package RN.transformer;

import org.junit.Assert;
import org.junit.Before;
import org.junit.Test;
import org.nd4j.linalg.api.buffer.DataType;
import org.nd4j.linalg.api.ndarray.INDArray;
import org.nd4j.linalg.factory.Nd4j;

import java.util.List;
import java.util.Map;

/**
 * Classe de test unitaire pour la classe LinearProjection.
 * Elle vérifie le bon fonctionnement des méthodes project, forward, backward,
 * ainsi que les accesseurs et mutateurs des paramètres et gradients.
 */
public class LinearProjectionTest {

    private LinearProjection lp;

    /**
     * Méthode exécutée avant chaque test.
     * Initialise une instance de LinearProjection avec des dimensions spécifiques.
     */
    @Before
    public void setUp() {
        // Initialisation de LinearProjection avec inputSize=10 et outputSize=5.
        lp = new LinearProjection(10, 5);
    }

    /**
     * Test de la méthode project avec une entrée de rang 2.
     * Vérifie que la sortie a la forme correcte [batchSize, outputSize].
     */
    @Test
    public void testProjectRank2() {
        // Créer une entrée de rang 2 [batchSize=2, inputSize=10]
        INDArray input = Nd4j.rand(DataType.FLOAT, 2, 10);

        // Effectuer la projection
        INDArray output = lp.project(input);

        // Vérifier la forme de la sortie : [2, 5]
        Assert.assertArrayEquals("Output shape should be [2, 5]", new long[]{2, 5}, output.shape());

        // Vérifier le type de données
        Assert.assertEquals("Output data type should be FLOAT", DataType.FLOAT, output.dataType());
    }

    /**
     * Test de la méthode project avec une entrée de rang 3.
     * Vérifie que la sortie a la forme correcte [batchSize, seqLength, outputSize].
     */
    @Test
    public void testProjectRank3() {
        // Créer une entrée de rang 3 [batchSize=3, seqLength=4, inputSize=10]
        INDArray input = Nd4j.rand(DataType.FLOAT, 3, 4, 10);

        // Effectuer la projection
        INDArray output = lp.project(input);

        // Vérifier la forme de la sortie : [3, 4, 5]
        Assert.assertArrayEquals("Output shape should be [3, 4, 5]", new long[]{3, 4, 5}, output.shape());

        // Vérifier le type de données
        Assert.assertEquals("Output data type should be FLOAT", DataType.FLOAT, output.dataType());
    }

    /**
     * Test de la méthode forward.
     * Vérifie que la sortie a la forme correcte et que LayerNorm est appliqué correctement.
     */
    @Test
    public void testForward() {
        // Créer une entrée de rang 2 [batchSize=2, inputSize=10]
        INDArray input = Nd4j.rand(DataType.FLOAT, 2, 10);

        // Effectuer le forward pass
        INDArray output = lp.forward(input);

        // Vérifier la forme de la sortie : [2, 5]
        Assert.assertArrayEquals("Output shape should be [2, 5]", new long[]{2, 5}, output.shape());

        // Vérifier le type de données
        Assert.assertEquals("Output data type should be FLOAT", DataType.FLOAT, output.dataType());

        // Optionnel: Vérifier les valeurs si possible, par exemple vérifier que output n'est pas NaN
        // Assert.assertFalse("Output should not contain NaN", output.isNaN());
        // Assert.assertFalse("Output should not contain Inf", output.isInfinite());
    }

    /**
     * Test de la méthode backward.
     * Vérifie que tous les gradients sont calculés et ont les bonnes formes.
     */
    @Test
    public void testBackward() {
        // Créer une entrée de rang 2 [batchSize=2, inputSize=10]
        INDArray input = Nd4j.rand(DataType.FLOAT, 2, 10);

        // Créer un gradOutput de rang 2 [batchSize=2, outputSize=5]
        INDArray gradOutput = Nd4j.rand(DataType.FLOAT, 2, 5);

        // Effectuer le backward pass
        Map<String, INDArray> grads = lp.backward(input, gradOutput);

        // Vérifier que tous les gradients sont présents
        Assert.assertTrue("Gradients should contain 'weights'", grads.containsKey("weights"));
        Assert.assertTrue("Gradients should contain 'bias'", grads.containsKey("bias"));
        Assert.assertTrue("Gradients should contain 'gamma'", grads.containsKey("gamma"));
        Assert.assertTrue("Gradients should contain 'beta'", grads.containsKey("beta"));
        Assert.assertTrue("Gradients should contain 'input'", grads.containsKey("input"));

        // Vérifier le nombre de gradients
        Assert.assertEquals("Should have 5 gradients", 5, grads.size());

        // Vérifier les formes des gradients
        Assert.assertArrayEquals("weights gradient shape should be [10,5]", new long[]{10, 5}, grads.get("weights").shape());
        Assert.assertArrayEquals("bias gradient shape should be [1,5]", new long[]{1, 5}, grads.get("bias").shape());
        Assert.assertArrayEquals("gamma gradient shape should be [1,10]", new long[]{1, 10}, grads.get("gamma").shape());
        Assert.assertArrayEquals("beta gradient shape should be [1,10]", new long[]{1, 10}, grads.get("beta").shape());
        Assert.assertArrayEquals("input gradient shape should be [2,10]", new long[]{2, 10}, grads.get("input").shape());

        // Optionnel: Vérifier que les gradients ne contiennent pas de NaN ou d'Inf
        // for (Map.Entry<String, INDArray> entry : grads.entrySet()) {
        //     Assert.assertFalse("Gradient " + entry.getKey() + " should not contain NaN", entry.getValue().isNaN());
        //     Assert.assertFalse("Gradient " + entry.getKey() + " should not contain Inf", entry.getValue().isInfinite());
        // }
    }

    /**
     * Test des accesseurs des paramètres.
     * Vérifie que les paramètres retournés sont corrects et ont les bonnes formes.
     */
    @Test
    public void testGetParameters() {
        // Obtenir les paramètres
        List<INDArray> params = lp.getParameters();

        // Vérifier qu'il y a 4 paramètres : weights, bias, gamma, beta
        Assert.assertEquals("Should have 4 parameters", 4, params.size());

        // Vérifier les formes des paramètres
        INDArray weights = params.get(0);
        INDArray bias = params.get(1);
        INDArray gamma = params.get(2);
        INDArray beta = params.get(3);

        Assert.assertArrayEquals("weights shape should be [10, 5]", new long[]{10, 5}, weights.shape());
        Assert.assertArrayEquals("bias shape should be [1, 5]", new long[]{1, 5}, bias.shape());
        Assert.assertArrayEquals("gamma shape should be [1, 10]", new long[]{1, 10}, gamma.shape());
        Assert.assertArrayEquals("beta shape should be [1, 10]", new long[]{1, 10}, beta.shape());
    }

    /**
     * Test des mutateurs des paramètres.
     * Vérifie que les paramètres sont correctement mis à jour.
     */
    @Test
    public void testSetParameters() {
        // Créer de nouvelles valeurs pour les paramètres
        INDArray newWeights = Nd4j.ones(DataType.FLOAT, 10, 5);
        INDArray newBias = Nd4j.ones(DataType.FLOAT, 1, 5);
        INDArray newGamma = Nd4j.ones(DataType.FLOAT, 1, 10);
        INDArray newBeta = Nd4j.ones(DataType.FLOAT, 1, 10);

        // Mettre à jour les paramètres
        lp.setParameters(newWeights, newBias, newGamma, newBeta);

        // Obtenir les paramètres mis à jour
        List<INDArray> params = lp.getParameters();

        // Vérifier que les paramètres ont été mis à jour
        Assert.assertEquals("weights should be all ones", Nd4j.ones(DataType.FLOAT, 10, 5), params.get(0));
        Assert.assertEquals("bias should be all ones", Nd4j.ones(DataType.FLOAT, 1, 5), params.get(1));
        Assert.assertEquals("gamma should be all ones", Nd4j.ones(DataType.FLOAT, 1, 10), params.get(2));
        Assert.assertEquals("beta should be all ones", Nd4j.ones(DataType.FLOAT, 1, 10), params.get(3));
    }

    /**
     * Test du nombre total de paramètres.
     * Vérifie que le nombre de paramètres correspond à ce qui est attendu.
     */
    @Test
    public void testNumberOfParameters() {
        // Nombre attendu de paramètres : weights + bias + gamma + beta
        long expected = (10 * 5) + (1 * 5) + (1 * 10) + (1 * 10); // 50 + 5 + 10 + 10 = 75

        Assert.assertEquals("Number of parameters should be 75", 75, lp.getNumberOfParameters());
    }

    /**
     * Test du nombre total de gradients.
     * Vérifie que le nombre de gradients correspond à ce qui est attendu.
     */
    @Test
    public void testNumberOfGradients() {
        // Créer une entrée et un gradOutput pour effectuer le backward
        INDArray input = Nd4j.rand(DataType.FLOAT, 2, 10);
        INDArray gradOutput = Nd4j.rand(DataType.FLOAT, 2, 5);

        // Effectuer le backward pass
        Map<String, INDArray> grads = lp.backward(input, gradOutput);

        // Nombre attendu de gradients : weights + bias + gamma + beta + input
        long expected = 5;

        // Vérifier le nombre de gradients
        Assert.assertEquals("Should have 5 gradients", expected, grads.size());
    }

    /**
     * Test de la méthode getGradients.
     * Vérifie que les gradients retournés sont corrects et ont les bonnes formes.
     */
    @Test
    public void testGetGradients() {
        // Créer une entrée et un gradOutput pour effectuer le backward
        INDArray input = Nd4j.rand(DataType.FLOAT, 2, 10);
        INDArray gradOutput = Nd4j.rand(DataType.FLOAT, 2, 5);

        // Effectuer le backward pass
        Map<String, INDArray> grads = lp.backward(input, gradOutput);

        // Obtenir les gradients
        List<INDArray> gradList = lp.getGradients();

        // Vérifier que gradList contient les gradients dans l'ordre [weights, bias, gamma, beta]
        Assert.assertEquals("Should have 4 gradients in getGradients", 4, gradList.size());

        // Vérifier les formes
        Assert.assertArrayEquals("weights gradient shape should be [10,5]", new long[]{10, 5}, gradList.get(0).shape());
        Assert.assertArrayEquals("bias gradient shape should be [1,5]", new long[]{1, 5}, gradList.get(1).shape());
        Assert.assertArrayEquals("gamma gradient shape should be [1,10]", new long[]{1, 10}, gradList.get(2).shape());
        Assert.assertArrayEquals("beta gradient shape should be [1,10]", new long[]{1, 10}, gradList.get(3).shape());

        // Optionnel: Vérifier que les gradients ne contiennent pas de NaN ou d'Inf
        // for (INDArray grad : gradList) {
        //     Assert.assertFalse("Gradient should not contain NaN", grad.isNaN());
        //     Assert.assertFalse("Gradient should not contain Inf", grad.isInfinite());
        // }
    }
}
package RN.transformer;

import static org.junit.jupiter.api.Assertions.*;

import org.junit.jupiter.api.BeforeAll;
import org.junit.jupiter.api.Test;
import org.nd4j.linalg.api.buffer.DataType;
import org.nd4j.linalg.api.ndarray.INDArray;
import org.nd4j.linalg.factory.Nd4j;
import java.util.Map;
import java.util.List;
import java.util.Arrays;
import org.apache.commons.lang3.tuple.Pair;

public class MultiHeadAttentionTest {

    private static TransformerModel transformerModel;
    private static int dModel;
    private static int numHeads;
    private static int vocabSize;

    @BeforeAll
    public static void setup() {
        // Initialisation des paramètres
        dModel = 300;
        numHeads = 6;
        vocabSize = 10000; // Exemple de taille de vocabulaire

        // Initialisation des embeddings pré-entraînés
        TransformerModel.initializeEmbeddings(vocabSize, dModel);
    }

    @Test
    public void backward0Test() {
        int depth = dModel / numHeads;
        int seqLength = 1;
        int batchSize = 1;

        // Création d'une instance de MultiHeadAttention
        MultiHeadAttention mha = new MultiHeadAttention(dModel, numHeads);

        // Entrées fictives
        // Création d'une entrée fictive [batchSize=1, seqLength=1, dModel=300]
        INDArray input = Nd4j.rand(DataType.FLOAT, batchSize, seqLength, dModel);

        // Masque fictif (optionnel)
        INDArray mask = null; // Ou créez un masque de forme [batchSize, 1, 1, seqLength] si nécessaire

        // Passe forward
        INDArray output = mha.forward(input, input, input, mask);

        // Vérification des dimensions de la sortie
        assertEquals(3, output.rank(), "Le tenseur devrait être d'ordre 3");
        assertArrayEquals(new long[]{batchSize, seqLength, dModel}, output.shape(), "La forme de la sortie est incorrecte");

        // Création d'un gradOutput fictif
        INDArray gradOutput = Nd4j.rand(DataType.FLOAT, batchSize, seqLength, dModel);

        // Passe backward
        Map<String, INDArray> gradients = mha.backward(gradOutput);

        // Assertions pour vérifier les gradients
        assertNotNull(gradients, "Les gradients ne devraient pas être null");
        assertFalse(gradients.isEmpty(), "Le map des gradients ne devrait pas être vide");
        assertTrue(gradients.containsKey("Wq"), "Les gradients devraient contenir la clé 'Wq'");
        assertTrue(gradients.containsKey("Wk"), "Les gradients devraient contenir la clé 'Wk'");
        assertTrue(gradients.containsKey("Wv"), "Les gradients devraient contenir la clé 'Wv'");
        assertTrue(gradients.containsKey("Wo"), "Les gradients devraient contenir la clé 'Wo'");
        assertTrue(gradients.containsKey("input"), "Les gradients devraient contenir la clé 'input'");

        // Vérifier les formes des gradients
        INDArray gradWq = gradients.get("Wq");
        INDArray gradWk = gradients.get("Wk");
        INDArray gradWv = gradients.get("Wv");
        INDArray gradWo = gradients.get("Wo");
        INDArray gradInput = gradients.get("input");

        assertEquals(dModel, gradWq.rows(), "Le nombre de lignes de gradWq devrait correspondre à dModel");
        assertEquals(numHeads * depth, gradWq.columns(), "Le nombre de colonnes de gradWq devrait correspondre à numHeads * depth");

        assertEquals(dModel, gradWk.rows(), "Le nombre de lignes de gradWk devrait correspondre à dModel");
        assertEquals(numHeads * depth, gradWk.columns(), "Le nombre de colonnes de gradWk devrait correspondre à numHeads * depth");

        assertEquals(numHeads * depth, gradWv.rows(), "Le nombre de lignes de gradWv devrait correspondre à numHeads * depth");
        assertEquals(dModel, gradWv.columns(), "Le nombre de colonnes de gradWv devrait correspondre à dModel");

        assertEquals(numHeads * depth, gradWo.rows(), "Le nombre de lignes de gradWo devrait correspondre à numHeads * depth");
        assertEquals(dModel, gradWo.columns(), "Le nombre de colonnes de gradWo devrait correspondre à dModel");

        // Vérifier les gradients des entrées
        assertEquals(batchSize, gradInput.shape()[0], "Le nombre de batch de gradInput est incorrect");
        assertEquals(seqLength, gradInput.shape()[1], "La longueur de séquence de gradInput est incorrecte");
        assertEquals(dModel, gradInput.shape()[2], "La dimension du modèle de gradInput est incorrecte");

        // Optionnel : Vérifier que les gradients ne contiennent pas de NaN ou d'Inf
        assertFalse(gradWq.isNaN().any(), "gradWq ne devrait pas contenir de NaN");
        assertFalse(gradWq.isInfinite().any(), "gradWq ne devrait pas contenir d'Inf");
        assertFalse(gradWk.isNaN().any(), "gradWk ne devrait pas contenir de NaN");
        assertFalse(gradWk.isInfinite().any(), "gradWk ne devrait pas contenir d'Inf");
        assertFalse(gradWv.isNaN().any(), "gradWv ne devrait pas contenir de NaN");
        assertFalse(gradWv.isInfinite().any(), "gradWv ne devrait pas contenir d'Inf");
        assertFalse(gradWo.isNaN().any(), "gradWo ne devrait pas contenir de NaN");
        assertFalse(gradWo.isInfinite().any(), "gradWo ne devrait pas contenir d'Inf");
        assertFalse(gradInput.isNaN().any(), "gradInput ne devrait pas contenir de NaN");
        assertFalse(gradInput.isInfinite().any(), "gradInput ne devrait pas contenir d'Inf");
    }

    @Test
    public void forwardTest(){
        int depth = dModel / numHeads;
        int seqLength = 1;
        int batchSize = 1;

        MultiHeadAttention mha = new MultiHeadAttention(dModel, numHeads);

        // Création d'une entrée fictive [batchSize=1, seqLength=1, dModel=300]
        INDArray input = Nd4j.rand(DataType.FLOAT, batchSize, seqLength, dModel);
        System.out.println("Input shape: " + Arrays.toString(input.shape()));

        // Forward pass
        INDArray output = mha.forward(input, input, input, null);
        System.out.println("Output shape: " + Arrays.toString(output.shape())); // Devrait être [1, 1, 300]

        // Vérifier les dimensions de la sortie
        assertEquals(3, output.rank(), "Le tenseur devrait être d'ordre 3");
        assertArrayEquals(new long[]{batchSize, seqLength, dModel}, output.shape(), "La forme de la sortie est incorrecte");
    }

    @Test
    public void backwardTest(){
        int depth = dModel / numHeads;
        int seqLength = 2;
        int batchSize = 1;

        MultiHeadAttention mha = new MultiHeadAttention(dModel, numHeads);

        // Create a dummy input [batchSize=1, seqLength=2, dModel=300]
        INDArray input = Nd4j.rand(DataType.FLOAT, batchSize, seqLength, dModel);
        System.out.println("Input shape: " + Arrays.toString(input.shape()));

        // Forward pass
        INDArray output = mha.forward(input, input, input, null);
        System.out.println("Output shape: " + Arrays.toString(output.shape())); // [1, 2, 300]

        // Simulate a gradient of output
        INDArray gradOutput = Nd4j.ones(DataType.FLOAT, batchSize, seqLength, dModel);
        System.out.println("Grad Output shape: " + Arrays.toString(gradOutput.shape()));

        // Backward pass
        Map<String, INDArray> gradients = mha.backward(gradOutput);
        for (Map.Entry<String, INDArray> entry : gradients.entrySet()) {
            System.out.println("Gradient for " + entry.getKey() + ": " + Arrays.toString(entry.getValue().shape()));
        }

        // Assertions pour vérifier les gradients
        assertNotNull(gradients, "Les gradients ne devraient pas être null");
        assertFalse(gradients.isEmpty(), "Le map des gradients ne devrait pas être vide");
        assertTrue(gradients.containsKey("Wq"), "Les gradients devraient contenir la clé 'Wq'");
        assertTrue(gradients.containsKey("Wk"), "Les gradients devraient contenir la clé 'Wk'");
        assertTrue(gradients.containsKey("Wv"), "Les gradients devraient contenir la clé 'Wv'");
        assertTrue(gradients.containsKey("Wo"), "Les gradients devraient contenir la clé 'Wo'");
        assertTrue(gradients.containsKey("input"), "Les gradients devraient contenir la clé 'input'");

        // Vérifier les formes des gradients
        INDArray gradWq = gradients.get("Wq");
        INDArray gradWk = gradients.get("Wk");
        INDArray gradWv = gradients.get("Wv");
        INDArray gradWo = gradients.get("Wo");
        INDArray gradInput = gradients.get("input");

        assertEquals(dModel, gradWq.rows(), "Le nombre de lignes de gradWq devrait correspondre à dModel");
        assertEquals(numHeads * depth, gradWq.columns(), "Le nombre de colonnes de gradWq devrait correspondre à numHeads * depth");

        assertEquals(dModel, gradWk.rows(), "Le nombre de lignes de gradWk devrait correspondre à dModel");
        assertEquals(numHeads * depth, gradWk.columns(), "Le nombre de colonnes de gradWk devrait correspondre à numHeads * depth");

        assertEquals(numHeads * depth, gradWv.rows(), "Le nombre de lignes de gradWv devrait correspondre à numHeads * depth");
        assertEquals(dModel, gradWv.columns(), "Le nombre de colonnes de gradWv devrait correspondre à dModel");

        assertEquals(numHeads * depth, gradWo.rows(), "Le nombre de lignes de gradWo devrait correspondre à numHeads * depth");
        assertEquals(dModel, gradWo.columns(), "Le nombre de colonnes de gradWo devrait correspondre à dModel");

        // Vérifier les gradients des entrées
        assertEquals(batchSize, gradInput.shape()[0], "Le nombre de batch de gradInput est incorrect");
        assertEquals(seqLength, gradInput.shape()[1], "La longueur de séquence de gradInput est incorrecte");
        assertEquals(dModel, gradInput.shape()[2], "La dimension du modèle de gradInput est incorrecte");

        // Optionnel : Vérifier que les gradients ne contiennent pas de NaN ou d'Inf
        assertFalse(gradWq.isNaN().any(), "gradWq ne devrait pas contenir de NaN");
        assertFalse(gradWq.isInfinite().any(), "gradWq ne devrait pas contenir d'Inf");
        assertFalse(gradWk.isNaN().any(), "gradWk ne devrait pas contenir de NaN");
        assertFalse(gradWk.isInfinite().any(), "gradWk ne devrait pas contenir d'Inf");
        assertFalse(gradWv.isNaN().any(), "gradWv ne devrait pas contenir de NaN");
        assertFalse(gradWv.isInfinite().any(), "gradWv ne devrait pas contenir d'Inf");
        assertFalse(gradWo.isNaN().any(), "gradWo ne devrait pas contenir de NaN");
        assertFalse(gradWo.isInfinite().any(), "gradWo ne devrait pas contenir d'Inf");
        assertFalse(gradInput.isNaN().any(), "gradInput ne devrait pas contenir de NaN");
        assertFalse(gradInput.isInfinite().any(), "gradInput ne devrait pas contenir d'Inf");
    }

    @Test
    public void multiBatchTest(){
        int depth = dModel / numHeads;
        int seqLength = 3;
        int batchSize = 2;

        MultiHeadAttention mha = new MultiHeadAttention(dModel, numHeads);

        // Création d'une entrée fictive [batchSize=2, seqLength=3, dModel=300]
        INDArray input = Nd4j.rand(DataType.FLOAT, batchSize, seqLength, dModel);
        System.out.println("Input shape: " + Arrays.toString(input.shape()));

        // Forward pass
        INDArray output = mha.forward(input, input, input, null);
        System.out.println("Output shape: " + Arrays.toString(output.shape())); // Devrait être [2, 3, 300]

        // Vérifier les dimensions de la sortie
        assertEquals(3, output.rank(), "Le tenseur devrait être d'ordre 3");
        assertArrayEquals(new long[]{batchSize, seqLength, dModel}, output.shape(), "La forme de la sortie est incorrecte");

        // Création d'un gradOutput fictif [batchSize=2, seqLength=3, dModel=300]
        INDArray gradOutput = Nd4j.rand(DataType.FLOAT, batchSize, seqLength, dModel);

        // Passe backward
        Map<String, INDArray> gradients = mha.backward(gradOutput);

        // Assertions pour vérifier les gradients
        assertNotNull(gradients, "Les gradients ne devraient pas être null");
        assertFalse(gradients.isEmpty(), "Le map des gradients ne devrait pas être vide");
        assertTrue(gradients.containsKey("Wq"), "Les gradients devraient contenir la clé 'Wq'");
        assertTrue(gradients.containsKey("Wk"), "Les gradients devraient contenir la clé 'Wk'");
        assertTrue(gradients.containsKey("Wv"), "Les gradients devraient contenir la clé 'Wv'");
        assertTrue(gradients.containsKey("Wo"), "Les gradients devraient contenir la clé 'Wo'");
        assertTrue(gradients.containsKey("input"), "Les gradients devraient contenir la clé 'input'");

        // Vérifier les formes des gradients
        INDArray gradWq = gradients.get("Wq");
        INDArray gradWk = gradients.get("Wk");
        INDArray gradWv = gradients.get("Wv");
        INDArray gradWo = gradients.get("Wo");
        INDArray gradInput = gradients.get("input");

        assertEquals(dModel, gradWq.rows(), "Le nombre de lignes de gradWq devrait correspondre à dModel");
        assertEquals(numHeads * depth, gradWq.columns(), "Le nombre de colonnes de gradWq devrait correspondre à numHeads * depth");

        assertEquals(dModel, gradWk.rows(), "Le nombre de lignes de gradWk devrait correspondre à dModel");
        assertEquals(numHeads * depth, gradWk.columns(), "Le nombre de colonnes de gradWk devrait correspondre à numHeads * depth");

        assertEquals(numHeads * depth, gradWv.rows(), "Le nombre de lignes de gradWv devrait correspondre à numHeads * depth");
        assertEquals(dModel, gradWv.columns(), "Le nombre de colonnes de gradWv devrait correspondre à dModel");

        assertEquals(numHeads * depth, gradWo.rows(), "Le nombre de lignes de gradWo devrait correspondre à numHeads * depth");
        assertEquals(dModel, gradWo.columns(), "Le nombre de colonnes de gradWo devrait correspondre à dModel");

        // Vérifier les gradients des entrées
        assertEquals(batchSize, gradInput.shape()[0], "Le nombre de batch de gradInput est incorrect");
        assertEquals(seqLength, gradInput.shape()[1], "La longueur de séquence de gradInput est incorrecte");
        assertEquals(dModel, gradInput.shape()[2], "La dimension du modèle de gradInput est incorrecte");

        // Optionnel : Vérifier que les gradients ne contiennent pas de NaN ou d'Inf
        assertFalse(gradWq.isNaN().any(), "gradWq ne devrait pas contenir de NaN");
        assertFalse(gradWq.isInfinite().any(), "gradWq ne devrait pas contenir d'Inf");
        assertFalse(gradWk.isNaN().any(), "gradWk ne devrait pas contenir de NaN");
        assertFalse(gradWk.isInfinite().any(), "gradWk ne devrait pas contenir d'Inf");
        assertFalse(gradWv.isNaN().any(), "gradWv ne devrait pas contenir de NaN");
        assertFalse(gradWv.isInfinite().any(), "gradWv ne devrait pas contenir d'Inf");
        assertFalse(gradWo.isNaN().any(), "gradWo ne devrait pas contenir de NaN");
        assertFalse(gradWo.isInfinite().any(), "gradWo ne devrait pas contenir d'Inf");
        assertFalse(gradInput.isNaN().any(), "gradInput ne devrait pas contenir de NaN");
        assertFalse(gradInput.isInfinite().any(), "gradInput ne devrait pas contenir d'Inf");
    }
}
package RN.transformer;

import static org.junit.Assert.assertArrayEquals; // Import assertArrayEquals for array comparisons

import org.junit.Test;
import org.nd4j.linalg.api.ndarray.INDArray;

public class PositionalEncodingTest {
    
    @Test
    public void testGetPositionalEncodingShape() {
        int dModel = 512;
        long sequenceLength = 20;
        PositionalEncoding pe = new PositionalEncoding(dModel);
        
        INDArray posEncoding = pe.getPositionalEncoding(sequenceLength);
        
        // Use assertArrayEquals to compare shapes
        assertArrayEquals("The shape of positional encoding should match [sequenceLength, dModel]", 
                          new long[]{sequenceLength, dModel}, 
                          posEncoding.shape());
    }
}
package RN.transformer;

import static org.junit.Assert.assertFalse;
import static org.junit.Assert.assertNotNull;
import static org.junit.Assert.assertTrue;

import java.io.File;
import java.io.IOException;
import java.nio.file.Files;
import java.nio.file.Path;
import java.util.Arrays;
import java.util.List;

import org.junit.Before;
import org.junit.Test;
import org.nd4j.linalg.api.ndarray.INDArray;

public class TransformerModelTest {

    private TransformerModel model;

    @Before
    public void setUp() throws Exception {
        // Initialisation de TransformerModel sans lancer d'exception
        model = new TransformerModel(); 
    }

    @Test
    public void testModelInitialization() {
        // Vérifications initiales du modèle
        assertNotNull("L'encoder ne devrait pas être null après l'initialisation du modèle", model.encoder);
        assertNotNull("Le decoder ne devrait pas être null après l'initialisation du modèle", model.decoder);
        assertNotNull("L'optimizer ne devrait pas être null après l'initialisation du modèle", model.optimizer);
        assertNotNull("Le tokenizer ne devrait pas être null après l'initialisation du modèle", model.tokenizer);
        assertFalse("Le modèle ne devrait pas être marqué comme entraîné initialement", model.isTrained());
    }
    

    @Test
    public void testSaveAndLoadState() throws IOException, ClassNotFoundException {
        // Créer une instance du modèle
        TransformerModel originalModel = new TransformerModel();

        // Simuler un entraînement en modifiant quelques paramètres
        originalModel.train(new MockDataGenerator(), 1); // Supposons que vous avez une implémentation mock de DataGenerator pour les tests
        
        // Sauvegarder l'état du modèle
        String filePath = "test_transformer_state.ser";
        originalModel.saveState(filePath);
        
        // Créer une nouvelle instance du modèle
        TransformerModel loadedModel = new TransformerModel();
        
        // Charger l'état sauvegardé
        loadedModel.loadState(filePath);
        
        // Vérifier que les états sont identiques
        assertTrue(compareModels(originalModel, loadedModel));
        
        // Nettoyer le fichier de test
        new File(filePath).delete();
    }
    
    private boolean compareModels(TransformerModel model1, TransformerModel model2) {
        // Comparer les paramètres de l'encodeur
        if (!compareParameters(model1.encoder.getParameters(), model2.encoder.getParameters())) {
            return false;
        }
        
        // Comparer les paramètres du décodeur
        if (!compareParameters(model1.decoder.getParameters(), model2.decoder.getParameters())) {
            return false;
        }
        
        // Comparer l'état de l'optimiseur
        if (model1.optimizer.getCurrentStep() != model2.optimizer.getCurrentStep() ||
            model1.optimizer.getEpoch() != model2.optimizer.getEpoch() ||
            model1.optimizer.getLearningRate() != model2.optimizer.getLearningRate()) {
            return false;
        }
        
        // Comparer l'état d'entraînement
        if (model1.isTrained() != model2.isTrained()) {
            return false;
        }
        
        return true;
    }
    
    private boolean compareParameters(List<INDArray> params1, List<INDArray> params2) {
        if (params1.size() != params2.size()) {
            return false;
        }
        
        for (int i = 0; i < params1.size(); i++) {
            if (!params1.get(i).equalsWithEps(params2.get(i), 1e-5)) {
                return false;
            }
        }
        
        return true;
    }
    
    // Classe mock pour DataGenerator
    private class MockDataGenerator extends DataGenerator {
        public MockDataGenerator() throws IOException {
            super("src/test/resources/mock_data.txt", "src/test/resources/mock_target.txt", new Tokenizer(Arrays.asList("tutu", "toto")), 1, 100);
        }
        
        @Override
        public boolean hasNextBatch() {
            return false; // Pour simplifier, on suppose qu'il n'y a pas de batch à traiter
        }
        
        @Override
        public Batch nextBatch() {
            return new Batch(List.of("mock input"), List.of("mock target"));
        }
    }

    @Test
    public void testTrainingChangesModelToTrained() throws Exception {
        // Utilisation de DummyDataGenerator pour simuler l'entraînement
        DataGenerator dummyDataGenerator = new DummyDataGenerator("src/test/resources/dummy-data.txt", "src/test/resources/dummy-data-target.txt", model.tokenizer, 32, 512, 5);
        model.train(dummyDataGenerator, 1);
        assertTrue("Le modèle devrait être marqué comme entraîné après l'entraînement", model.isTrained());
    }

    @Test(expected = IllegalStateException.class)
    public void testInferenceBeforeTrainingThrowsException() {
        // Tentative d'inférence avant l'entraînement devrait lancer une exception
        model.infer("Some input text", 30);
    }
    


    @Test
    public void testInferenceAfterTraining() throws Exception {
        // Simuler l'entraînement
        DataGenerator dummyDataGenerator = new DummyDataGenerator("src/test/resources/dummy-data.txt", "src/test/resources/dummy-data-target.txt", model.tokenizer, 32, 512, 5);
        model.train(dummyDataGenerator, 1);
        
        String inputPrompt = "Some input text";
        String response = model.infer(inputPrompt, 45);
        assertNotNull("L'inférence devrait retourner une réponse non-null", response);
        System.out.println("Inférence 1 prompt: " + inputPrompt + " : " + response);
        // Ici, vous pouvez ajouter d'autres assertions pour vérifier la plausibilité de la réponse.
        inputPrompt = "This is a dummy sentence";
        response = model.infer(inputPrompt, 32);
        assertNotNull("L'inférence devrait retourner une réponse non-null", response);
        System.out.println("Inférence 2 prompt: " + inputPrompt + " : " + response);

    }

    @Test
    public void testInferenceAfterTraining2() throws Exception {
        // Initialiser le tokenizer et le modèle
        TransformerModel model = new TransformerModel(2, 300, 6, 2048, 0.1); // Utiliser 2 layers pour le test

        // Créer un DummyDataGenerator avec 3 batches par epoch
        DummyDataGenerator dataGenerator = new DummyDataGenerator(
            "src/test/resources/dummy-data.txt",
            "src/test/resources/dummy-data-target.txt",
            model.tokenizer,
            2,  // batchSize
            50, // maxTokensPerBatch
            3   // maxBatchesPerEpoch
        );

        // Simuler l'entraînement
        model.train(dataGenerator, 1);

        // Effectuer l'inférence
        String response1 = model.infer("Some input text", 45);
        assertNotNull("L'inférence devrait retourner une réponse non-null", response1);
        System.out.println("Inference Response 1: " + response1);

        String response2 = model.infer("This is a dummy sentence", 32);
        assertNotNull("L'inférence devrait retourner une réponse non-null", response2);
        System.out.println("Inference Response 2: " + response2);
    }


}
package RN.transformer;


import static org.junit.Assert.assertArrayEquals;
import static org.junit.Assert.assertEquals;
import static org.junit.Assert.assertFalse;
import static org.junit.Assert.assertNotNull;
import static org.junit.Assert.assertTrue;
import static org.junit.jupiter.api.Assertions.assertArrayEquals;
import static org.junit.jupiter.api.Assertions.assertEquals;
import static org.junit.jupiter.api.Assertions.assertFalse;
import static org.junit.jupiter.api.Assertions.assertNotEquals;
import static org.junit.jupiter.api.Assertions.assertNotNull;
import static org.junit.jupiter.api.Assertions.assertTrue;
import static org.junit.jupiter.api.Assertions.fail;

import java.io.File;
import java.io.IOException;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.List;
import java.util.Map;

import org.junit.Before;
import org.junit.Test;
import org.nd4j.linalg.api.buffer.DataType;
import org.nd4j.linalg.api.ndarray.INDArray;
import org.nd4j.linalg.factory.Nd4j;
import org.apache.commons.lang3.tuple.Pair;

public class TransformerTest {
    
    private TransformerModel model;

    @Before
    public void setUp() throws Exception {
        // Initialisation de TransformerModel sans lancer d'exception
        model = new TransformerModel(); 
    }
    
    // @After
    // public void tearDown() throws Exception {
    // 	model.cleanGradients();
    //     model = null;
    // }
    
    @Test
    public void testMaskCreation() {
        // Créer un batch de tokens (List<List<Integer>>)
        List<List<Integer>> tokensBatch = Arrays.asList(
            Arrays.asList(1, 2, 3, 0, 0) // 0 est supposé être le token de padding
        );
        
        // Appeler createPaddingMask avec le batch de tokens
        INDArray paddingMask = model.createPaddingMask(tokensBatch);
        
        // Afficher le masque généré pour le débogage
        System.out.println("paddingMask: " + paddingMask);
        
        // Créer le masque attendu
        // Comme le masque a la forme [batchSize, 1, 1, seqLength], nous devons créer un tableau 4D
        INDArray expectedMask = Nd4j.create(new float[][][][]{
            { { { 0.0f, 0.0f, 0.0f, Float.NEGATIVE_INFINITY, Float.NEGATIVE_INFINITY } } }
        });
        
        // S'assurer que les types de données correspondent
        paddingMask = paddingMask.castTo(expectedMask.dataType());
        
        // Comparer les masques élément par élément
        long[] shape = paddingMask.shape();
        for (int i = 0; i < shape[0]; i++) {
            for (int j = 0; j < shape[1]; j++) {
                for (int k = 0; k < shape[2]; k++) {
                    for (int l = 0; l < shape[3]; l++) {
                        float actualValue = paddingMask.getFloat(i, j, k, l);
                        float expectedValue = expectedMask.getFloat(i, j, k, l);
                        if (Float.isInfinite(expectedValue)) {
                            assertTrue(Float.isInfinite(actualValue),
                                    String.format("Position (%d,%d,%d,%d) expected Infinite but got %f", i, j, k, l, actualValue));
                        } else {
                            assertEquals(expectedValue, actualValue, 1e-5f,
                                    String.format("Position (%d,%d,%d,%d) values don't match", i, j, k, l));
                        }
                    }
                }
            }
        }
        
        // Tester le lookAheadMask si nécessaire
        INDArray lookAheadMask = model.createLookAheadMask(5);
        for (int i = 0; i < 5; i++) {
            for (int j = 0; j < 5; j++) {
                float expectedValue = j > i ? Float.NEGATIVE_INFINITY : 0.0f;
                float actualValue = lookAheadMask.getFloat(i, j);
                if (Float.isInfinite(expectedValue)) {
                    assertTrue(Float.isInfinite(actualValue),
                            String.format("Position (%d,%d) expected Infinite but got %f", i, j, actualValue));
                } else {
                    assertEquals(expectedValue, actualValue, 1e-5f,
                            String.format("Position (%d,%d) values don't match", i, j));
                }
            }
        }
    }
    
    
    
    public static boolean compareWithInfinity(INDArray matrix1, INDArray matrix2, double epsilon) {
        
    	// Vérifier si les deux matrices ont la même forme
        if (!matrix1.shapeInfoToString().equals(matrix2.shapeInfoToString())) {
            return false;
        }

        // Parcourir chaque élément pour comparer
        for (int i = 0; i < matrix1.length(); i++) {
            double val1 = matrix1.getDouble(i);
            double val2 = matrix2.getDouble(i);

            if (Double.isInfinite(val1) && Double.isInfinite(val2)) {
                // Si les deux valeurs sont infinies, elles sont considérées égales
                continue;
            }

            if (Double.isNaN(val1) || Double.isNaN(val2)) {
                // Gérer les NaN explicitement
                if (Double.isNaN(val1) != Double.isNaN(val2)) {
                    return false;
                }
            } else if (Math.abs(val1 - val2) > epsilon) {
                // Comparaison normale avec tolérance epsilon
                return false;
            }
        }

        return true;
    }
    
    
    @Test
    public void testTokenizationAndDetokenization() {
        String originalText = "The quick brown fox jumps over the lazy dog";
        Tokenizer tokenizer = new Tokenizer(Arrays.asList(originalText.split(" ")));
        List<String> tokens = tokenizer.tokenize(originalText);
        List<Integer> tokenIds = tokenizer.tokensToIds(tokens);
        String reconstructedText = tokenizer.idsToTokens(tokenIds);
        
        org.junit.Assert.assertEquals("Text should be preserved after tokenization and detokenization", originalText, reconstructedText);
    }

    @Test
    public void testEncodeDecode() {
        // Initialize the Tokenizer with a known vocabulary
        List<String> vocabulary = Arrays.asList("le", "chat", "est", "sur", "tapis", "les", "chiens", "dans", "jardin", "aiment", "manger");
        Tokenizer tokenizer = new Tokenizer(vocabulary);
    
        // Initialize the model with appropriate parameters
        int numLayers = 2;
        int dModel = 300;
        int numHeads = 6;
        int dff = 512;
        int vocabSize = vocabulary.size();
        TransformerModel model = new TransformerModel(numLayers, dModel, numHeads, dff, vocabSize);
        model.tokenizer = tokenizer; // Associate the tokenizer with the model
    
        String testInput = "le chat est sur le tapis les chiens dans le jardin";
        System.out.println("Test input: " + testInput);
        List<String> tokens = model.tokenizer.tokenize(testInput);
        System.out.println("Tokens: " + tokens);
    
        // Token IDs
        List<Integer> inputTokenIds = model.tokenizer.tokensToIds(tokens);
        System.out.println("Input Token IDs: " + inputTokenIds);
    
        // Prepare inputTokenIdsBatch as List<List<Integer>>
        List<List<Integer>> inputTokenIdsBatch = Arrays.asList(inputTokenIds);
    
        // Convert inputTokenIdsBatch to INDArray for encoder input
        int batchSize = inputTokenIdsBatch.size();
        int seqLength = inputTokenIds.size();
    
        // Create an INDArray for encoder input
        INDArray encoderInput = Nd4j.create(DataType.INT32, batchSize, seqLength);
        for (int i = 0; i < batchSize; i++) {
            List<Integer> sequence = inputTokenIdsBatch.get(i);
            for (int j = 0; j < sequence.size(); j++) {
                encoderInput.putScalar(new int[]{i, j}, sequence.get(j));
            }
        }
    
        // Create padding masks
        INDArray encoderPaddingMask = model.createPaddingMask(inputTokenIdsBatch);
        INDArray decoderPaddingMask = model.createPaddingMask(inputTokenIdsBatch);
        INDArray lookAheadMask = model.createLookAheadMask(seqLength);
    
        System.out.println("Encoder Padding Mask shape: " + Arrays.toString(encoderPaddingMask.shape()));
        System.out.println("Encoder Padding Mask: " + encoderPaddingMask);
    

        List<List<Integer>> inputTokenIdsBatchFromArray = new ArrayList<>();
        List<Integer> sequence = new ArrayList<>();
        for (int j = 0; j < seqLength; j++) {
            sequence.add(encoderInput.getInt(0, j));
        }
        inputTokenIdsBatchFromArray.add(sequence);

        // Encode the input
        INDArray encoded = model.encoder.encode(false, inputTokenIdsBatchFromArray, encoderPaddingMask);

        assertNotNull(encoded, "Encoded output should not be null.");
        System.out.println("Encoded output shape: " + Arrays.toString(encoded.shape()));
    
        // Verify the shape of the encoding
        assertEquals(1, (int) encoded.shape()[0], "Batch size should be 1");
        assertEquals(seqLength, (int) encoded.shape()[1], "Sequence length should match the input tokens size");
        assertEquals(dModel, (int) encoded.shape()[2], "dModel should be " + dModel);
    
        // Préparer decoderInput comme un tenseur de rang 3 et de type FLOAT
        INDArray decoderInput = Nd4j.rand(DataType.FLOAT, batchSize, seqLength, dModel);
        System.out.println("decoderInput shape: " + Arrays.toString(decoderInput.shape()));

        // Decode the output
        INDArray decoded = model.decoder.decode(false, decoderInput, encoded, lookAheadMask, decoderPaddingMask);
        assertNotNull(decoded, "Decoded output should not be null.");
        System.out.println("Decoded output shape: " + Arrays.toString(decoded.shape()));
        System.out.println("Decoded output: " + decoded);
    
        // Verify the shape of the decoder output
        assertEquals(1, (int) decoded.shape()[0], "Batch size should be 1");
        assertEquals(seqLength, (int) decoded.shape()[1], "Sequence length should match the input tokens size");

        int outputSize = TransformerModel.getVocabSize(); // Assurez-vous que cette valeur est correcte
        assertEquals(outputSize, (int) decoded.shape()[2], "Output size should be " + outputSize);

    }
    
        
    
    @Test
    public void testBackwardPropagation() {
        // Création des entrées fictives
        String testInput = "Test input";
        List<String> tokens = model.tokenizer.tokenize(testInput);
    
        List<Integer> inputTokenIds = model.tokenizer.tokensToIds(tokens);
        List<Integer> targetTokenIds = model.tokenizer.tokensToIds(tokens);
    
        // Préparer inputTokenIdsBatch et targetTokenIdsBatch comme List<List<Integer>>
        List<List<Integer>> inputTokenIdsBatch = Arrays.asList(inputTokenIds);
        List<List<Integer>> targetTokenIdsBatch = Arrays.asList(targetTokenIds);
    
        INDArray encoderPaddingMask = model.createPaddingMask(inputTokenIdsBatch);
        INDArray decoderPaddingMask = model.createPaddingMask(targetTokenIdsBatch);
        INDArray lookAheadMask = model.createLookAheadMask(targetTokenIds.size());
    
        // Passe forward
        INDArray encoded = model.encoder.encode(true, inputTokenIdsBatch, encoderPaddingMask);
        INDArray decoderOutput = model.decoder.decode(true, encoded, encoded, lookAheadMask, decoderPaddingMask);
    
        // Vérifier la forme des logits
        System.out.println("Decoder Output Shape: " + Arrays.toString(decoderOutput.shape()));
        
        // Création d'un gradient fictif pour la rétropropagation
        INDArray gradOutput = Nd4j.rand(decoderOutput.shape()).castTo(DataType.FLOAT);
    
        // Appel de la méthode backward
        Map<String, INDArray> gradients = model.decoder.backward(gradOutput);
    
        // Assertions pour vérifier les gradients
        assertNotNull("Les gradients ne devraient pas être null", gradients);
        assertFalse("Le map des gradients ne devrait pas être vide", gradients.isEmpty());
        
        // Définir les clés attendues
        List<String> expectedKeys = Arrays.asList("input", "gamma", "beta", "W1", "b1", "W2", "b2");
    
        for (String key : expectedKeys) {
            assertTrue("Les gradients devraient contenir la clé '" + key + "'", gradients.containsKey(key));
        }
    
        // S'assurer qu'il n'y a pas de clés inattendues
        // for (String key : gradients.keySet()) {
        //     assertTrue("Clé de gradient inattendue : " + key, expectedKeys.contains(key));
        // }
    
        // Optionnel : Vérifier que les gradients ne contiennent pas de NaN ou d'Inf
        for (Map.Entry<String, INDArray> entry : gradients.entrySet()) {
            INDArray grad = entry.getValue();
            assertFalse("Le gradient pour " + entry.getKey() + " contient des NaN", grad.isNaN().any());
            assertFalse("Le gradient pour " + entry.getKey() + " contient des Inf", grad.isInfinite().any());
        }
    
        // Vérifier que les gradients ont des formes cohérentes
        for (Map.Entry<String, INDArray> entry : gradients.entrySet()) {
            String key = entry.getKey();
            INDArray grad = entry.getValue();
            switch (key) {
                case "W1":
                    assertArrayEquals("Forme des gradients de W1", new long[]{model.getDModel(), 2048}, grad.shape());
                    break;
                case "b1":
                    assertArrayEquals("Forme des gradients de b1", new long[]{1, 2048}, grad.shape());
                    break;
                case "W2":
                    assertArrayEquals("Forme des gradients de W2", new long[]{2048, model.getDModel()}, grad.shape());
                    break;
                case "b2":
                    assertArrayEquals("Forme des gradients de b2", new long[]{1, model.getDModel()}, grad.shape());
                    break;
                // case "gamma":
                //     assertArrayEquals("Forme des gradients de gamma", new long[]{1, 512}, grad.shape());
                //     break;
                // case "beta":
                //     assertArrayEquals("Forme des gradients de beta", new long[]{1, 512}, grad.shape());
                //     break;
              
                // default:
                //     fail("Clé de gradient inattendue : " + key);
            }
        }
    }
    

    

    @Test
    public void testParameterUpdates() throws IOException {
    	
        // Créez un DataGenerator mock qui ne nécessite pas de fichiers réels
        DataGenerator mockDataGenerator = new DummyDataGenerator("src/test/resources/dummy-data.txt", "src/test/resources/dummy-data-target.txt", model.tokenizer, 32, 512, 5);
        
        INDArray initialWeights = model.getCombinedParameters().get(0).dup();
        model.train(mockDataGenerator, 5);
        INDArray updatedWeights = model.getCombinedParameters().get(0);
        assertFalse(initialWeights.equalsWithEps(updatedWeights, 1e-6), "Weights should be updated after training.");
    }

    @Test
    public void testLossCalculation() {
        int batchSize = 2;
        int seqLength = 5;
        int vocabSize = 15;

        // Assurez-vous que vocabSize est correct
        assertEquals("Vocab size should match", vocabSize, TransformerModel.getVocabSize());

        // Création des logits : [batchSize, seqLength, vocabSize]
        INDArray logitsArray = Nd4j.rand(new int[]{batchSize, seqLength, vocabSize}, 'c');
        List<INDArray> logits = Arrays.asList(logitsArray);

        // Création des targetTokenIds : 2 séquences de 5 tokens chacune
        // Assurez-vous que les IDs sont valides (entre 0 et vocabSize -1)
        List<Integer> targetTokenIds1 = Arrays.asList(1, 2, 3, 4, 5);
        List<Integer> targetTokenIds2 = Arrays.asList(2, 3, 4, 5, 6);
        List<List<Integer>> targetBatchTokenIds = Arrays.asList(targetTokenIds1, targetTokenIds2);

        // Calcul de la perte et des gradients
        Pair<Float, INDArray> lossAndGradients = model.calculateCrossEntropyLossAndGradient(logits, targetBatchTokenIds);
        float loss = lossAndGradients.getLeft();
        INDArray gradients = lossAndGradients.getRight();

        // Vérification que la perte est non négative
        assertTrue("La perte doit être non-négative.", loss >= 0);

        // Vérification de la forme des gradients
        assertArrayEquals("Forme des gradients doit correspondre aux logits",
            logitsArray.shape(), gradients.shape());

        // // Vérification que les gradients sont dans des plages attendues
        // assertFalse("Les gradients ne doivent pas contenir de NaN.", gradients.isNaN());
        // assertFalse("Les gradients ne doivent pas contenir d'Inf.", gradients.isInfinite());

        // (Optionnel) Vérification de valeurs spécifiques pour des cas simples
        // Vous pouvez définir des logits et des cibles spécifiques et vérifier la perte et les gradients attendus
    }


    @Test
    public void testInference() throws IOException {
        // Initialiser le tokenizer et le modèle
        TransformerModel model = new TransformerModel(2, 300, 6, 2048, 0.1);

        // Simuler un entraînement (optionnel)
        model.setTrained(true); // Assurez-vous d'avoir un setter pour cette variable si elle est privée

        // Effectuer l'inférence
        String response = model.infer("Hello world", 100);
        assertFalse(response.isEmpty(), "Response should not be empty.");
    }
    
    @Test
    public void testTrainingState() {
        assertFalse(model.isTrained(), "Model should not be trained initially");
        
        // Simulez un entraînement
        model.setTrained(true); // Assurez-vous d'avoir un setter pour cette variable si elle est privée
        
        assertTrue(model.isTrained(), "Model should be marked as trained after training");
    }

    @Test
    public void testSaveAndLoadState() throws IOException, ClassNotFoundException {
        // Simuler un entraînement
        model.setTrained(true); // Assurez-vous d'avoir un setter pour cette variable si elle est privée
        INDArray initialWeights = model.encoder.getParameters().get(0).dup();
        
        // Sauvegarder l'état
        String filePath = "test_model_state.ser";
        model.saveState(filePath);
        
        // Créer un nouveau modèle et charger l'état
        TransformerModel loadedModel = new TransformerModel();
        loadedModel.loadState(filePath);
        
        // Vérifier que l'état chargé correspond à l'état sauvegardé
        assertTrue(loadedModel.isTrained(), "Loaded model should be marked as trained");
        INDArray loadedWeights = loadedModel.encoder.getParameters().get(0);
        assertTrue(initialWeights.equalsWithEps(loadedWeights, 1e-6), "Loaded weights should match initial weights");
        
        // Nettoyer le fichier de test
        new File(filePath).delete();
    }    
    
    @Test
    public void testOptimizerUpdate() {
        // Ajouter les paramètres au modèle
        model.addCombinedParameters();
        
        // Récupérer les paramètres ajoutés
        List<INDArray> parameters = model.getCombinedParameters();
        
        // Initialiser l'optimiseur **après** l'ajout des paramètres
        float initialLr = 0.001f;
        int dmodel = 3;
        int warmupSteps = 1000;
        model.optimizer = new CustomAdamOptimizer(initialLr, dmodel, warmupSteps, parameters);
        
        // Générer des gradients fictifs
        List<INDArray> gradients = new ArrayList<>();
        for (INDArray param : parameters) {
            gradients.add(Nd4j.rand(param.shape()));
        }
        
        // Copier les paramètres initiaux
        List<INDArray> initialParams = new ArrayList<>();
        for (INDArray param : parameters) {
            initialParams.add(param.dup());
        }
        
        // Effectuer une mise à jour
        model.optimizer.update(parameters, gradients);
        
        // Vérifier que les paramètres ont été mis à jour
        for (int i = 0; i < parameters.size(); i++) {
            assertFalse(parameters.get(i).equalsWithEps(initialParams.get(i), 1e-6), 
                        "Parameter " + i + " should have been updated");
        }
    }

    @Test
    public void testOptimizerMultipleUpdates() {
        // Ajouter les paramètres au modèle
        model.addCombinedParameters();
        
        // Récupérer les paramètres ajoutés
        List<INDArray> parameters = model.getCombinedParameters();
        
        // Initialiser l'optimiseur avec warmupSteps = 1
        float initialLr = 0.001f;
        int dmodel = 3;
        int warmupSteps = 1;
        model.optimizer = new CustomAdamOptimizer(initialLr, dmodel, warmupSteps, parameters);
        
        // Générer des gradients fictifs
        List<INDArray> gradients = new ArrayList<>();
        for (INDArray param : parameters) {
            gradients.add(Nd4j.rand(param.shape()));
        }
        
        // Copier les paramètres initiaux
        List<INDArray> initialParams = new ArrayList<>();
        for (INDArray param : parameters) {
            initialParams.add(param.dup());
        }
        
        // Effectuer plusieurs mises à jour
        int numUpdates = 10;
        for (int u = 0; u < numUpdates; u++) {
            model.optimizer.update(parameters, gradients);
        }
        
        // Vérifier que les paramètres ont été mis à jour
        for (int i = 0; i < parameters.size(); i++) {
            assertFalse(parameters.get(i).equalsWithEps(initialParams.get(i), 1e-6), 
                        "Parameter " + i + " should have been updated");
        }
    }
    
    
    
    @Test
    public void testAdaptiveLearningRate() {
        double initialLr = model.optimizer.getLearningRate();
        
        // Simuler plusieurs étapes d'entraînement
        for (int i = 0; i < 2000; i++) {
            model.optimizer.setCurrentStep(i);
            model.optimizer.calculateLearningRate();

        }
        
        double laterLr = model.optimizer.getLearningRate();
        assertNotEquals(initialLr, laterLr, "Learning rate should change over time");
    }
    
    
    
//    @Test
//    public void testLossDecrease() {
//        // Assuming you have a method to run multiple epochs and return the last loss
//        double initialLoss = model.runEpochAndGetLoss();
//        double laterLoss = model.runEpochAndGetLoss();
//        assertTrue(laterLoss < initialLoss, "Loss should decrease after training for an epoch.");
//    }
//
//    @Test
//    public void testGradientsNonZero() {
//        // Run a single backward step and check gradients
//        model.train(new DataGenerator("dummy_path", "dummy_target", model.tokenizer, 1, 10));
//        boolean allNonZero = model.decoder.getGradients().stream()
//                             .allMatch(g -> !g.isZeroNumber());
//        assertTrue(allNonZero, "All gradients should be non-zero after training step.");
//    }
//
//    @Test
//    public void testOutputRange() {
//        // Assuming outputs are probabilities from the last layer
//        double[] outputs = model.forwardPassAndGetOutputs(new double[]{0.1, 0.2, 0.7}); // Example input
//        for (double output : outputs) {
//            assertTrue(output >= 0 && output <= 1, "Each output should be a valid probability.");
//        }
//        assertEquals(1.0, Arrays.stream(outputs).sum(), 0.01, "Sum of output probabilities should be 1.");
//    }
}
package RN.transformer;


import org.junit.Before;
import org.junit.Test;
import org.nd4j.linalg.api.ndarray.INDArray;
import org.nd4j.linalg.factory.Nd4j;

import static org.junit.Assert.*;

import java.util.ArrayList;
import java.util.List;

public class calculateLearningRateOptimizerTest {

    private CustomAdamOptimizer optimizer;
    private final float initialLr = 0.001f;
    private final int warmupSteps = 1000;
    private final int modelSize = 512;
    private List<INDArray> parameters;
    private List<INDArray> gradients;

    @Before
    public void setUp() {
        parameters = new ArrayList<>();
        parameters.add(Nd4j.create(new float[]{0.1f, -0.2f}, new int[]{1, 2}));
        gradients = new ArrayList<>();
        gradients.add(Nd4j.create(new float[]{0.01f, -0.01f}, new int[]{1, 2}));
        
        optimizer = new CustomAdamOptimizer(initialLr, modelSize, warmupSteps, parameters);
    }

    @Test
    public void testCalculateLearningRateAtStart() {
        // Au début, le taux d'apprentissage devrait être initialLr
        assertEquals("At start, learning rate should be initialLr", initialLr, optimizer.getLearningRate(), 0.0);
    }

    @Test
    public void testCalculateLearningRateDuringWarmup() {

        // Simuler un pas pendant la période de warmup, par exemple à mi-chemin.
        optimizer.setCurrentStep(warmupSteps / 2);
        double lrMidWarmup = optimizer.calculateLearningRate();
        
        // Simuler le dernier pas de la période de warmup.
        optimizer.setCurrentStep(warmupSteps - 1);
        double lrEndWarmup = optimizer.calculateLearningRate();

        // Vérifier que le taux d'apprentissage à mi-chemin est supérieur à celui du départ.
        assertTrue("During warmup, learning rate should be more than initial at the start", lrMidWarmup > 0);

        // Vérifier que le taux d'apprentissage à la fin de la période de warmup ne dépasse pas initialLr.
        assertTrue("At the end of warmup, learning rate should not exceed initial learning rate", lrEndWarmup <= initialLr);

        // Optionnellement, vérifier que le taux d'apprentissage augmente au cours du warmup.
        assertTrue("Learning rate should increase during warmup", lrEndWarmup > lrMidWarmup);
    }

    @Test
    public void testCalculateLearningRateJustAfterWarmup() {
        // Juste après la période de warmup
        optimizer.setCurrentStep(warmupSteps + 1);
        double lrJustAfterWarmup = optimizer.calculateLearningRate();
        assertTrue("Just after warmup, learning rate should start to decrease", lrJustAfterWarmup < initialLr);
    }

    @Test
    public void testCalculateLearningRateWellAfterWarmup() {
        // Bien après la période de warmup
        optimizer.setCurrentStep(warmupSteps * 2);
        double lrWellAfterWarmup = optimizer.calculateLearningRate();
        assertTrue("Well after warmup, learning rate should be significantly lower than initialLr", lrWellAfterWarmup < initialLr);
    }
}
