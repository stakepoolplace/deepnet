21:01:20.622 [main] INFO org.nd4j.linalg.factory.Nd4jBackend - Loaded [CpuBackend] backend
21:01:21.388 [main] INFO org.nd4j.nativeblas.NativeOpsHolder - Number of threads used for linear algebra: 1
21:01:21.392 [main] INFO org.nd4j.linalg.cpu.nativecpu.CpuNDArrayFactory - Binary level Generic x86 optimization level Generic x86
21:01:21.449 [main] INFO org.nd4j.nativeblas.Nd4jBlas - Number of threads used for OpenMP BLAS: 8
21:01:21.561 [main] INFO org.nd4j.linalg.api.ops.executioner.DefaultOpExecutioner - Backend used: [CPU]; OS: [Mac OS X]
21:01:21.561 [main] INFO org.nd4j.linalg.api.ops.executioner.DefaultOpExecutioner - Cores: [8]; Memory: [2,0GB];
21:01:21.562 [main] INFO org.nd4j.linalg.api.ops.executioner.DefaultOpExecutioner - Blas vendor: [OPENBLAS]
21:01:21.565 [main] INFO org.nd4j.linalg.cpu.nativecpu.CpuBackend - Backend build information:
 Clang: "12.0.0 (clang-1200.0.32.29)"
STD version: 201103L
DEFAULT_ENGINE: samediff::ENGINE_CPU
HAVE_FLATBUFFERS
HAVE_OPENBLAS
21:01:21.593 [main] INFO org.deeplearning4j.models.sequencevectors.SequenceVectors - Starting vocabulary building...
21:01:21.595 [main] DEBUG org.deeplearning4j.models.word2vec.wordstore.VocabConstructor - Target vocab size before building: [0]
21:01:21.604 [main] DEBUG org.deeplearning4j.models.word2vec.wordstore.VocabConstructor - Trying source iterator: [0]
21:01:21.604 [main] DEBUG org.deeplearning4j.models.word2vec.wordstore.VocabConstructor - Target vocab size before building: [0]
21:01:21.613 [main] DEBUG org.deeplearning4j.models.word2vec.wordstore.VocabConstructor - Waiting till all processes stop...
21:01:21.617 [main] DEBUG org.deeplearning4j.models.word2vec.wordstore.VocabConstructor - Vocab size before truncation: [43],  NumWords: [92], sequences parsed: [14], counter: [86]
21:01:21.617 [main] DEBUG org.deeplearning4j.models.word2vec.wordstore.VocabConstructor - Scavenger: Words before: 43; Words after: 43;
21:01:21.618 [main] DEBUG org.deeplearning4j.models.word2vec.wordstore.VocabConstructor - Vocab size after truncation: [43],  NumWords: [92], sequences parsed: [14], counter: [86]
21:01:21.639 [main] INFO org.deeplearning4j.models.word2vec.wordstore.VocabConstructor - Sequences checked: [14], Current vocabulary size: [43]; Sequences/sec: [311,11];
21:01:21.645 [main] INFO org.deeplearning4j.models.embeddings.loader.WordVectorSerializer - Projected memory use for model: [0,02 MB]
21:01:21.686 [main] INFO org.deeplearning4j.models.embeddings.inmemory.InMemoryLookupTable - Initializing syn1...
21:01:21.686 [main] INFO org.deeplearning4j.models.sequencevectors.SequenceVectors - Building learning algorithms:
21:01:21.687 [main] INFO org.deeplearning4j.models.sequencevectors.SequenceVectors -           building ElementsLearningAlgorithm: [SkipGram]
21:01:21.694 [main] INFO org.deeplearning4j.models.sequencevectors.SequenceVectors - Starting learning process...
21:01:21.766 [main] INFO org.deeplearning4j.models.sequencevectors.SequenceVectors - Epoch [1] finished; Elements processed so far: [860];  Sequences processed: [140]
21:01:21.766 [main] INFO org.deeplearning4j.models.sequencevectors.SequenceVectors - Time spent on training: 72 ms
21:01:21.779 [main] DEBUG org.deeplearning4j.models.embeddings.loader.WordVectorSerializer - Saving header: 43 50 14
Embedding pour le token 'mauvais' initialisé avec WordVectors.
Embedding pour le token 'aujourd'hui' initialisé avec WordVectors.
Embedding pour le token 'aiment' initialisé avec WordVectors.
Embedding pour le token 'est' initialisé avec WordVectors.
Embedding pour le token 'agréable' initialisé avec WordVectors.
Embedding pour le token 'journée' initialisé avec WordVectors.
Embedding pour le token 'temps' initialisé avec WordVectors.
Embedding pour le token 'sol' initialisé avec WordVectors.
Embedding pour le token 'sur' initialisé avec WordVectors.
Embedding pour le token 'mange' initialisé avec WordVectors.
Embedding pour le token 'belle' initialisé avec WordVectors.
Embedding pour le token 'un' initialisé avec WordVectors.
Embedding pour le token 'les' initialisé avec WordVectors.
Embedding pour le token 'chiens' initialisé avec WordVectors.
Embedding pour le token 'tapis' initialisé avec WordVectors.
Embedding pour le token 'intéressant' initialisé avec WordVectors.
Embedding pour le token 'ce' initialisé avec WordVectors.
Embedding pour le token 'jardin' initialisé avec WordVectors.
Embedding pour le token 'pas' initialisé avec WordVectors.
Embedding pour le token 'c'est' initialisé avec WordVectors.
Embedding pour le token 'excellent' initialisé avec WordVectors.
Embedding pour le token 'heureux' initialisé avec WordVectors.
Embedding pour le token 'fantastique' initialisé avec WordVectors.
Embedding pour le token 'film' initialisé avec WordVectors.
Embedding pour le token 'court' initialisé avec WordVectors.
Embedding pour le token 'dans' initialisé avec WordVectors.
Embedding pour le token 'chien' initialisé avec WordVectors.
Embedding pour le token 'livre' initialisé avec WordVectors.
Embedding pour le token 'n'aime' initialisé avec WordVectors.
Embedding pour le token 'triste' initialisé avec WordVectors.
Embedding pour le token 'souris' initialisé avec WordVectors.
Embedding pour le token 'déteste' initialisé avec WordVectors.
Embedding pour le token 'la' initialisé avec WordVectors.
Embedding pour le token 'chat' initialisé avec WordVectors.
Embedding pour le token 'quelle' initialisé avec WordVectors.
Embedding pour le token 'chats' initialisé avec WordVectors.
Embedding pour le token 'repas' initialisé avec WordVectors.
Embedding pour le token 'le' initialisé avec WordVectors.
Embedding pour le token 'suis' initialisé avec WordVectors.
Embedding pour le token 'je' initialisé avec WordVectors.
Embedding pour le token 'très' initialisé avec WordVectors.
Embedding pour le token 'mauvais' initialisé avec WordVectors.
Embedding pour le token 'aujourd'hui' initialisé avec WordVectors.
Embedding pour le token 'aiment' initialisé avec WordVectors.
Embedding pour le token 'est' initialisé avec WordVectors.
Embedding pour le token 'agréable' initialisé avec WordVectors.
Embedding pour le token 'journée' initialisé avec WordVectors.
Embedding pour le token 'temps' initialisé avec WordVectors.
Embedding pour le token 'sol' initialisé avec WordVectors.
Embedding pour le token 'sur' initialisé avec WordVectors.
Embedding pour le token 'mange' initialisé avec WordVectors.
Embedding pour le token 'belle' initialisé avec WordVectors.
Embedding pour le token 'un' initialisé avec WordVectors.
Embedding pour le token 'les' initialisé avec WordVectors.
Embedding pour le token 'chiens' initialisé avec WordVectors.
Embedding pour le token 'tapis' initialisé avec WordVectors.
Embedding pour le token 'intéressant' initialisé avec WordVectors.
Embedding pour le token 'ce' initialisé avec WordVectors.
Embedding pour le token 'jardin' initialisé avec WordVectors.
Embedding pour le token 'pas' initialisé avec WordVectors.
Embedding pour le token 'c'est' initialisé avec WordVectors.
Embedding pour le token 'excellent' initialisé avec WordVectors.
Embedding pour le token 'heureux' initialisé avec WordVectors.
Embedding pour le token 'fantastique' initialisé avec WordVectors.
Embedding pour le token 'film' initialisé avec WordVectors.
Embedding pour le token 'court' initialisé avec WordVectors.
Embedding pour le token 'dans' initialisé avec WordVectors.
Embedding pour le token 'chien' initialisé avec WordVectors.
Embedding pour le token 'livre' initialisé avec WordVectors.
Embedding pour le token 'n'aime' initialisé avec WordVectors.
Embedding pour le token 'triste' initialisé avec WordVectors.
Embedding pour le token 'souris' initialisé avec WordVectors.
Embedding pour le token 'déteste' initialisé avec WordVectors.
Embedding pour le token 'la' initialisé avec WordVectors.
Embedding pour le token 'chat' initialisé avec WordVectors.
Embedding pour le token 'quelle' initialisé avec WordVectors.
Embedding pour le token 'chats' initialisé avec WordVectors.
Embedding pour le token 'repas' initialisé avec WordVectors.
Embedding pour le token 'le' initialisé avec WordVectors.
Embedding pour le token 'suis' initialisé avec WordVectors.
Embedding pour le token 'je' initialisé avec WordVectors.
Embedding pour le token 'très' initialisé avec WordVectors.
Vocabulaire du Tokenizer:
Token: '<PAD>' -> ID: 0
Token: '<UNK>' -> ID: 1
Token: '<START>' -> ID: 2
Token: '<END>' -> ID: 3
Token: 'mauvais' -> ID: 4
Token: 'aujourd'hui' -> ID: 5
Token: 'aiment' -> ID: 6
Token: 'est' -> ID: 7
Token: 'agréable' -> ID: 8
Token: 'journée' -> ID: 9
Token: 'temps' -> ID: 10
Token: 'sol' -> ID: 11
Token: 'sur' -> ID: 12
Token: 'mange' -> ID: 13
Token: 'belle' -> ID: 14
Token: 'un' -> ID: 15
Token: 'les' -> ID: 16
Token: 'chiens' -> ID: 17
Token: 'tapis' -> ID: 18
Token: 'intéressant' -> ID: 19
Token: 'ce' -> ID: 20
Token: 'jardin' -> ID: 21
Token: 'pas' -> ID: 22
Token: 'c'est' -> ID: 23
Token: 'excellent' -> ID: 24
Token: 'heureux' -> ID: 25
Token: 'fantastique' -> ID: 26
Token: 'film' -> ID: 27
Token: 'court' -> ID: 28
Token: 'dans' -> ID: 29
Token: 'chien' -> ID: 30
Token: 'livre' -> ID: 31
Token: 'n'aime' -> ID: 32
Token: 'triste' -> ID: 33
Token: 'souris' -> ID: 34
Token: 'déteste' -> ID: 35
Token: 'la' -> ID: 36
Token: 'chat' -> ID: 37
Token: 'quelle' -> ID: 38
Token: 'chats' -> ID: 39
Token: 'repas' -> ID: 40
Token: 'le' -> ID: 41
Token: 'suis' -> ID: 42
Token: 'je' -> ID: 43
Token: 'très' -> ID: 44
21:01:22.001 [main] DEBUG org.deeplearning4j.models.embeddings.loader.WordVectorSerializer - Trying full model restoration...
21:01:22.005 [main] DEBUG org.deeplearning4j.models.embeddings.loader.WordVectorSerializer - Trying CSV model restoration...
21:01:22.006 [main] DEBUG org.deeplearning4j.models.embeddings.loader.WordVectorSerializer - First line is a header
Paramètre est un token spécial: false
Paramètre ajouté à combinedParameters (Encodeur): [[   -0.0844,    0.0617,    0.0832,  ...    0.2607,   -0.0019,   -0.2141],
 [   -0.0598,   -0.0027,    0.0698,  ...    0.0310,    0.0580,   -0.0314],
 [   -0.0435,   -0.1681,   -0.2886,  ...    0.1426,   -0.2793,   -0.1692],
  ...,
 [   -0.0945,    0.0144,    0.0828,  ...    0.1059,    0.2320,    0.0645],
 [   -0.0422,    0.1751,    0.0666,  ...   -0.0298,   -0.0664,    0.1427],
 [   -0.1967,    0.0931,    0.0176,  ...   -0.0824,    0.0331,   -0.0134]]
Paramètre est un token spécial: false
Paramètre ajouté à combinedParameters (Encodeur): [[   -0.0418,    0.1887,   -0.1221,  ...   -0.0927,   -0.0043,   -0.0293],
 [    0.0334,   -0.0700,   -0.0917,  ...    0.0309,   -0.0800,   -0.0830],
 [   -0.0866,   -0.1075,    0.0078,  ...   -0.1952,   -0.0439,    0.1055],
  ...,
 [    0.0752,    0.0066,    0.1763,  ...    0.1281,   -0.1616,   -0.1329],
 [   -0.1547,   -0.0322,   -0.0624,  ...   -0.0370,    0.0801,   -0.2383],
 [   -0.0238,    0.0577,    0.1954,  ...    0.0373,    0.1351,   -0.0928]]
Paramètre est un token spécial: false
Paramètre ajouté à combinedParameters (Encodeur): [[    0.1290,    0.0948,    0.0410,  ...    0.0488,   -0.0887,   -0.0241],
 [   -0.0023,   -0.0150,   -0.1110,  ...    0.0575,   -0.1892,   -0.1236],
 [    0.0872,    0.1642,   -0.0988,  ...   -0.1794,   -0.0397,    0.1258],
  ...,
 [   -0.1205,    0.2002,   -0.0166,  ...    0.0515,    0.1153,   -0.0093],
 [   -0.0047,   -0.1291,    0.2472,  ...   -0.1375,    0.0950,   -0.1108],
 [   -0.1345,    0.0086,    0.1330,  ...   -0.0219,   -0.1269,    0.0798]]
Paramètre est un token spécial: false
Paramètre ajouté à combinedParameters (Encodeur): [[   -0.0608,   -0.0628,   -0.2419,  ...    0.0075,    0.0386,    0.0894],
 [   -0.1272,    0.0677,   -0.0643,  ...    0.0768,    0.1176,    0.2742],
 [   -0.1676,   -0.0827,    0.0291,  ...   -0.2926,   -0.1570,   -0.0099],
  ...,
 [    0.1905,    0.0946,   -0.0330,  ...   -0.1010,   -0.0181,    0.1454],
 [   -0.1203,   -0.0449,   -0.2742,  ...   -0.0747,    0.0403,    0.3371],
 [    0.1208,   -0.1043,   -0.2368,  ...   -0.1065,    0.0556,   -0.1198]]
Paramètre est un token spécial: false
Paramètre ajouté à combinedParameters (Encodeur): [[    0.0249,   -0.4292,    0.2069,  ...   -0.1668,    0.0059,   -0.1176],
 [    0.1876,    0.0300,   -0.0901,  ...   -0.1398,   -0.3371,    0.0138],
 [    0.1198,   -0.0411,   -0.1106,  ...   -0.1122,    0.1205,   -0.3702],
  ...,
 [    0.1816,    0.0796,    0.1188,  ...   -0.1188,   -0.1211,   -0.2343],
 [   -0.1902,   -0.0612,   -0.0720,  ...   -0.1866,    0.1635,   -0.3122],
 [   -0.2629,    0.0642,    0.0608,  ...    0.1750,   -0.3786,   -0.2601]]
Paramètre est un token spécial: false
Paramètre ajouté à combinedParameters (Encodeur): [[         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0]]
Paramètre est un token spécial: false
Paramètre ajouté à combinedParameters (Encodeur): [[   -0.0199,   -0.0250,    0.0152,  ...   -0.0117,    0.0033,    0.0456],
 [   -0.0546,    0.0654,    0.0463,  ...   -0.0079,   -0.0105,    0.0759],
 [    0.0209,   -0.0182,    0.0002,  ...    0.0476,   -0.0492,    0.0230],
  ...,
 [   -0.0506,   -0.0048,   -0.0535,  ...    0.1078,   -0.0328,    0.0808],
 [   -0.0062,    0.0457,    0.0654,  ...    0.0357,   -0.0469,   -0.0273],
 [    0.0372,    0.0158,   -0.0206,  ...    0.0246,   -0.0028,    0.0164]]
Paramètre est un token spécial: false
Paramètre ajouté à combinedParameters (Encodeur): [[         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0]]
Paramètre est un token spécial: false
Paramètre ajouté à combinedParameters (Encodeur): [[[    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000]]]
Paramètre est un token spécial: false
Paramètre ajouté à combinedParameters (Encodeur): [[[         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0]]]
Paramètre est un token spécial: false
Paramètre ajouté à combinedParameters (Encodeur): [[[    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000]]]
Paramètre est un token spécial: false
Paramètre ajouté à combinedParameters (Encodeur): [[[         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0]]]
Paramètre est un token spécial: false
Paramètre ajouté à combinedParameters (Encodeur): [[[    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000]]]
Paramètre est un token spécial: false
Paramètre ajouté à combinedParameters (Encodeur): [[[         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0]]]
Paramètre est un token spécial: false
Paramètre ajouté à combinedParameters (Décodeur): [[    0.1561,    0.0973,   -0.0319,  ...   -0.1603,   -0.1063,    0.1173],
 [    0.1456,    0.0854,   -0.0937,  ...    0.1186,    0.2199,   -0.1933],
 [    0.0728,    0.0930,    0.3107,  ...   -0.1643,-2.6131e-5,   -0.0289],
  ...,
 [   -0.0479,   -0.0128,    0.0957,  ...    0.0109,    0.1189,    0.2280],
 [    0.1106,   -0.1168,   -0.0572,  ...   -0.0065,    0.0020,   -0.1381],
 [    0.0380,   -0.1865,   -0.1810,  ...   -0.1268,    0.1951,    0.0505]]
Paramètre est un token spécial: false
Paramètre ajouté à combinedParameters (Décodeur): [[    0.2069,    0.1457,    0.0401,  ...    0.0886,    0.0004,   -0.1718],
 [   -0.4065,   -0.0842,   -0.1545,  ...   -0.0476,   -0.0734,   -0.2267],
 [    0.2054,    0.0351,   -0.0111,  ...    0.1434,    0.0288,   -0.0868],
  ...,
 [    0.3289,    0.1696,   -0.1066,  ...   -0.0810,    0.1848,    0.0802],
 [   -0.0850,    0.0413,    0.0753,  ...    0.1113,    0.0689,   -0.0536],
 [    0.1573,   -0.0396,    0.2022,  ...   -0.1060,   -0.0303,    0.0842]]
Paramètre est un token spécial: false
Paramètre ajouté à combinedParameters (Décodeur): [[   -0.1605,    0.1314,   -0.0715,  ...    0.0688,    0.0643,   -0.0518],
 [   -0.0684,   -0.1979,    0.0007,  ...    0.0140,    0.3018,   -0.0310],
 [   -0.0889,    0.0930,   -0.0275,  ...    0.0408,   -0.1881,   -0.0479],
  ...,
 [   -0.1495,    0.0358,    0.2257,  ...    0.0902,    0.2786,   -0.0942],
 [   -0.0852,    0.1979,   -0.0579,  ...   -0.0468,   -0.0623,   -0.0485],
 [   -0.1274,    0.0026,   -0.0728,  ...   -0.1759,   -0.0453,    0.0355]]
Paramètre est un token spécial: false
Paramètre ajouté à combinedParameters (Décodeur): [[   -0.2300,   -0.0997,    0.1170,  ...   -0.1041,   -0.0012,   -0.1104],
 [   -0.0976,   -0.0360,    0.0024,  ...    0.0147,    0.2216,    0.1029],
 [   -0.0825,   -0.0488,   -0.2216,  ...    0.0773,    0.1875,   -0.0369],
  ...,
 [    0.2267,   -0.0670,    0.2562,  ...    0.0739,    0.0887,   -0.2035],
 [    0.0232,    0.0309,    0.1793,  ...   -0.0709,   -0.0140,    0.0069],
 [    0.0806,   -0.1119,    0.1337,  ...   -0.0516,   -0.1859,   -0.0347]]
Paramètre est un token spécial: false
Paramètre ajouté à combinedParameters (Décodeur): [[   -0.0889,    0.1010,   -0.1870,  ...   -0.1630,   -0.0076,   -0.3015],
 [   -0.1282,   -0.0167,    0.1658,  ...   -0.3012,   -0.0772,    0.2729],
 [    0.3699,   -0.0650,    0.1525,  ...    0.0362,    0.0972,    0.0746],
  ...,
 [   -0.0118,    0.1907,    0.2561,  ...    0.1049,    0.1494,   -0.0998],
 [    0.0416,    0.2106,    0.1682,  ...   -0.1928,   -0.1448,    0.1705],
 [   -0.0523,   -0.0641,    0.0807,  ...    0.2252,    0.0507,    0.0010]]
Paramètre est un token spécial: false
Paramètre ajouté à combinedParameters (Décodeur): [[   -0.3217,    0.0046,    0.0335,  ...    0.1202,    0.2827,   -0.0871],
 [   -0.0086,   -0.2510,    0.0854,  ...    0.0188,   -0.1295,    0.0699],
 [    0.1266,    0.1317,   -0.2455,  ...   -0.0710,   -0.1902,   -0.0079],
  ...,
 [   -0.1854,    0.1517,   -0.0312,  ...    0.1806,   -0.1768,    0.0046],
 [   -0.1004,    0.1428,    0.0142,  ...    0.2244,   -0.2605,   -0.0556],
 [    0.0893,   -0.1642,   -0.0910,  ...    0.0194,    0.0460,    0.0077]]
Paramètre est un token spécial: false
Paramètre ajouté à combinedParameters (Décodeur): [[    0.0339,   -0.2351,   -0.2140,  ...   -0.0877,    0.1424,   -0.0363],
 [    0.0648,    0.0612,   -0.2516,  ...   -0.1171,   -0.0237,    0.2187],
 [    0.0238,    0.0243,    0.0707,  ...    0.0824,    0.0585,   -0.1566],
  ...,
 [   -0.0258,    0.2363,   -0.0748,  ...    0.1770,    0.0347,   -0.1309],
 [    0.3317,   -0.1800,   -0.0987,  ...   -0.3325,    0.3201,   -0.0858],
 [   -0.1601,   -0.3314,    0.2477,  ...   -0.3392,   -0.0274,   -0.0025]]
Paramètre est un token spécial: false
Paramètre ajouté à combinedParameters (Décodeur): [[    0.1290,    0.0316,   -0.1389,  ...    0.2504,    0.1082,   -0.0213],
 [    0.0533,    0.0361,   -0.0318,  ...    0.1732,    0.0614,    0.0338],
 [    0.0164,    0.0440,   -0.1032,  ...   -0.2849,   -0.0610,   -0.2634],
  ...,
 [   -0.0938,    0.1099,   -0.0729,  ...   -0.0275,    0.0294,    0.0291],
 [   -0.0556,    0.2538,    0.0286,  ...    0.1424,   -0.1218,    0.2487],
 [   -0.0606,   -0.1851,    0.0202,  ...    0.0295,   -0.1996,   -0.1876]]
Paramètre est un token spécial: false
Paramètre ajouté à combinedParameters (Décodeur): [[    0.2070,   -0.1129,   -0.1752,  ...   -0.1623,   -0.0626,   -0.0790],
 [    0.0698,    0.1671,    0.2927,  ...   -0.2586,    0.2823,    0.1265],
 [    0.1299,    0.1974,    0.0438,  ...   -0.3001,    0.1481,   -0.1029],
  ...,
 [   -0.2438,    0.1536,   -0.3168,  ...   -0.0993,    0.1363,   -0.0080],
 [   -0.1310,   -0.2213,   -0.0441,  ...    0.3004,   -0.0652,    0.0888],
 [    0.2819,   -0.2780,   -0.0137,  ...   -0.1382,   -0.1021,   -0.2935]]
Paramètre est un token spécial: false
Paramètre ajouté à combinedParameters (Décodeur): [[         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0]]
Paramètre est un token spécial: false
Paramètre ajouté à combinedParameters (Décodeur): [[    0.0047,   -0.0374,   -0.0297,  ...   -0.0402,   -0.0645,    0.0095],
 [    0.0272,    0.0708,    0.0395,  ...    0.0092,    0.0084,    0.0574],
 [   -0.0501,    0.0130,   -0.0458,  ...    0.0782,   -0.0410,    0.0275],
  ...,
 [    0.0319,    0.0394,   -0.0302,  ...   -0.0401,    0.0518,   -0.0411],
 [   -0.0191,   -0.0273,   -0.0624,  ...   -0.0462,    0.0249,   -0.0205],
 [   -0.0358,    0.0083,   -0.0078,  ...   -0.0388,   -0.0030,   -0.0018]]
Paramètre est un token spécial: false
Paramètre ajouté à combinedParameters (Décodeur): [[         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0]]
Paramètre est un token spécial: false
Paramètre ajouté à combinedParameters (Décodeur): [[[    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000]]]
Paramètre est un token spécial: false
Paramètre ajouté à combinedParameters (Décodeur): [[[         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0]]]
Paramètre est un token spécial: false
Paramètre ajouté à combinedParameters (Décodeur): [[[    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000]]]
Paramètre est un token spécial: false
Paramètre ajouté à combinedParameters (Décodeur): [[[         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0]]]
Paramètre est un token spécial: false
Paramètre ajouté à combinedParameters (Décodeur): [[[    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000]]]
Paramètre est un token spécial: false
Paramètre ajouté à combinedParameters (Décodeur): [[[         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0]]]
Paramètre est un token spécial: false
Paramètre ajouté à combinedParameters (Décodeur): [[[    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000]]]
Paramètre est un token spécial: false
Paramètre ajouté à combinedParameters (Décodeur): [[[         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0]]]
Paramètre est un token spécial: false
Paramètre ajouté à combinedParameters (Décodeur): [[   -0.0956,   -0.0906,   -0.0901,  ...   -0.1833,    0.0285,   -0.1271],
 [   -0.1823,    0.1664,    0.0923,  ...    0.3075,    0.0496,    0.1134],
 [   -0.1719,   -0.0329,    0.0501,  ...    0.3060,    0.1670,    0.1512],
  ...,
 [    0.1485,   -0.0626,    0.0794,  ...   -0.2123,   -0.1243,    0.1419],
 [   -0.1296,    0.0630,   -0.0006,  ...    0.2086,   -0.1719,   -0.0862],
 [   -0.1038,   -0.1518,    0.1757,  ...   -0.0928,   -0.0096,   -0.0948]]
Paramètre est un token spécial: false
Paramètre ajouté à combinedParameters (Décodeur): [[         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0]]
Paramètre est un token spécial: false
Paramètre ajouté à combinedParameters (Décodeur): [[    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000,    1.0000]]
Paramètre est un token spécial: false
Paramètre ajouté à combinedParameters (Décodeur): [[         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0,         0]]
Paramètre est un token spécial: false
Paramètre ajouté à combinedParameters (Tokenizer): [[         0,         0,         0,  ...         0,         0,         0],
 [    0.0008,-8.8538e-5,    0.0002,  ...    0.0026,    0.0005,    0.0004],
 [   -0.1747,   -0.0991,    0.1625,  ...    0.1007,   -0.1520,    0.2564],
  ...,
 [    0.0091,   -0.0011,   -0.0031,  ...    0.0006,   -0.0032,   -0.0017],
 [   -0.0070,    0.0028,   -0.0084,  ...    0.0106,   -0.0017,    0.0087],
 [   -0.0021,    0.0041,   -0.0021,  ...   -0.0018,   -0.0044,   -0.0078]]
Token ID 0 embedding was frozen.
Token ID 2 embedding was frozen.
Token ID 3 embedding was frozen.
<UNK> embedding has calculated values. Skipping freezing.
Texte: 'chat mange la souris'
Tokens: [<START>, chat, mange, la, souris, <END>, <PAD>, <PAD>, <PAD>]
IDs: [2, 37, 13, 36, 34, 3, 0, 0, 0]
Texte: 'chien court dans le jardin'
Tokens: [<START>, chien, court, dans, le, jardin, <END>, <PAD>, <PAD>]
IDs: [2, 30, 28, 29, 41, 21, 3, 0, 0]
Texte: 'les chats aiment les chiens'
Tokens: [<START>, les, chats, aiment, les, chiens, <END>, <PAD>, <PAD>]
IDs: [2, 16, 39, 6, 16, 17, 3, 0, 0]
Texte: 'tapis sur le sol'
Tokens: [<START>, tapis, sur, le, sol, <END>, <PAD>, <PAD>, <PAD>]
IDs: [2, 18, 12, 41, 11, 3, 0, 0, 0]
Texte: 'ce film est fantastique'
Tokens: [<START>, ce, film, est, fantastique, <END>, <PAD>, <PAD>, <PAD>]
IDs: [2, 20, 27, 7, 26, 3, 0, 0, 0]
Texte: 'je déteste ce temps'
Tokens: [<START>, je, déteste, ce, temps, <END>, <PAD>, <PAD>, <PAD>]
IDs: [2, 43, 35, 20, 10, 3, 0, 0, 0]
Texte: 'quelle belle journée'
Tokens: [<START>, quelle, belle, journée, <END>, <PAD>, <PAD>, <PAD>, <PAD>]
IDs: [2, 38, 14, 9, 3, 0, 0, 0, 0]
Texte: 'le chat sol'
Tokens: [<START>, le, chat, sol, <END>, <PAD>, <PAD>, <PAD>, <PAD>]
IDs: [2, 41, 37, 11, 3, 0, 0, 0, 0]
Texte: 'c'est un mauvais film'
Tokens: [<START>, c, ', est, un, mauvais, film, <END>, <PAD>]
IDs: [2, 1, 1, 7, 15, 4, 27, 3, 0]
Texte: 'ce livre est intéressant'
Tokens: [<START>, ce, livre, est, intéressant, <END>, <PAD>, <PAD>, <PAD>]
IDs: [2, 20, 31, 7, 19, 3, 0, 0, 0]
Texte: 'je n'aime pas ce repas'
Tokens: [<START>, je, n, ', aime, pas, ce, repas, <END>]
IDs: [2, 43, 1, 1, 1, 22, 20, 40, 3]
Texte: 'ce film est excellent'
Tokens: [<START>, ce, film, est, excellent, <END>, <PAD>, <PAD>, <PAD>]
IDs: [2, 20, 27, 7, 24, 3, 0, 0, 0]
Texte: 'je suis triste aujourd'hui'
Tokens: [<START>, je, suis, triste, aujourd, ', hui, <END>, <PAD>]
IDs: [2, 43, 42, 33, 1, 1, 1, 3, 0]
Texte: 'ce temps est agréable'
Tokens: [<START>, ce, temps, est, agréable, <END>, <PAD>, <PAD>, <PAD>]
IDs: [2, 20, 10, 7, 8, 3, 0, 0, 0]
Texte: 'le chat mange'
Tokens: [<START>, le, chat, mange, <END>, <PAD>, <PAD>, <PAD>, <PAD>]
IDs: [2, 41, 37, 13, 3, 0, 0, 0, 0]
Texte: 'le chien court'
Tokens: [<START>, le, chien, court, <END>, <PAD>, <PAD>, <PAD>, <PAD>]
IDs: [2, 41, 30, 28, 3, 0, 0, 0, 0]
Texte: 'les chats aiment'
Tokens: [<START>, les, chats, aiment, <END>, <PAD>, <PAD>, <PAD>, <PAD>]
IDs: [2, 16, 39, 6, 3, 0, 0, 0, 0]
Texte: 'le tapis sur'
Tokens: [<START>, le, tapis, sur, <END>, <PAD>, <PAD>, <PAD>, <PAD>]
IDs: [2, 41, 18, 12, 3, 0, 0, 0, 0]
Texte: 'le film est fantastique'
Tokens: [<START>, le, film, est, fantastique, <END>, <PAD>, <PAD>, <PAD>]
IDs: [2, 41, 27, 7, 26, 3, 0, 0, 0]
Texte: 'je déteste'
Tokens: [<START>, je, déteste, <END>, <PAD>, <PAD>, <PAD>, <PAD>, <PAD>]
IDs: [2, 43, 35, 3, 0, 0, 0, 0, 0]
Texte: 'quelle belle'
Tokens: [<START>, quelle, belle, <END>, <PAD>, <PAD>, <PAD>, <PAD>, <PAD>]
IDs: [2, 38, 14, 3, 0, 0, 0, 0, 0]
Texte: 'le chat'
Tokens: [<START>, le, chat, <END>, <PAD>, <PAD>, <PAD>, <PAD>, <PAD>]
IDs: [2, 41, 37, 3, 0, 0, 0, 0, 0]
Texte: 'c'est un mauvais'
Tokens: [<START>, c, ', est, un, mauvais, <END>, <PAD>, <PAD>]
IDs: [2, 1, 1, 7, 15, 4, 3, 0, 0]
Texte: 'le livre est intéressant'
Tokens: [<START>, le, livre, est, intéressant, <END>, <PAD>, <PAD>, <PAD>]
IDs: [2, 41, 31, 7, 19, 3, 0, 0, 0]
Texte: 'je n'aime pas'
Tokens: [<START>, je, n, ', aime, pas, <END>, <PAD>, <PAD>]
IDs: [2, 43, 1, 1, 1, 22, 3, 0, 0]
Texte: 'le film est excellent'
Tokens: [<START>, le, film, est, excellent, <END>, <PAD>, <PAD>, <PAD>]
IDs: [2, 41, 27, 7, 24, 3, 0, 0, 0]
Texte: 'je suis triste'
Tokens: [<START>, je, suis, triste, <END>, <PAD>, <PAD>, <PAD>, <PAD>]
IDs: [2, 43, 42, 33, 3, 0, 0, 0, 0]
Texte: 'le temps est agréable'
Tokens: [<START>, le, temps, est, agréable, <END>, <PAD>, <PAD>, <PAD>]
IDs: [2, 41, 10, 7, 8, 3, 0, 0, 0]
Embedding Before:
[   -0.0051,    0.0081,   -0.0024,   -0.0027,    0.0019,    0.0077,   -0.0029,   -0.0010,   -0.0100,    0.0064,   -0.0031,    0.0047,   -0.0091,   -0.0008,    0.0052,   -0.0103,    0.0071,    0.0032,    0.0013,    0.0032,    0.0070,   -0.0066,    0.0042,    0.0065,    0.0016,   -0.0079,    0.0044,    0.0026,    0.0044,   -0.0084,   -0.0003,    0.0070,    0.0057,   -0.0077,    0.0017,    0.0092,    0.0066,    0.0006,   -0.0076,   -0.0048,    0.0028,    0.0064,    0.0049,    0.0037,    0.0039,    0.0095,   -0.0056,    0.0089,   -0.0072,    0.0071]
Epoch 1 / 3
Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0], 
   [0],
   [0]]]]
Attention Weights:
[[[[    0.1276,    0.1342,    0.1518,    0.1767,    0.1997,    0.2100,         0,         0,         0],
   [    0.1311,    0.1369,    0.1529,    0.1759,    0.1973,    0.2058,         0,         0,         0],
   [    0.1326,    0.1390,    0.1543,    0.1758,    0.1953,    0.2029,         0,         0,         0],
   [    0.1311,    0.1395,    0.1555,    0.1766,    0.1948,    0.2025,         0,         0,         0],
   [    0.1280,    0.1383,    0.1559,    0.1777,    0.1959,    0.2042,         0,         0,         0],
   [    0.1266,    0.1370,    0.1551,    0.1782,    0.1974,    0.2057,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
===== Échantillon 1 =====
===== Encoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        chat           mange          la             souris         <END>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,1276         0,1342         0,1518         0,1767         0,1997         0,2100         0,0000         0,0000         0,0000
chat           0,1311         0,1369         0,1529         0,1759         0,1973         0,2058         0,0000         0,0000         0,0000
mange          0,1326         0,1390         0,1543         0,1758         0,1953         0,2029         0,0000         0,0000         0,0000
la             0,1311         0,1395         0,1555         0,1766         0,1948         0,2025         0,0000         0,0000         0,0000
souris         0,1280         0,1383         0,1559         0,1777         0,1959         0,2042         0,0000         0,0000         0,0000
<END>          0,1266         0,1370         0,1551         0,1782         0,1974         0,2057         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Self-Attention Weights =====
Les poids d'attention ne sont pas disponibles. Effectuez d'abord une passe forward.
===== Decoder Layer 1 Cross-Attention Weights =====
Les poids d'attention ne sont pas disponibles. Effectuez d'abord une passe forward.

Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0], 
   [0], 
   [0],
   [0]]]]
Attention Weights:
[[[[    1.0000,         0,         0,         0,         0,         0,         0,         0,         0],
   [    0.4938,    0.5062,         0,         0,         0,         0,         0,         0,         0],
   [    0.3298,    0.3450,    0.3252,         0,         0,         0,         0,         0,         0],
   [    0.2555,    0.2731,    0.2571,    0.2144,         0,         0,         0,         0,         0],
   [    0.2164,    0.2348,    0.2232,    0.1839,    0.1417,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Attention Weights:
[[[[    0.2131,    0.1764,    0.1603,    0.1555,    0.1513,    0.1433,         0,         0,         0],
   [    0.2151,    0.1841,    0.1659,    0.1548,    0.1441,    0.1359,         0,         0,         0],
   [    0.2154,    0.1900,    0.1714,    0.1548,    0.1386,    0.1298,         0,         0,         0],
   [    0.2130,    0.1928,    0.1765,    0.1559,    0.1360,    0.1258,         0,         0,         0],
   [    0.2097,    0.1935,    0.1801,    0.1573,    0.1355,    0.1239,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Perte pour ce batch: 4.50466
Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0],
   [0]]]]
Attention Weights:
[[[[    0.1021,    0.1059,    0.1224,    0.1463,    0.1684,    0.1797,    0.1751,         0,         0],
   [    0.1062,    0.1090,    0.1241,    0.1460,    0.1664,    0.1765,    0.1718,         0,         0],
   [    0.1078,    0.1108,    0.1255,    0.1461,    0.1651,    0.1744,    0.1704,         0,         0],
   [    0.1066,    0.1114,    0.1268,    0.1466,    0.1644,    0.1732,    0.1710,         0,         0],
   [    0.1042,    0.1104,    0.1268,    0.1471,    0.1649,    0.1739,    0.1728,         0,         0],
   [    0.1033,    0.1095,    0.1262,    0.1475,    0.1659,    0.1750,    0.1726,         0,         0],
   [    0.1039,    0.1085,    0.1248,    0.1477,    0.1677,    0.1768,    0.1706,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
===== Échantillon 1 =====
===== Encoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        chien          court          dans           le             jardin         <END>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,1021         0,1059         0,1224         0,1463         0,1684         0,1797         0,1751         0,0000         0,0000
chien          0,1062         0,1090         0,1241         0,1460         0,1664         0,1765         0,1718         0,0000         0,0000
court          0,1078         0,1108         0,1255         0,1461         0,1651         0,1744         0,1704         0,0000         0,0000
dans           0,1066         0,1114         0,1268         0,1466         0,1644         0,1732         0,1710         0,0000         0,0000
le             0,1042         0,1104         0,1268         0,1471         0,1649         0,1739         0,1728         0,0000         0,0000
jardin         0,1033         0,1095         0,1262         0,1475         0,1659         0,1750         0,1726         0,0000         0,0000
<END>          0,1039         0,1085         0,1248         0,1477         0,1677         0,1768         0,1706         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        le             chien          court          <END>          <PAD>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        1,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
le             0,4938         0,5062         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
chien          0,3298         0,3450         0,3252         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
court          0,2555         0,2731         0,2571         0,2144         0,0000         0,0000         0,0000         0,0000         0,0000
<END>          0,2164         0,2348         0,2232         0,1839         0,1417         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Cross-Attention Weights =====
===== Tête 1 =====
Requête        <START>        chien          court          dans           le             jardin         <END>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,2131         0,1764         0,1603         0,1555         0,1513         0,1433         0,0000         0,0000         0,0000
le             0,2151         0,1841         0,1659         0,1548         0,1441         0,1359         0,0000         0,0000         0,0000
chien          0,2154         0,1900         0,1714         0,1548         0,1386         0,1298         0,0000         0,0000         0,0000
court          0,2130         0,1928         0,1765         0,1559         0,1360         0,1258         0,0000         0,0000         0,0000
<END>          0,2097         0,1935         0,1801         0,1573         0,1355         0,1239         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0], 
   [0], 
   [0],
   [0]]]]
Attention Weights:
[[[[    1.0000,         0,         0,         0,         0,         0,         0,         0,         0],
   [    0.4929,    0.5071,         0,         0,         0,         0,         0,         0,         0],
   [    0.3370,    0.3514,    0.3117,         0,         0,         0,         0,         0,         0],
   [    0.2741,    0.2891,    0.2536,    0.1832,         0,         0,         0,         0,         0],
   [    0.2394,    0.2560,    0.2250,    0.1618,    0.1178,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Attention Weights:
[[[[    0.1997,    0.1585,    0.1354,    0.1308,    0.1315,    0.1285,    0.1156,         0,         0],
   [    0.2003,    0.1642,    0.1393,    0.1299,    0.1266,    0.1233,    0.1163,         0,         0],
   [    0.2007,    0.1690,    0.1435,    0.1301,    0.1229,    0.1187,    0.1152,         0,         0],
   [    0.1976,    0.1714,    0.1478,    0.1324,    0.1219,    0.1160,    0.1128,         0,         0],
   [    0.1958,    0.1729,    0.1508,    0.1342,    0.1216,    0.1143,    0.1104,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Perte pour ce batch: 3.7855709
Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0],
   [0]]]]
Attention Weights:
[[[[    0.1020,    0.1054,    0.1214,    0.1453,    0.1684,    0.1815,    0.1759,         0,         0],
   [    0.1053,    0.1081,    0.1230,    0.1454,    0.1667,    0.1786,    0.1729,         0,         0],
   [    0.1077,    0.1105,    0.1244,    0.1453,    0.1650,    0.1761,    0.1711,         0,         0],
   [    0.1065,    0.1113,    0.1257,    0.1456,    0.1640,    0.1751,    0.1719,         0,         0],
   [    0.1038,    0.1105,    0.1260,    0.1462,    0.1644,    0.1757,    0.1734,         0,         0],
   [    0.1031,    0.1098,    0.1255,    0.1464,    0.1654,    0.1767,    0.1731,         0,         0],
   [    0.1037,    0.1087,    0.1241,    0.1467,    0.1674,    0.1783,    0.1712,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
===== Échantillon 1 =====
===== Encoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        les            chats          aiment         les            chiens         <END>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,1020         0,1054         0,1214         0,1453         0,1684         0,1815         0,1759         0,0000         0,0000
les            0,1053         0,1081         0,1230         0,1454         0,1667         0,1786         0,1729         0,0000         0,0000
chats          0,1077         0,1105         0,1244         0,1453         0,1650         0,1761         0,1711         0,0000         0,0000
aiment         0,1065         0,1113         0,1257         0,1456         0,1640         0,1751         0,1719         0,0000         0,0000
les            0,1038         0,1105         0,1260         0,1462         0,1644         0,1757         0,1734         0,0000         0,0000
chiens         0,1031         0,1098         0,1255         0,1464         0,1654         0,1767         0,1731         0,0000         0,0000
<END>          0,1037         0,1087         0,1241         0,1467         0,1674         0,1783         0,1712         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        les            chats          aiment         <END>          <PAD>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        1,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
les            0,4929         0,5071         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
chats          0,3370         0,3514         0,3117         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
aiment         0,2741         0,2891         0,2536         0,1832         0,0000         0,0000         0,0000         0,0000         0,0000
<END>          0,2394         0,2560         0,2250         0,1618         0,1178         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Cross-Attention Weights =====
===== Tête 1 =====
Requête        <START>        les            chats          aiment         les            chiens         <END>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,1997         0,1585         0,1354         0,1308         0,1315         0,1285         0,1156         0,0000         0,0000
les            0,2003         0,1642         0,1393         0,1299         0,1266         0,1233         0,1163         0,0000         0,0000
chats          0,2007         0,1690         0,1435         0,1301         0,1229         0,1187         0,1152         0,0000         0,0000
aiment         0,1976         0,1714         0,1478         0,1324         0,1219         0,1160         0,1128         0,0000         0,0000
<END>          0,1958         0,1729         0,1508         0,1342         0,1216         0,1143         0,1104         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0], 
   [0], 
   [0],
   [0]]]]
Attention Weights:
[[[[    1.0000,         0,         0,         0,         0,         0,         0,         0,         0],
   [    0.4902,    0.5098,         0,         0,         0,         0,         0,         0,         0],
   [    0.3351,    0.3544,    0.3105,         0,         0,         0,         0,         0,         0],
   [    0.2734,    0.2920,    0.2535,    0.1811,         0,         0,         0,         0,         0],
   [    0.2405,    0.2590,    0.2253,    0.1608,    0.1144,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Attention Weights:
[[[[    0.2007,    0.1591,    0.1352,    0.1295,    0.1326,    0.1273,    0.1156,         0,         0],
   [    0.2019,    0.1652,    0.1390,    0.1285,    0.1274,    0.1218,    0.1161,         0,         0],
   [    0.2010,    0.1691,    0.1431,    0.1291,    0.1244,    0.1180,    0.1153,         0,         0],
   [    0.1987,    0.1716,    0.1476,    0.1309,    0.1228,    0.1153,    0.1130,         0,         0],
   [    0.1964,    0.1728,    0.1512,    0.1329,    0.1224,    0.1138,    0.1106,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Perte pour ce batch: 3.843824
Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0], 
   [0],
   [0]]]]
Attention Weights:
[[[[    0.1229,    0.1278,    0.1460,    0.1764,    0.2062,    0.2207,         0,         0,         0],
   [    0.1268,    0.1307,    0.1475,    0.1757,    0.2032,    0.2161,         0,         0,         0],
   [    0.1289,    0.1332,    0.1495,    0.1756,    0.2005,    0.2124,         0,         0,         0],
   [    0.1275,    0.1339,    0.1511,    0.1765,    0.1996,    0.2115,         0,         0,         0],
   [    0.1254,    0.1335,    0.1518,    0.1774,    0.1999,    0.2121,         0,         0,         0],
   [    0.1241,    0.1325,    0.1511,    0.1778,    0.2013,    0.2132,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
===== Échantillon 1 =====
===== Encoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        tapis          sur            le             sol            <END>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,1229         0,1278         0,1460         0,1764         0,2062         0,2207         0,0000         0,0000         0,0000
tapis          0,1268         0,1307         0,1475         0,1757         0,2032         0,2161         0,0000         0,0000         0,0000
sur            0,1289         0,1332         0,1495         0,1756         0,2005         0,2124         0,0000         0,0000         0,0000
le             0,1275         0,1339         0,1511         0,1765         0,1996         0,2115         0,0000         0,0000         0,0000
sol            0,1254         0,1335         0,1518         0,1774         0,1999         0,2121         0,0000         0,0000         0,0000
<END>          0,1241         0,1325         0,1511         0,1778         0,2013         0,2132         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        le             tapis          sur            <END>          <PAD>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        1,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
le             0,4902         0,5098         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
tapis          0,3351         0,3544         0,3105         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
sur            0,2734         0,2920         0,2535         0,1811         0,0000         0,0000         0,0000         0,0000         0,0000
<END>          0,2405         0,2590         0,2253         0,1608         0,1144         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Cross-Attention Weights =====
===== Tête 1 =====
Requête        <START>        tapis          sur            le             sol            <END>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,2007         0,1591         0,1352         0,1295         0,1326         0,1273         0,1156         0,0000         0,0000
le             0,2019         0,1652         0,1390         0,1285         0,1274         0,1218         0,1161         0,0000         0,0000
tapis          0,2010         0,1691         0,1431         0,1291         0,1244         0,1180         0,1153         0,0000         0,0000
sur            0,1987         0,1716         0,1476         0,1309         0,1228         0,1153         0,1130         0,0000         0,0000
<END>          0,1964         0,1728         0,1512         0,1329         0,1224         0,1138         0,1106         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0], 
   [0], 
   [0],
   [0]]]]
Attention Weights:
[[[[    1.0000,         0,         0,         0,         0,         0,         0,         0,         0],
   [    0.4879,    0.5121,         0,         0,         0,         0,         0,         0,         0],
   [    0.3357,    0.3567,    0.3077,         0,         0,         0,         0,         0,         0],
   [    0.2756,    0.2954,    0.2527,    0.1763,         0,         0,         0,         0,         0],
   [    0.2422,    0.2632,    0.2268,    0.1579,    0.1098,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Attention Weights:
[[[[    0.2295,    0.1813,    0.1543,    0.1456,    0.1462,    0.1431,         0,         0,         0],
   [    0.2293,    0.1873,    0.1587,    0.1455,    0.1413,    0.1378,         0,         0,         0],
   [    0.2290,    0.1920,    0.1629,    0.1457,    0.1374,    0.1331,         0,         0,         0],
   [    0.2252,    0.1942,    0.1675,    0.1473,    0.1360,    0.1297,         0,         0,         0],
   [    0.2226,    0.1957,    0.1709,    0.1483,    0.1351,    0.1273,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Perte pour ce batch: 4.7588916
Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0], 
   [0],
   [0]]]]
Attention Weights:
[[[[    0.1216,    0.1253,    0.1466,    0.1756,    0.2082,    0.2227,         0,         0,         0],
   [    0.1254,    0.1281,    0.1483,    0.1751,    0.2052,    0.2179,         0,         0,         0],
   [    0.1274,    0.1303,    0.1499,    0.1752,    0.2028,    0.2144,         0,         0,         0],
   [    0.1267,    0.1318,    0.1518,    0.1759,    0.2013,    0.2126,         0,         0,         0],
   [    0.1243,    0.1315,    0.1524,    0.1767,    0.2016,    0.2135,         0,         0,         0],
   [    0.1231,    0.1306,    0.1518,    0.1770,    0.2028,    0.2146,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
===== Échantillon 1 =====
===== Encoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        ce             film           est            fantastique    <END>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,1216         0,1253         0,1466         0,1756         0,2082         0,2227         0,0000         0,0000         0,0000
ce             0,1254         0,1281         0,1483         0,1751         0,2052         0,2179         0,0000         0,0000         0,0000
film           0,1274         0,1303         0,1499         0,1752         0,2028         0,2144         0,0000         0,0000         0,0000
est            0,1267         0,1318         0,1518         0,1759         0,2013         0,2126         0,0000         0,0000         0,0000
fantastique    0,1243         0,1315         0,1524         0,1767         0,2016         0,2135         0,0000         0,0000         0,0000
<END>          0,1231         0,1306         0,1518         0,1770         0,2028         0,2146         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        le             film           est            fantastique    <END>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        1,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
le             0,4879         0,5121         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
film           0,3357         0,3567         0,3077         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
est            0,2756         0,2954         0,2527         0,1763         0,0000         0,0000         0,0000         0,0000         0,0000
fantastique    0,2422         0,2632         0,2268         0,1579         0,1098         0,0000         0,0000         0,0000         0,0000
<END>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Cross-Attention Weights =====
===== Tête 1 =====
Requête        <START>        ce             film           est            fantastique    <END>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,2295         0,1813         0,1543         0,1456         0,1462         0,1431         0,0000         0,0000         0,0000
le             0,2293         0,1873         0,1587         0,1455         0,1413         0,1378         0,0000         0,0000         0,0000
film           0,2290         0,1920         0,1629         0,1457         0,1374         0,1331         0,0000         0,0000         0,0000
est            0,2252         0,1942         0,1675         0,1473         0,1360         0,1297         0,0000         0,0000         0,0000
fantastique    0,2226         0,1957         0,1709         0,1483         0,1351         0,1273         0,0000         0,0000         0,0000
<END>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0], 
   [0],
   [0]]]]
Attention Weights:
[[[[    1.0000,         0,         0,         0,         0,         0,         0,         0,         0],
   [    0.4862,    0.5138,         0,         0,         0,         0,         0,         0,         0],
   [    0.3344,    0.3576,    0.3080,         0,         0,         0,         0,         0,         0],
   [    0.2761,    0.2983,    0.2535,    0.1721,         0,         0,         0,         0,         0],
   [    0.2451,    0.2688,    0.2279,    0.1544,    0.1038,         0,         0,         0,         0],
   [    0.2180,    0.2440,    0.2093,    0.1427,    0.0960,    0.0901,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Attention Weights:
[[[[    0.2326,    0.1816,    0.1515,    0.1448,    0.1450,    0.1445,         0,         0,         0],
   [    0.2315,    0.1872,    0.1559,    0.1450,    0.1405,    0.1398,         0,         0,         0],
   [    0.2310,    0.1920,    0.1602,    0.1454,    0.1366,    0.1348,         0,         0,         0],
   [    0.2269,    0.1939,    0.1645,    0.1473,    0.1356,    0.1318,         0,         0,         0],
   [    0.2248,    0.1952,    0.1674,    0.1483,    0.1348,    0.1295,         0,         0,         0],
   [    0.2233,    0.1965,    0.1695,    0.1490,    0.1340,    0.1277,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Perte pour ce batch: 4.084125
Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0], 
   [0],
   [0]]]]
Attention Weights:
[[[[    0.1202,    0.1232,    0.1447,    0.1755,    0.2102,    0.2262,         0,         0,         0],
   [    0.1243,    0.1262,    0.1461,    0.1746,    0.2074,    0.2215,         0,         0,         0],
   [    0.1264,    0.1287,    0.1479,    0.1742,    0.2049,    0.2179,         0,         0,         0],
   [    0.1252,    0.1301,    0.1499,    0.1752,    0.2035,    0.2161,         0,         0,         0],
   [    0.1232,    0.1300,    0.1507,    0.1761,    0.2036,    0.2163,         0,         0,         0],
   [    0.1221,    0.1291,    0.1502,    0.1766,    0.2046,    0.2173,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
===== Échantillon 1 =====
===== Encoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        je             déteste        ce             temps          <END>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,1202         0,1232         0,1447         0,1755         0,2102         0,2262         0,0000         0,0000         0,0000
je             0,1243         0,1262         0,1461         0,1746         0,2074         0,2215         0,0000         0,0000         0,0000
déteste        0,1264         0,1287         0,1479         0,1742         0,2049         0,2179         0,0000         0,0000         0,0000
ce             0,1252         0,1301         0,1499         0,1752         0,2035         0,2161         0,0000         0,0000         0,0000
temps          0,1232         0,1300         0,1507         0,1761         0,2036         0,2163         0,0000         0,0000         0,0000
<END>          0,1221         0,1291         0,1502         0,1766         0,2046         0,2173         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        je             déteste        <END>          <PAD>          <PAD>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        1,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
je             0,4862         0,5138         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
déteste        0,3344         0,3576         0,3080         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<END>          0,2761         0,2983         0,2535         0,1721         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,2451         0,2688         0,2279         0,1544         0,1038         0,0000         0,0000         0,0000         0,0000
<PAD>          0,2180         0,2440         0,2093         0,1427         0,0960         0,0901         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Cross-Attention Weights =====
===== Tête 1 =====
Requête        <START>        je             déteste        ce             temps          <END>          <PAD>          <PAD>          <PAD>
------------------------------
------------------------------
------------------------------------------------------------------------------------------
<START>        0,2326         0,1816         0,1515         0,1448         0,1450         0,1445         0,0000         0,0000         0,0000
je             0,2315         0,1872         0,1559         0,1450         0,1405         0,1398         0,0000         0,0000         0,0000
déteste        0,2310         0,1920         0,1602         0,1454         0,1366         0,1348         0,0000         0,0000         0,0000
<END>          0,2269         0,1939         0,1645         0,1473         0,1356         0,1318         0,0000         0,0000         0,0000
<PAD>          0,2248         0,1952         0,1674         0,1483         0,1348         0,1295         0,0000         0,0000         0,0000
<PAD>          0,2233         0,1965         0,1695         0,1490         0,1340         0,1277         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0], 
   [0], 
   [0], 
   [0],
   [0]]]]
Attention Weights:
[[[[    1.0000,         0,         0,         0,         0,         0,         0,         0,         0],
   [    0.4810,    0.5190,         0,         0,         0,         0,         0,         0,         0],
   [    0.3327,    0.3629,    0.3043,         0,         0,         0,         0,         0,         0],
   [    0.2789,    0.3066,    0.2533,    0.1612,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Attention Weights:
[[[[    0.2354,    0.1814,    0.1482,    0.1419,    0.1462,    0.1470,         0,         0,         0],
   [    0.2346,    0.1863,    0.1518,    0.1423,    0.1422,    0.1428,         0,         0,         0],
   [    0.2328,    0.1901,    0.1559,    0.1433,    0.1392,    0.1387,         0,         0,         0],
   [    0.2280,    0.1914,    0.1601,    0.1456,    0.1387,    0.1361,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Perte pour ce batch: 3.9539554
Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0], 
   [0], 
   [0],
   [0]]]]
Attention Weights:
[[[[    0.1531,    0.1588,    0.1845,    0.2298,    0.2737,         0,         0,         0,         0],
   [    0.1573,    0.1618,    0.1850,    0.2274,    0.2685,         0,         0,         0,         0],
   [    0.1593,    0.1646,    0.1868,    0.2260,    0.2633,         0,         0,         0,         0],
   [    0.1581,    0.1659,    0.1889,    0.2263,    0.2607,         0,         0,         0,         0],
   [    0.1557,    0.1659,    0.1905,    0.2274,    0.2605,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
===== Échantillon 1 =====
===== Encoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        quelle         belle          journée        <END>          <PAD>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,1531         0,1588         0,1845         0,2298         0,2737         0,0000         0,0000         0,0000         0,0000
quelle         0,1573         0,1618         0,1850         0,2274         0,2685         0,0000         0,0000         0,0000         0,0000
belle          0,1593         0,1646         0,1868         0,2260         0,2633         0,0000         0,0000         0,0000         0,0000
journée        0,1581         0,1659         0,1889         0,2263         0,2607         0,0000         0,0000         0,0000         0,0000
<END>          0,1557         0,1659         0,1905         0,2274         0,2605         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        quelle         belle          <END>          <PAD>          <PAD>          <PAD>          <PAD>          <PAD>
-------------------------
-------------------------
----------------------------------------------------------------------------------------------------
<START>        1,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
quelle         0,4810         0,5190         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
belle          0,3327         0,3629         0,3043         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<END>          0,2789         0,3066         0,2533         0,1612         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Cross-Attention Weights =====
===== Tête 1 =====
Requête        <START>        quelle         belle          journée        <END>          <PAD>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,2354         0,1814         0,1482         0,1419         0,1462         0,1470         0,0000         0,0000         0,0000
quelle         0,2346         0,1863         0,1518         0,1423         0,1422         0,1428         0,0000         0,0000         0,0000
belle          0,2328         0,1901         0,1559         0,1433         0,1392         0,1387         0,0000         0,0000         0,0000
<END>          0,2280         0,1914         0,1601         0,1456         0,1387         0,1361         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0], 
   [0], 
   [0], 
   [0],
   [0]]]]
Attention Weights:
[[[[    1.0000,         0,         0,         0,         0,         0,         0,         0,         0],
   [    0.4794,    0.5206,         0,         0,         0,         0,         0,         0,         0],
   [    0.3341,    0.3656,    0.3003,         0,         0,         0,         0,         0,         0],
   [    0.2823,    0.3124,    0.2532,    0.1521,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Attention Weights:
[[[[    0.2735,    0.2137,    0.1748,    0.1651,    0.1728,         0,         0,         0,         0],
   [    0.2707,    0.2169,    0.1780,    0.1654,    0.1691,         0,         0,         0,         0],
   [    0.2672,    0.2200,    0.1819,    0.1658,    0.1651,         0,         0,         0,         0],
   [    0.2617,    0.2216,    0.1858,    0.1675,    0.1634,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Perte pour ce batch: 4.1415043
Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0], 
   [0], 
   [0],
   [0]]]]
Attention Weights:
[[[[    0.1527,    0.1565,    0.1832,    0.2292,    0.2785,         0,         0,         0,         0],
   [    0.1568,    0.1593,    0.1839,    0.2270,    0.2730,         0,         0,         0,         0],
   [    0.1586,    0.1618,    0.1855,    0.2258,    0.2683,         0,         0,         0,         0],
   [    0.1581,    0.1640,    0.1878,    0.2256,    0.2647,         0,         0,         0,         0],
   [    0.1558,    0.1645,    0.1895,    0.2265,    0.2638,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
===== Échantillon 1 =====
===== Encoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        le             chat           sol            <END>          <PAD>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,1527         0,1565         0,1832         0,2292         0,2785         0,0000         0,0000         0,0000         0,0000
le             0,1568         0,1593         0,1839         0,2270         0,2730         0,0000         0,0000         0,0000         0,0000
chat           0,1586         0,1618         0,1855         0,2258         0,2683         0,0000         0,0000         0,0000         0,0000
sol            0,1581         0,1640         0,1878         0,2256         0,2647         0,0000         0,0000         0,0000         0,0000
<END>          0,1558         0,1645         0,1895         0,2265         0,2638         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        le             chat           <END>          <PAD>          <PAD>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        1,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
le             0,4794         0,5206         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
chat           0,3341         0,3656         0,3003         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<END>          0,2823         0,3124         0,2532         0,1521         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Cross-Attention Weights =====
===== Tête 1 =====
Requête        <START>        le             chat           sol            <END>          <PAD>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,2735         0,2137         0,1748         0,1651         0,1728         0,0000         0,0000         0,0000         0,0000
le             0,2707         0,2169         0,1780         0,1654         0,1691         0,0000         0,0000         0,0000         0,0000
chat           0,2672         0,2200         0,1819         0,1658         0,1651         0,0000         0,0000         0,0000         0,0000
<END>          0,2617         0,2216         0,1858         0,1675         0,1634         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0], 
   [0], 
   [0], 
   [0],
   [0]]]]
Attention Weights:
[[[[    1.0000,         0,         0,         0,         0,         0,         0,         0,         0],
   [    0.4765,    0.5235,         0,         0,         0,         0,         0,         0,         0],
   [    0.3344,    0.3697,    0.2959,         0,         0,         0,         0,         0,         0],
   [    0.2865,    0.3200,    0.2524,    0.1410,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Attention Weights:
[[[[    0.2760,    0.2110,    0.1739,    0.1655,    0.1737,         0,         0,         0,         0],
   [    0.2723,    0.2137,    0.1770,    0.1660,    0.1710,         0,         0,         0,         0],
   [    0.2682,    0.2161,    0.1806,    0.1669,    0.1682,         0,         0,         0,         0],
   [    0.2631,    0.2173,    0.1842,    0.1686,    0.1668,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Perte pour ce batch: 3.5268984
Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0]]]]
Attention Weights:
[[[[    0.0817,    0.0833,    0.0976,    0.1229,    0.1520,    0.1639,    0.1589,    0.1397,         0],
   [    0.0851,    0.0860,    0.0992,    0.1233,    0.1507,    0.1615,    0.1564,    0.1379,         0],
   [    0.0870,    0.0882,    0.1010,    0.1236,    0.1487,    0.1589,    0.1547,    0.1378,         0],
   [    0.0868,    0.0896,    0.1026,    0.1241,    0.1466,    0.1566,    0.1541,    0.1396,         0],
   [    0.0852,    0.0893,    0.1030,    0.1242,    0.1457,    0.1561,    0.1548,    0.1417,         0],
   [    0.0847,    0.0891,    0.1032,    0.1248,    0.1466,    0.1568,    0.1545,    0.1404,         0],
   [    0.0852,    0.0883,    0.1022,    0.1254,    0.1498,    0.1594,    0.1538,    0.1360,         0],
   [    0.0859,    0.0872,    0.1009,    0.1258,    0.1535,    0.1626,    0.1530,    0.1311,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
===== Échantillon 1 =====
===== Encoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        <UNK>          <UNK>          est            un             mauvais        film           <END>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,0817         0,0833         0,0976         0,1229         0,1520         0,1639         0,1589         0,1397         0,0000
<UNK>          0,0851         0,0860         0,0992         0,1233         0,1507         0,1615         0,1564         0,1379         0,0000
<UNK>          0,0870         0,0882         0,1010         0,1236         0,1487         0,1589         0,1547         0,1378         0,0000
est            0,0868         0,0896         0,1026         0,1241         0,1466         0,1566         0,1541         0,1396         0,0000
un             0,0852         0,0893         0,1030         0,1242         0,1457         0,1561         0,1548         0,1417         0,0000
mauvais        0,0847         0,0891         0,1032         0,1248         0,1466         0,1568         0,1545         0,1404         0,0000
film           0,0852         0,0883         0,1022         0,1254         0,1498         0,1594         0,1538         0,1360         0,0000
<END>          0,0859         0,0872         0,1009         0,1258         0,1535         0,1626         0,1530         0,1311         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        <UNK>          <UNK>          est            un             mauvais        <END>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        1,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<UNK>          0,4765         0,5235         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<UNK>          0,3344         0,3697         0,2959         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
est            0,2865         0,3200         0,2524         0,1410         0,0000         0,0000         0,0000         0,0000         0,0000
un             0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
mauvais        0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<END>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Cross-Attention Weights =====
===== Tête 1 =====
Requête        <START>        <UNK>          <UNK>          est            un             mauvais        film           <END>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,2760         0,2110         0,1739         0,1655         0,1737         0,0000         0,0000         0,0000         0,0000
<UNK>          0,2723         0,2137         0,1770         0,1660         0,1710         0,0000         0,0000         0,0000         0,0000
<UNK>          0,2682         0,2161         0,1806         0,1669         0,1682         0,0000         0,0000         0,0000         0,0000
est            0,2631         0,2173         0,1842         0,1686         0,1668         0,0000         0,0000         0,0000         0,0000
un             0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
mauvais        0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<END>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0],
   [0]]]]
Attention Weights:
[[[[    1.0000,         0,         0,         0,         0,         0,         0,         0,         0],
   [    0.4812,    0.5188,         0,         0,         0,         0,         0,         0,         0],
   [    0.3432,    0.3722,    0.2846,         0,         0,         0,         0,         0,         0],
   [    0.2971,    0.3228,    0.2430,    0.1371,         0,         0,         0,         0,         0],
   [    0.2728,    0.3001,    0.2258,    0.1273,    0.0740,         0,         0,         0,         0],
   [    0.2503,    0.2795,    0.2122,    0.1201,    0.0699,    0.0681,         0,         0,         0],
   [    0.2211,    0.2528,    0.1948,    0.1092,    0.0621,    0.0597,    0.1003,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Attention Weights:
[[[[    0.1836,    0.1363,    0.1075,    0.1052,    0.1157,    0.1201,    0.1130,    0.1185,         0],
   [    0.1812,    0.1380,    0.1089,    0.1051,    0.1132,    0.1177,    0.1136,    0.1223,         0],
   [    0.1784,    0.1396,    0.1115,    0.1062,    0.1122,    0.1158,    0.1130,    0.1233,         0],
   [    0.1757,    0.1407,    0.1145,    0.1084,    0.1131,    0.1152,    0.1114,    0.1211,         0],
   [    0.1742,    0.1416,    0.1168,    0.1101,    0.1140,    0.1149,    0.1098,    0.1185,         0],
   [    0.1739,    0.1431,    0.1186,    0.1109,    0.1135,    0.1136,    0.1084,    0.1180,         0],
   [    0.1764,    0.1457,    0.1193,    0.1097,    0.1107,    0.1107,    0.1072,    0.1203,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Perte pour ce batch: 4.1359444
Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0], 
   [0],
   [0]]]]
Attention Weights:
[[[[    0.1155,    0.1167,    0.1377,    0.1749,    0.2172,    0.2380,         0,         0,         0],
   [    0.1197,    0.1199,    0.1395,    0.1746,    0.2137,    0.2326,         0,         0,         0],
   [    0.1221,    0.1225,    0.1413,    0.1747,    0.2109,    0.2286,         0,         0,         0],
   [    0.1226,    0.1253,    0.1441,    0.1755,    0.2079,    0.2247,         0,         0,         0],
   [    0.1210,    0.1260,    0.1455,    0.1764,    0.2071,    0.2239,         0,         0,         0],
   [    0.1199,    0.1251,    0.1449,    0.1767,    0.2084,    0.2250,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
===== Échantillon 1 =====
===== Encoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        ce             livre          est            intéressant    <END>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,1155         0,1167         0,1377         0,1749         0,2172         0,2380         0,0000         0,0000         0,0000
ce             0,1197         0,1199         0,1395         0,1746         0,2137         0,2326         0,0000         0,0000         0,0000
livre          0,1221         0,1225         0,1413         0,1747         0,2109         0,2286         0,0000         0,0000         0,0000
est            0,1226         0,1253         0,1441         0,1755         0,2079         0,2247         0,0000         0,0000         0,0000
intéressant    0,1210         0,1260         0,1455         0,1764         0,2071         0,2239         0,0000         0,0000         0,0000
<END>          0,1199         0,1251         0,1449         0,1767         0,2084         0,2250         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        le             livre          est            intéressant    <END>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        1,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
le             0,4812         0,5188         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
livre          0,3432         0,3722         0,2846         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
est            0,2971         0,3228         0,2430         0,1371         0,0000         0,0000         0,0000         0,0000         0,0000
intéressant    0,2728         0,3001         0,2258         0,1273         0,0740         0,0000         0,0000         0,0000         0,0000
<END>          0,2503         0,2795         0,2122         0,1201         0,0699         0,0681         0,0000         0,0000         0,0000
<PAD>          0,2211         0,2528         0,1948         0,1092         0,0621         0,0597         0,1003         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Cross-Attention Weights =====
===== Tête 1 =====
Requête        <START>        ce             livre          est            intéressant    <END>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,1836         0,1363         0,1075         0,1052         0,1157         0,1201         0,1130         0,1185         0,0000
le             0,1812         0,1380         0,1089         0,1051         0,1132         0,1177         0,1136         0,1223         0,0000
livre          0,1784         0,1396         0,1115         0,1062         0,1122         0,1158         0,1130         0,1233         0,0000
est            0,1757         0,1407         0,1145         0,1084         0,1131         0,1152         0,1114         0,1211         0,0000
intéressant    0,1742         0,1416         0,1168         0,1101         0,1140         0,1149         0,1098         0,1185         0,0000
<END>          0,1739         0,1431         0,1186         0,1109         0,1135         0,1136         0,1084         0,1180         0,0000
<PAD>          0,1764         0,1457         0,1193         0,1097         0,1107         0,1107         0,1072         0,1203         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0], 
   [0],
   [0]]]]
Attention Weights:
[[[[    1.0000,         0,         0,         0,         0,         0,         0,         0,         0],
   [    0.4823,    0.5177,         0,         0,         0,         0,         0,         0,         0],
   [    0.3473,    0.3735,    0.2792,         0,         0,         0,         0,         0,         0],
   [    0.3019,    0.3271,    0.2409,    0.1301,         0,         0,         0,         0,         0],
   [    0.2782,    0.3063,    0.2248,    0.1215,    0.0691,         0,         0,         0,         0],
   [    0.2575,    0.2871,    0.2120,    0.1155,    0.0659,    0.0620,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Attention Weights:
[[[[    0.2351,    0.1770,    0.1424,    0.1390,    0.1497,    0.1568,         0,         0,         0],
   [    0.2315,    0.1791,    0.1445,    0.1402,    0.1489,    0.1557,         0,         0,         0],
   [    0.2284,    0.1808,    0.1472,    0.1415,    0.1482,    0.1539,         0,         0,         0],
   [    0.2253,    0.1815,    0.1499,    0.1431,    0.1481,    0.1522,         0,         0,         0],
   [    0.2228,    0.1818,    0.1520,    0.1444,    0.1482,    0.1509,         0,         0,         0],
   [    0.2222,    0.1828,    0.1533,    0.1448,    0.1475,    0.1494,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Perte pour ce batch: 3.502263
Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [1.0000]]]]
Attention Weights:
[[[[    0.0712,    0.0716,    0.0849,    0.1095,    0.1370,    0.1507,    0.1408,    0.1226,    0.1117],
   [    0.0747,    0.0744,    0.0868,    0.1100,    0.1359,    0.1483,    0.1382,    0.1211,    0.1107],
   [    0.0764,    0.0764,    0.0884,    0.1101,    0.1340,    0.1454,    0.1365,    0.1212,    0.1116],
   [    0.0760,    0.0775,    0.0896,    0.1101,    0.1318,    0.1423,    0.1358,    0.1228,    0.1140],
   [    0.0747,    0.0776,    0.0902,    0.1101,    0.1307,    0.1407,    0.1359,    0.1243,    0.1159],
   [    0.0744,    0.0776,    0.0903,    0.1107,    0.1317,    0.1415,    0.1360,    0.1234,    0.1144],
   [    0.0750,    0.0772,    0.0900,    0.1119,    0.1349,    0.1448,    0.1361,    0.1203,    0.1099],
   [    0.0759,    0.0766,    0.0892,    0.1130,    0.1390,    0.1493,    0.1362,    0.1164,    0.1044],
   [    0.0752,    0.0754,    0.0882,    0.1136,    0.1417,    0.1525,    0.1370,    0.1147,    0.1017]]]]
===== Échantillon 1 =====
===== Encoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        je             <UNK>          <UNK>          <UNK>          pas            ce             repas          <END>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,0712         0,0716         0,0849         0,1095         0,1370         0,1507         0,1408         0,1226         0,1117
je             0,0747         0,0744         0,0868         0,1100         0,1359         0,1483         0,1382         0,1211         0,1107
<UNK>          0,0764         0,0764         0,0884         0,1101         0,1340         0,1454         0,1365         0,1212         0,1116
<UNK>          0,0760         0,0775         0,0896         0,1101         0,1318         0,1423         0,1358         0,1228         0,1140
<UNK>          0,0747         0,0776         0,0902         0,1101         0,1307         0,1407         0,1359         0,1243         0,1159
pas            0,0744         0,0776         0,0903         0,1107         0,1317         0,1415         0,1360         0,1234         0,1144
ce             0,0750         0,0772         0,0900         0,1119         0,1349         0,1448         0,1361         0,1203         0,1099
repas          0,0759         0,0766         0,0892         0,1130         0,1390         0,1493         0,1362         0,1164         0,1044
<END>          0,0752         0,0754         0,0882         0,1136         0,1417         0,1525         0,1370         0,1147         0,1017

===== Decoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        je             <UNK>          <UNK>          <UNK>          pas            <END>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        1,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
je             0,4823         0,5177         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<UNK>          0,3473         0,3735         0,2792         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<UNK>          0,3019         0,3271         0,2409         0,1301         0,0000         0,0000         0,0000         0,0000         0,0000
<UNK>          0,2782         0,3063         0,2248         0,1215         0,0691         0,0000         0,0000         0,0000         0,0000
pas            0,2575         0,2871         0,2120         0,1155         0,0659         0,0620         0,0000         0,0000         0,0000
<END>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Cross-Attention Weights =====
===== Tête 1 =====
Requête        <START>        je             <UNK>          <UNK>          <UNK>          pas            ce             repas          <END>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,2351         0,1770         0,1424         0,1390         0,1497         0,1568         0,0000         0,0000         0,0000
je             0,2315         0,1791         0,1445         0,1402         0,1489         0,1557         0,0000         0,0000         0,0000
<UNK>          0,2284         0,1808         0,1472         0,1415         0,1482         0,1539         0,0000         0,0000         0,0000
<UNK>          0,2253         0,1815         0,1499         0,1431         0,1481         0,1522         0,0000         0,0000         0,0000
<UNK>          0,2228         0,1818         0,1520         0,1444         0,1482         0,1509         0,0000         0,0000         0,0000
pas            0,2222         0,1828         0,1533         0,1448         0,1475         0,1494         0,0000         0,0000         0,0000
<END>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0],
   [0]]]]
Attention Weights:
[[[[    1.0000,         0,         0,         0,         0,         0,         0,         0,         0],
   [    0.4815,    0.5185,         0,         0,         0,         0,         0,         0,         0],
   [    0.3495,    0.3774,    0.2732,         0,         0,         0,         0,         0,         0],
   [    0.3075,    0.3327,    0.2367,    0.1230,         0,         0,         0,         0,         0],
   [    0.2849,    0.3121,    0.2219,    0.1151,    0.0659,         0,         0,         0,         0],
   [    0.2649,    0.2936,    0.2102,    0.1093,    0.0625,    0.0595,         0,         0,         0],
   [    0.2364,    0.2679,    0.1942,    0.0998,    0.0556,    0.0524,    0.0938,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Attention Weights:
[[[[    0.1566,    0.1173,    0.0915,    0.0898,    0.0994,    0.1059,    0.1016,    0.1072,    0.1307],
   [    0.1540,    0.1178,    0.0921,    0.0897,    0.0978,    0.1040,    0.1017,    0.1092,    0.1336],
   [    0.1515,    0.1186,    0.0939,    0.0908,    0.0976,    0.1031,    0.1015,    0.1097,    0.1334],
   [    0.1498,    0.1194,    0.0962,    0.0929,    0.0991,    0.1037,    0.1011,    0.1081,    0.1297],
   [    0.1494,    0.1205,    0.0983,    0.0946,    0.1002,    0.1041,    0.1003,    0.1063,    0.1263],
   [    0.1491,    0.1213,    0.0995,    0.0953,    0.1003,    0.1036,    0.0995,    0.1058,    0.1256],
   [    0.1496,    0.1222,    0.0993,    0.0940,    0.0978,    0.1010,    0.0985,    0.1077,    0.1300],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Perte pour ce batch: 3.9230816
Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0], 
   [0],
   [0]]]]
Attention Weights:
[[[[    0.1131,    0.1138,    0.1364,    0.1746,    0.2202,    0.2420,         0,         0,         0],
   [    0.1176,    0.1172,    0.1385,    0.1742,    0.2164,    0.2361,         0,         0,         0],
   [    0.1205,    0.1203,    0.1410,    0.1744,    0.2130,    0.2308,         0,         0,         0],
   [    0.1210,    0.1231,    0.1440,    0.1751,    0.2102,    0.2267,         0,         0,         0],
   [    0.1193,    0.1237,    0.1454,    0.1759,    0.2097,    0.2260,         0,         0,         0],
   [    0.1183,    0.1231,    0.1451,    0.1763,    0.2106,    0.2266,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
===== Échantillon 1 =====
===== Encoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        ce             film           est            excellent      <END>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,1131         0,1138         0,1364         0,1746         0,2202         0,2420         0,0000         0,0000         0,0000
ce             0,1176         0,1172         0,1385         0,1742         0,2164         0,2361         0,0000         0,0000         0,0000
film           0,1205         0,1203         0,1410         0,1744         0,2130         0,2308         0,0000         0,0000         0,0000
est            0,1210         0,1231         0,1440         0,1751         0,2102         0,2267         0,0000         0,0000         0,0000
excellent      0,1193         0,1237         0,1454         0,1759         0,2097         0,2260         0,0000         0,0000         0,0000
<END>          0,1183         0,1231         0,1451         0,1763         0,2106         0,2266         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        le             film           est            excellent      <END>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        1,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
le             0,4815         0,5185         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
film           0,3495         0,3774         0,2732         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
est            0,3075         0,3327         0,2367         0,1230         0,0000         0,0000         0,0000         0,0000         0,0000
excellent      0,2849         0,3121         0,2219         0,1151         0,0659         0,0000         0,0000         0,0000         0,0000
<END>          0,2649         0,2936         0,2102         0,1093         0,0625         0,0595         0,0000         0,0000         0,0000
<PAD>          0,2364         0,2679         0,1942         0,0998         0,0556         0,0524         0,0938         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Cross-Attention Weights =====
===== Tête 1 =====
Requête        <START>        ce             film           est            excellent      <END>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,1566         0,1173         0,0915         0,0898         0,0994         0,1059         0,1016         0,1072         0,1307
le             0,1540         0,1178         0,0921         0,0897         0,0978         0,1040         0,1017         0,1092         0,1336
film           0,1515         0,1186         0,0939         0,0908         0,0976         0,1031         0,1015         0,1097         0,1334
est            0,1498         0,1194         0,0962         0,0929         0,0991         0,1037         0,1011         0,1081         0,1297
excellent      0,1494         0,1205         0,0983         0,0946         0,1002         0,1041         0,1003         0,1063         0,1263
<END>          0,1491         0,1213         0,0995         0,0953         0,1003         0,1036         0,0995         0,1058         0,1256
<PAD>          0,1496         0,1222         0,0993         0,0940         0,0978         0,1010         0,0985         0,1077         0,1300
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0], 
   [0],
   [0]]]]
Attention Weights:
[[[[    1.0000,         0,         0,         0,         0,         0,         0,         0,         0],
   [    0.4859,    0.5141,         0,         0,         0,         0,         0,         0,         0],
   [    0.3524,    0.3743,    0.2733,         0,         0,         0,         0,         0,         0],
   [    0.3081,    0.3291,    0.2373,    0.1254,         0,         0,         0,         0,         0],
   [    0.2847,    0.3089,    0.2226,    0.1175,    0.0663,         0,         0,         0,         0],
   [    0.2653,    0.2911,    0.2106,    0.1114,    0.0628,    0.0587,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Attention Weights:
[[[[    0.2306,    0.1766,    0.1426,    0.1402,    0.1505,    0.1594,         0,         0,         0],
   [    0.2268,    0.1776,    0.1445,    0.1415,    0.1505,    0.1591,         0,         0,         0],
   [    0.2235,    0.1785,    0.1467,    0.1429,    0.1502,    0.1581,         0,         0,         0],
   [    0.2209,    0.1785,    0.1482,    0.1442,    0.1507,    0.1575,         0,         0,         0],
   [    0.2191,    0.1786,    0.1496,    0.1452,    0.1509,    0.1566,         0,         0,         0],
   [    0.2184,    0.1795,    0.1508,    0.1457,    0.1504,    0.1553,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Perte pour ce batch: 3.0460107
Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0]]]]
Attention Weights:
[[[[    0.0790,    0.0792,    0.0942,    0.1243,    0.1571,    0.1719,    0.1589,    0.1353,         0],
   [    0.0830,    0.0824,    0.0963,    0.1248,    0.1555,    0.1688,    0.1560,    0.1333,         0],
   [    0.0849,    0.0847,    0.0982,    0.1250,    0.1536,    0.1659,    0.1543,    0.1333,         0],
   [    0.0851,    0.0866,    0.1006,    0.1252,    0.1511,    0.1625,    0.1534,    0.1354,         0],
   [    0.0838,    0.0870,    0.1017,    0.1254,    0.1500,    0.1612,    0.1536,    0.1373,         0],
   [    0.0832,    0.0867,    0.1016,    0.1259,    0.1509,    0.1619,    0.1535,    0.1362,         0],
   [    0.0836,    0.0860,    0.1004,    0.1267,    0.1539,    0.1647,    0.1528,    0.1319,         0],
   [    0.0841,    0.0847,    0.0985,    0.1274,    0.1576,    0.1684,    0.1522,    0.1271,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
===== Échantillon 1 =====
===== Encoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        je             suis           triste         <UNK>          <UNK>          <UNK>          <END>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,0790         0,0792         0,0942         0,1243         0,1571         0,1719         0,1589         0,1353         0,0000
je             0,0830         0,0824         0,0963         0,1248         0,1555         0,1688         0,1560         0,1333         0,0000
suis           0,0849         0,0847         0,0982         0,1250         0,1536         0,1659         0,1543         0,1333         0,0000
triste         0,0851         0,0866         0,1006         0,1252         0,1511         0,1625         0,1534         0,1354         0,0000
<UNK>          0,0838         0,0870         0,1017         0,1254         0,1500         0,1612         0,1536         0,1373         0,0000
<UNK>          0,0832         0,0867         0,1016         0,1259         0,1509         0,1619         0,1535         0,1362         0,0000
<UNK>          0,0836         0,0860         0,1004         0,1267         0,1539         0,1647         0,1528         0,1319         0,0000
<END>          0,0841         0,0847         0,0985         0,1274         0,1576         0,1684         0,1522         0,1271         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        je             suis           triste         <END>          <PAD>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        1,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
je             0,4859         0,5141         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
suis           0,3524         0,3743         0,2733         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
triste         0,3081         0,3291         0,2373         0,1254         0,0000         0,0000         0,0000         0,0000         0,0000
<END>          0,2847         0,3089         0,2226         0,1175         0,0663         0,0000         0,0000         0,0000         0,0000
<PAD>          0,2653         0,2911         0,2106         0,1114         0,0628         0,0587         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Cross-Attention Weights =====
===== Tête 1 =====
Requête        <START>        je             suis           triste         <UNK>          <UNK>          <UNK>          <END>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,2306         0,1766         0,1426         0,1402         0,1505         0,1594         0,0000         0,0000         0,0000
je             0,2268         0,1776         0,1445         0,1415         0,1505         0,1591         0,0000         0,0000         0,0000
suis           0,2235         0,1785         0,1467         0,1429         0,1502         0,1581         0,0000         0,0000         0,0000
triste         0,2209         0,1785         0,1482         0,1442         0,1507         0,1575         0,0000         0,0000         0,0000
<END>          0,2191         0,1786         0,1496         0,1452         0,1509         0,1566         0,0000         0,0000         0,0000
<PAD>          0,2184         0,1795         0,1508         0,1457         0,1504         0,1553         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0], 
   [0], 
   [0],
   [0]]]]
Attention Weights:
[[[[    1.0000,         0,         0,         0,         0,         0,         0,         0,         0],
   [    0.4853,    0.5147,         0,         0,         0,         0,         0,         0,         0],
   [    0.3531,    0.3755,    0.2713,         0,         0,         0,         0,         0,         0],
   [    0.3106,    0.3311,    0.2360,    0.1223,         0,         0,         0,         0,         0],
   [    0.2875,    0.3106,    0.2217,    0.1144,    0.0658,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Attention Weights:
[[[[    0.1730,    0.1347,    0.1074,    0.1032,    0.1107,    0.1179,    0.1197,    0.1334,         0],
   [    0.1705,    0.1352,    0.1083,    0.1034,    0.1100,    0.1171,    0.1201,    0.1354,         0],
   [    0.1679,    0.1354,    0.1098,    0.1045,    0.1101,    0.1167,    0.1199,    0.1356,         0],
   [    0.1661,    0.1354,    0.1113,    0.1059,    0.1112,    0.1170,    0.1194,    0.1337,         0],
   [    0.1652,    0.1359,    0.1128,    0.1071,    0.1118,    0.1168,    0.1185,    0.1319,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Perte pour ce batch: 3.5355392
Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0], 
   [0],
   [0]]]]
Attention Weights:
[[[[    0.1109,    0.1114,    0.1334,    0.1748,    0.2244,    0.2451,         0,         0,         0],
   [    0.1156,    0.1150,    0.1359,    0.1745,    0.2202,    0.2388,         0,         0,         0],
   [    0.1186,    0.1183,    0.1385,    0.1745,    0.2166,    0.2334,         0,         0,         0],
   [    0.1191,    0.1212,    0.1418,    0.1754,    0.2135,    0.2289,         0,         0,         0],
   [    0.1176,    0.1219,    0.1433,    0.1763,    0.2129,    0.2281,         0,         0,         0],
   [    0.1164,    0.1212,    0.1429,    0.1767,    0.2139,    0.2288,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
===== Échantillon 1 =====
===== Encoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        ce             temps          est            agréable       <END>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,1109         0,1114         0,1334         0,1748         0,2244         0,2451         0,0000         0,0000         0,0000
ce             0,1156         0,1150         0,1359         0,1745         0,2202         0,2388         0,0000         0,0000         0,0000
temps          0,1186         0,1183         0,1385         0,1745         0,2166         0,2334         0,0000         0,0000         0,0000
est            0,1191         0,1212         0,1418         0,1754         0,2135         0,2289         0,0000         0,0000         0,0000
agréable       0,1176         0,1219         0,1433         0,1763         0,2129         0,2281         0,0000         0,0000         0,0000
<END>          0,1164         0,1212         0,1429         0,1767         0,2139         0,2288         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        le             temps          est            agréable       <END>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        1,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
le             0,4853         0,5147         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
temps          0,3531         0,3755         0,2713         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
est            0,3106         0,3311         0,2360         0,1223         0,0000         0,0000         0,0000         0,0000         0,0000
agréable       0,2875         0,3106         0,2217         0,1144         0,0658         0,0000         0,0000         0,0000         0,0000
<END>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Cross-Attention Weights =====
===== Tête 1 =====
Requête        <START>        ce             temps          est            agréable       <END>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,1730         0,1347         0,1074         0,1032         0,1107         0,1179         0,1197         0,1334         0,0000
le             0,1705         0,1352         0,1083         0,1034         0,1100         0,1171         0,1201         0,1354         0,0000
temps          0,1679         0,1354         0,1098         0,1045         0,1101         0,1167         0,1199         0,1356         0,0000
est            0,1661         0,1354         0,1113         0,1059         0,1112         0,1170         0,1194         0,1337         0,0000
agréable       0,1652         0,1359         0,1128         0,1071         0,1118         0,1168         0,1185         0,1319         0,0000
<END>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0], 
   [0],
   [0]]]]
Attention Weights:
[[[[    1.0000,         0,         0,         0,         0,         0,         0,         0,         0],
   [    0.4887,    0.5113,         0,         0,         0,         0,         0,         0,         0],
   [    0.3545,    0.3722,    0.2733,         0,         0,         0,         0,         0,         0],
   [    0.3091,    0.3270,    0.2381,    0.1258,         0,         0,         0,         0,         0],
   [    0.2860,    0.3069,    0.2234,    0.1176,    0.0661,         0,         0,         0,         0],
   [    0.2664,    0.2894,    0.2116,    0.1114,    0.0626,    0.0585,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Attention Weights:
[[[[    0.2267,    0.1799,    0.1473,    0.1414,    0.1474,    0.1572,         0,         0,         0],
   [    0.2232,    0.1804,    0.1488,    0.1425,    0.1477,    0.1574,         0,         0,         0],
   [    0.2202,    0.1807,    0.1504,    0.1438,    0.1480,    0.1569,         0,         0,         0],
   [    0.2181,    0.1806,    0.1515,    0.1448,    0.1485,    0.1565,         0,         0,         0],
   [    0.2163,    0.1805,    0.1527,    0.1457,    0.1488,    0.1559,         0,         0,         0],
   [    0.2158,    0.1811,    0.1536,    0.1461,    0.1484,    0.1549,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Perte pour ce batch: 3.1982157
Epoch 1 completed with average loss: 0.04578988
Epoch 2 / 3
Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0], 
   [0],
   [0]]]]
Attention Weights:
[[[[    0.1094,    0.1106,    0.1316,    0.1757,    0.2257,    0.2469,         0,         0,         0],
   [    0.1139,    0.1141,    0.1336,    0.1752,    0.2221,    0.2411,         0,         0,         0],
   [    0.1170,    0.1179,    0.1366,    0.1755,    0.2182,    0.2348,         0,         0,         0],
   [    0.1173,    0.1204,    0.1395,    0.1766,    0.2154,    0.2307,         0,         0,         0],
   [    0.1159,    0.1212,    0.1413,    0.1778,    0.2145,    0.2293,         0,         0,         0],
   [    0.1147,    0.1205,    0.1410,    0.1782,    0.2155,    0.2301,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
===== Échantillon 1 =====
===== Encoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        chat           mange          la             souris         <END>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,1094         0,1106         0,1316         0,1757         0,2257         0,2469         0,0000         0,0000         0,0000
chat           0,1139         0,1141         0,1336         0,1752         0,2221         0,2411         0,0000         0,0000         0,0000
mange          0,1170         0,1179         0,1366         0,1755         0,2182         0,2348         0,0000         0,0000         0,0000
la             0,1173         0,1204         0,1395         0,1766         0,2154         0,2307         0,0000         0,0000         0,0000
souris         0,1159         0,1212         0,1413         0,1778         0,2145         0,2293         0,0000         0,0000         0,0000
<END>          0,1147         0,1205         0,1410         0,1782         0,2155         0,2301         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        le             chat           mange          <END>          <PAD>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        1,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
le             0,4887         0,5113         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
chat           0,3545         0,3722         0,2733         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
mange          0,3091         0,3270         0,2381         0,1258         0,0000         0,0000         0,0000         0,0000         0,0000
<END>          0,2860         0,3069         0,2234         0,1176         0,0661         0,0000         0,0000         0,0000         0,0000
<PAD>          0,2664         0,2894         0,2116         0,1114         0,0626         0,0585         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Cross-Attention Weights =====
===== Tête 1 =====
Requête        <START>        chat           mange          la             souris         <END>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,2267         0,1799         0,1473         0,1414         0,1474         0,1572         0,0000         0,0000         0,0000
le             0,2232         0,1804         0,1488         0,1425         0,1477         0,1574         0,0000         0,0000         0,0000
chat           0,2202         0,1807         0,1504         0,1438         0,1480         0,1569         0,0000         0,0000         0,0000
mange          0,2181         0,1806         0,1515         0,1448         0,1485         0,1565         0,0000         0,0000         0,0000
<END>          0,2163         0,1805         0,1527         0,1457         0,1488         0,1559         0,0000         0,0000         0,0000
<PAD>          0,2158         0,1811         0,1536         0,1461         0,1484         0,1549         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0], 
   [0], 
   [0],
   [0]]]]
Attention Weights:
[[[[    1.0000,         0,         0,         0,         0,         0,         0,         0,         0],
   [    0.4881,    0.5119,         0,         0,         0,         0,         0,         0,         0],
   [    0.3554,    0.3744,    0.2702,         0,         0,         0,         0,         0,         0],
   [    0.3105,    0.3294,    0.2361,    0.1240,         0,         0,         0,         0,         0],
   [    0.2873,    0.3084,    0.2217,    0.1159,    0.0666,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Attention Weights:
[[[[    0.2239,    0.1812,    0.1487,    0.1421,    0.1474,    0.1567,         0,         0,         0],
   [    0.2210,    0.1814,    0.1499,    0.1431,    0.1477,    0.1569,         0,         0,         0],
   [    0.2177,    0.1814,    0.1515,    0.1444,    0.1482,    0.1567,         0,         0,         0],
   [    0.2157,    0.1811,    0.1525,    0.1453,    0.1488,    0.1565,         0,         0,         0],
   [    0.2139,    0.1811,    0.1536,    0.1460,    0.1492,    0.1561,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Perte pour ce batch: 3.1520388
Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0],
   [0]]]]
Attention Weights:
[[[[    0.0886,    0.0889,    0.1079,    0.1446,    0.1847,    0.2017,    0.1835,         0,         0],
   [    0.0933,    0.0926,    0.1102,    0.1448,    0.1821,    0.1973,    0.1797,         0,         0],
   [    0.0959,    0.0955,    0.1127,    0.1453,    0.1799,    0.1934,    0.1773,         0,         0],
   [    0.0961,    0.0976,    0.1153,    0.1462,    0.1779,    0.1902,    0.1767,         0,         0],
   [    0.0945,    0.0977,    0.1164,    0.1469,    0.1775,    0.1894,    0.1776,         0,         0],
   [    0.0937,    0.0972,    0.1162,    0.1474,    0.1783,    0.1901,    0.1772,         0,         0],
   [    0.0938,    0.0959,    0.1144,    0.1475,    0.1805,    0.1925,    0.1754,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
===== Échantillon 1 =====
===== Encoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        chien          court          dans           le             jardin         <END>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,0886         0,0889         0,1079         0,1446         0,1847         0,2017         0,1835         0,0000         0,0000
chien          0,0933         0,0926         0,1102         0,1448         0,1821         0,1973         0,1797         0,0000         0,0000
court          0,0959         0,0955         0,1127         0,1453         0,1799         0,1934         0,1773         0,0000         0,0000
dans           0,0961         0,0976         0,1153         0,1462         0,1779         0,1902         0,1767         0,0000         0,0000
le             0,0945         0,0977         0,1164         0,1469         0,1775         0,1894         0,1776         0,0000         0,0000
jardin         0,0937         0,0972         0,1162         0,1474         0,1783         0,1901         0,1772         0,0000         0,0000
<END>          0,0938         0,0959         0,1144         0,1475         0,1805         0,1925         0,1754         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        le             chien          court          <END>          <PAD>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        1,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
le             0,4881         0,5119         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
chien          0,3554         0,3744         0,2702         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
court          0,3105         0,3294         0,2361         0,1240         0,0000         0,0000         0,0000         0,0000         0,0000
<END>          0,2873         0,3084         0,2217         0,1159         0,0666         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Cross-Attention Weights =====
===== Tête 1 =====
Requête        <START>        chien          court          dans           le             jardin         <END>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,2239         0,1812         0,1487         0,1421         0,1474         0,1567         0,0000         0,0000         0,0000
le             0,2210         0,1814         0,1499         0,1431         0,1477         0,1569         0,0000         0,0000         0,0000
chien          0,2177         0,1814         0,1515         0,1444         0,1482         0,1567         0,0000         0,0000         0,0000
court          0,2157         0,1811         0,1525         0,1453         0,1488         0,1565         0,0000         0,0000         0,0000
<END>          0,2139         0,1811         0,1536         0,1460         0,1492         0,1561         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0], 
   [0], 
   [0],
   [0]]]]
Attention Weights:
[[[[    1.0000,         0,         0,         0,         0,         0,         0,         0,         0],
   [    0.4954,    0.5046,         0,         0,         0,         0,         0,         0,         0],
   [    0.3633,    0.3714,    0.2652,         0,         0,         0,         0,         0,         0],
   [    0.3171,    0.3267,    0.2314,    0.1248,         0,         0,         0,         0,         0],
   [    0.2930,    0.3057,    0.2168,    0.1164,    0.0681,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Attention Weights:
[[[[    0.1911,    0.1561,    0.1286,    0.1214,    0.1264,    0.1334,    0.1429,         0,         0],
   [    0.1888,    0.1562,    0.1295,    0.1220,    0.1265,    0.1334,    0.1436,         0,         0],
   [    0.1864,    0.1562,    0.1306,    0.1230,    0.1269,    0.1334,    0.1435,         0,         0],
   [    0.1846,    0.1558,    0.1313,    0.1240,    0.1278,    0.1337,    0.1429,         0,         0],
   [    0.1835,    0.1560,    0.1322,    0.1249,    0.1281,    0.1334,    0.1420,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Perte pour ce batch: 2.6818433
Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0],
   [0]]]]
Attention Weights:
[[[[    0.0882,    0.0884,    0.1068,    0.1433,    0.1851,    0.2043,    0.1839,         0,         0],
   [    0.0923,    0.0919,    0.1091,    0.1438,    0.1827,    0.2001,    0.1803,         0,         0],
   [    0.0955,    0.0953,    0.1117,    0.1442,    0.1800,    0.1957,    0.1776,         0,         0],
   [    0.0955,    0.0973,    0.1142,    0.1450,    0.1779,    0.1928,    0.1772,         0,         0],
   [    0.0937,    0.0977,    0.1156,    0.1459,    0.1773,    0.1919,    0.1779,         0,         0],
   [    0.0932,    0.0973,    0.1155,    0.1462,    0.1780,    0.1925,    0.1774,         0,         0],
   [    0.0933,    0.0959,    0.1137,    0.1462,    0.1806,    0.1948,    0.1756,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
===== Échantillon 1 =====
===== Encoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        les            chats          aiment         les            chiens         <END>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,0882         0,0884         0,1068         0,1433         0,1851         0,2043         0,1839         0,0000         0,0000
les            0,0923         0,0919         0,1091         0,1438         0,1827         0,2001         0,1803         0,0000         0,0000
chats          0,0955         0,0953         0,1117         0,1442         0,1800         0,1957         0,1776         0,0000         0,0000
aiment         0,0955         0,0973         0,1142         0,1450         0,1779         0,1928         0,1772         0,0000         0,0000
les            0,0937         0,0977         0,1156         0,1459         0,1773         0,1919         0,1779         0,0000         0,0000
chiens         0,0932         0,0973         0,1155         0,1462         0,1780         0,1925         0,1774         0,0000         0,0000
<END>          0,0933         0,0959         0,1137         0,1462         0,1806         0,1948         0,1756         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        les            chats          aiment         <END>          <PAD>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        1,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
les            0,4954         0,5046         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
chats          0,3633         0,3714         0,2652         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
aiment         0,3171         0,3267         0,2314         0,1248         0,0000         0,0000         0,0000         0,0000         0,0000
<END>          0,2930         0,3057         0,2168         0,1164         0,0681         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Cross-Attention Weights =====
===== Tête 1 =====
Requête        <START>        les            chats          aiment         les            chiens         <END>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,1911         0,1561         0,1286         0,1214         0,1264         0,1334         0,1429         0,0000         0,0000
les            0,1888         0,1562         0,1295         0,1220         0,1265         0,1334         0,1436         0,0000         0,0000
chats          0,1864         0,1562         0,1306         0,1230         0,1269         0,1334         0,1435         0,0000         0,0000
aiment         0,1846         0,1558         0,1313         0,1240         0,1278         0,1337         0,1429         0,0000         0,0000
<END>          0,1835         0,1560         0,1322         0,1249         0,1281         0,1334         0,1420         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0], 
   [0], 
   [0],
   [0]]]]
Attention Weights:
[[[[    1.0000,         0,         0,         0,         0,         0,         0,         0,         0],
   [    0.4956,    0.5044,         0,         0,         0,         0,         0,         0,         0],
   [    0.3623,    0.3711,    0.2666,         0,         0,         0,         0,         0,         0],
   [    0.3158,    0.3262,    0.2327,    0.1253,         0,         0,         0,         0,         0],
   [    0.2926,    0.3056,    0.2182,    0.1170,    0.0665,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Attention Weights:
[[[[    0.1897,    0.1580,    0.1302,    0.1223,    0.1255,    0.1313,    0.1431,         0,         0],
   [    0.1877,    0.1582,    0.1310,    0.1227,    0.1255,    0.1313,    0.1437,         0,         0],
   [    0.1852,    0.1578,    0.1319,    0.1237,    0.1262,    0.1316,    0.1437,         0,         0],
   [    0.1835,    0.1570,    0.1325,    0.1246,    0.1270,    0.1321,    0.1433,         0,         0],
   [    0.1821,    0.1569,    0.1335,    0.1255,    0.1273,    0.1321,    0.1426,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Perte pour ce batch: 3.2476811
Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0], 
   [0],
   [0]]]]
Attention Weights:
[[[[    0.1075,    0.1086,    0.1296,    0.1758,    0.2285,    0.2500,         0,         0,         0],
   [    0.1123,    0.1123,    0.1319,    0.1754,    0.2245,    0.2435,         0,         0,         0],
   [    0.1154,    0.1161,    0.1352,    0.1758,    0.2203,    0.2372,         0,         0,         0],
   [    0.1152,    0.1181,    0.1380,    0.1771,    0.2178,    0.2338,         0,         0,         0],
   [    0.1138,    0.1188,    0.1397,    0.1783,    0.2169,    0.2324,         0,         0,         0],
   [    0.1127,    0.1181,    0.1393,    0.1787,    0.2180,    0.2331,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
===== Échantillon 1 =====
===== Encoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        tapis          sur            le             sol            <END>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,1075         0,1086         0,1296         0,1758         0,2285         0,2500         0,0000         0,0000         0,0000
tapis          0,1123         0,1123         0,1319         0,1754         0,2245         0,2435         0,0000         0,0000         0,0000
sur            0,1154         0,1161         0,1352         0,1758         0,2203         0,2372         0,0000         0,0000         0,0000
le             0,1152         0,1181         0,1380         0,1771         0,2178         0,2338         0,0000         0,0000         0,0000
sol            0,1138         0,1188         0,1397         0,1783         0,2169         0,2324         0,0000         0,0000         0,0000
<END>          0,1127         0,1181         0,1393         0,1787         0,2180         0,2331         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        le             tapis          sur            <END>          <PAD>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        1,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
le             0,4956         0,5044         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
tapis          0,3623         0,3711         0,2666         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
sur            0,3158         0,3262         0,2327         0,1253         0,0000         0,0000         0,0000         0,0000         0,0000
<END>          0,2926         0,3056         0,2182         0,1170         0,0665         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Cross-Attention Weights =====
===== Tête 1 =====
Requête        <START>        tapis          sur            le             sol            <END>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,1897         0,1580         0,1302         0,1223         0,1255         0,1313         0,1431         0,0000         0,0000
le             0,1877         0,1582         0,1310         0,1227         0,1255         0,1313         0,1437         0,0000         0,0000
tapis          0,1852         0,1578         0,1319         0,1237         0,1262         0,1316         0,1437         0,0000         0,0000
sur            0,1835         0,1570         0,1325         0,1246         0,1270         0,1321         0,1433         0,0000         0,0000
<END>          0,1821         0,1569         0,1335         0,1255         0,1273         0,1321         0,1426         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0], 
   [0], 
   [0],
   [0]]]]
Attention Weights:
[[[[    1.0000,         0,         0,         0,         0,         0,         0,         0,         0],
   [    0.4920,    0.5080,         0,         0,         0,         0,         0,         0,         0],
   [    0.3587,    0.3728,    0.2685,         0,         0,         0,         0,         0,         0],
   [    0.3118,    0.3278,    0.2348,    0.1256,         0,         0,         0,         0,         0],
   [    0.2885,    0.3076,    0.2206,    0.1173,    0.0661,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Attention Weights:
[[[[    0.2203,    0.1843,    0.1527,    0.1431,    0.1461,    0.1535,         0,         0,         0],
   [    0.2176,    0.1842,    0.1536,    0.1439,    0.1467,    0.1541,         0,         0,         0],
   [    0.2149,    0.1837,    0.1546,    0.1449,    0.1474,    0.1544,         0,         0,         0],
   [    0.2126,    0.1828,    0.1551,    0.1459,    0.1486,    0.1551,         0,         0,         0],
   [    0.2107,    0.1823,    0.1558,    0.1467,    0.1493,    0.1552,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Perte pour ce batch: 3.2758203
Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0], 
   [0],
   [0]]]]
Attention Weights:
[[[[    0.1069,    0.1072,    0.1310,    0.1745,    0.2298,    0.2506,         0,         0,         0],
   [    0.1116,    0.1110,    0.1337,    0.1743,    0.2256,    0.2438,         0,         0,         0],
   [    0.1145,    0.1143,    0.1366,    0.1746,    0.2220,    0.2380,         0,         0,         0],
   [    0.1150,    0.1171,    0.1399,    0.1755,    0.2189,    0.2336,         0,         0,         0],
   [    0.1129,    0.1174,    0.1413,    0.1766,    0.2186,    0.2332,         0,         0,         0],
   [    0.1119,    0.1169,    0.1410,    0.1770,    0.2194,    0.2337,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
===== Échantillon 1 =====
===== Encoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        ce             film           est            fantastique    <END>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,1069         0,1072         0,1310         0,1745         0,2298         0,2506         0,0000         0,0000         0,0000
ce             0,1116         0,1110         0,1337         0,1743         0,2256         0,2438         0,0000         0,0000         0,0000
film           0,1145         0,1143         0,1366         0,1746         0,2220         0,2380         0,0000         0,0000         0,0000
est            0,1150         0,1171         0,1399         0,1755         0,2189         0,2336         0,0000         0,0000         0,0000
fantastique    0,1129         0,1174         0,1413         0,1766         0,2186         0,2332         0,0000         0,0000         0,0000
<END>          0,1119         0,1169         0,1410         0,1770         0,2194         0,2337         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        le             film           est            fantastique    <END>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        1,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
le             0,4920         0,5080         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
film           0,3587         0,3728         0,2685         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
est            0,3118         0,3278         0,2348         0,1256         0,0000         0,0000         0,0000         0,0000         0,0000
fantastique    0,2885         0,3076         0,2206         0,1173         0,0661         0,0000         0,0000         0,0000         0,0000
<END>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Cross-Attention Weights =====
===== Tête 1 =====
Requête        <START>        ce             film           est            fantastique    <END>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,2203         0,1843         0,1527         0,1431         0,1461         0,1535         0,0000         0,0000         0,0000
le             0,2176         0,1842         0,1536         0,1439         0,1467         0,1541         0,0000         0,0000         0,0000
film           0,2149         0,1837         0,1546         0,1449         0,1474         0,1544         0,0000         0,0000         0,0000
est            0,2126         0,1828         0,1551         0,1459         0,1486         0,1551         0,0000         0,0000         0,0000
fantastique    0,2107         0,1823         0,1558         0,1467         0,1493         0,1552         0,0000         0,0000         0,0000
<END>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0], 
   [0],
   [0]]]]
Attention Weights:
[[[[    1.0000,         0,         0,         0,         0,         0,         0,         0,         0],
   [    0.4920,    0.5080,         0,         0,         0,         0,         0,         0,         0],
   [    0.3567,    0.3712,    0.2721,         0,         0,         0,         0,         0,         0],
   [    0.3091,    0.3259,    0.2370,    0.1280,         0,         0,         0,         0,         0],
   [    0.2858,    0.3059,    0.2221,    0.1192,    0.0671,         0,         0,         0,         0],
   [    0.2665,    0.2889,    0.2106,    0.1127,    0.0632,    0.0581,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Attention Weights:
[[[[    0.2199,    0.1853,    0.1535,    0.1431,    0.1441,    0.1541,         0,         0,         0],
   [    0.2171,    0.1851,    0.1543,    0.1439,    0.1448,    0.1549,         0,         0,         0],
   [    0.2144,    0.1846,    0.1551,    0.1449,    0.1456,    0.1553,         0,         0,         0],
   [    0.2120,    0.1835,    0.1554,    0.1460,    0.1469,    0.1562,         0,         0,         0],
   [    0.2099,    0.1827,    0.1559,    0.1470,    0.1479,    0.1566,         0,         0,         0],
   [    0.2092,    0.1827,    0.1563,    0.1474,    0.1480,    0.1563,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Perte pour ce batch: 2.9201825
Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0], 
   [0],
   [0]]]]
Attention Weights:
[[[[    0.1063,    0.1061,    0.1302,    0.1750,    0.2304,    0.2520,         0,         0,         0],
   [    0.1114,    0.1102,    0.1328,    0.1745,    0.2262,    0.2449,         0,         0,         0],
   [    0.1143,    0.1137,    0.1356,    0.1746,    0.2226,    0.2392,         0,         0,         0],
   [    0.1137,    0.1160,    0.1386,    0.1760,    0.2203,    0.2355,         0,         0,         0],
   [    0.1121,    0.1167,    0.1401,    0.1770,    0.2195,    0.2345,         0,         0,         0],
   [    0.1110,    0.1162,    0.1399,    0.1776,    0.2203,    0.2350,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
===== Échantillon 1 =====
===== Encoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        je             déteste        ce             temps          <END>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,1063         0,1061         0,1302         0,1750         0,2304         0,2520         0,0000         0,0000         0,0000
je             0,1114         0,1102         0,1328         0,1745         0,2262         0,2449         0,0000         0,0000         0,0000
déteste        0,1143         0,1137         0,1356         0,1746         0,2226         0,2392         0,0000         0,0000         0,0000
ce             0,1137         0,1160         0,1386         0,1760         0,2203         0,2355         0,0000         0,0000         0,0000
temps          0,1121         0,1167         0,1401         0,1770         0,2195         0,2345         0,0000         0,0000         0,0000
<END>          0,1110         0,1162         0,1399         0,1776         0,2203         0,2350         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        je             déteste        <END>          <PAD>          <PAD>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        1,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
je             0,4920         0,5080         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
déteste        0,3567         0,3712         0,2721         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<END>          0,3091         0,3259         0,2370         0,1280         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,2858         0,3059         0,2221         0,1192         0,0671         0,0000         0,0000         0,0000         0,0000
<PAD>          0,2665         0,2889         0,2106         0,1127         0,0632         0,0581         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Cross-Attention Weights =====
===== Tête 1 =====
Requête        <START>        je             déteste        ce             temps          <END>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,2199         0,1853         0,1535         0,1431         0,1441         0,1541         0,0000         0,0000         0,0000
je             0,2171         0,1851         0,1543         0,1439         0,1448         0,1549         0,0000         0,0000         0,0000
déteste        0,2144         0,1846         0,1551         0,1449         0,1456         0,1553         0,0000         0,0000         0,0000
<END>          0,2120         0,1835         0,1554         0,1460         0,1469         0,1562         0,0000         0,0000         0,0000
<PAD>          0,2099         0,1827         0,1559         0,1470         0,1479         0,1566         0,0000         0,0000         0,0000
<PAD>          0,2092         0,1827         0,1563         0,1474         0,1480         0,1563         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0], 
   [0], 
   [0], 
   [0],
   [0]]]]
Attention Weights:
[[[[    1.0000,         0,         0,         0,         0,         0,         0,         0,         0],
   [    0.4900,    0.5100,         0,         0,         0,         0,         0,         0,         0],
   [    0.3551,    0.3729,    0.2720,         0,         0,         0,         0,         0,         0],
   [    0.3078,    0.3285,    0.2375,    0.1262,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Attention Weights:
[[[[    0.2197,    0.1856,    0.1529,    0.1428,    0.1446,    0.1543,         0,         0,         0],
   [    0.2173,    0.1853,    0.1535,    0.1437,    0.1452,    0.1551,         0,         0,         0],
   [    0.2144,    0.1845,    0.1541,    0.1449,    0.1462,    0.1558,         0,         0,         0],
   [    0.2115,    0.1829,    0.1543,    0.1461,    0.1481,    0.1572,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Perte pour ce batch: 2.9449437
Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0], 
   [0], 
   [0],
   [0]]]]
Attention Weights:
[[[[    0.1410,    0.1425,    0.1721,    0.2363,    0.3081,         0,         0,         0,         0],
   [    0.1461,    0.1464,    0.1737,    0.2336,    0.3002,         0,         0,         0,         0],
   [    0.1488,    0.1504,    0.1767,    0.2321,    0.2921,         0,         0,         0,         0],
   [    0.1477,    0.1523,    0.1797,    0.2326,    0.2876,         0,         0,         0,         0],
   [    0.1453,    0.1527,    0.1817,    0.2339,    0.2864,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
===== Échantillon 1 =====
===== Encoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        quelle         belle          journée        <END>          <PAD>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,1410         0,1425         0,1721         0,2363         0,3081         0,0000         0,0000         0,0000         0,0000
quelle         0,1461         0,1464         0,1737         0,2336         0,3002         0,0000         0,0000         0,0000         0,0000
belle          0,1488         0,1504         0,1767         0,2321         0,2921         0,0000         0,0000         0,0000         0,0000
journée        0,1477         0,1523         0,1797         0,2326         0,2876         0,0000         0,0000         0,0000         0,0000
<END>          0,1453         0,1527         0,1817         0,2339         0,2864         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        quelle         belle          <END>          <PAD>          <PAD>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        1,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
quelle         0,4900         0,5100         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
belle          0,3551         0,3729         0,2720         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<END>          0,3078         0,3285         0,2375         0,1262         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Cross-Attention Weights =====
===== Tête 1 =====
Requête        <START>        quelle         belle          journée        <END>          <PAD>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,2197         0,1856         0,1529         0,1428         0,1446         0,1543         0,0000         0,0000         0,0000
quelle         0,2173         0,1853         0,1535         0,1437         0,1452         0,1551         0,0000         0,0000         0,0000
belle          0,2144         0,1845         0,1541         0,1449         0,1462         0,1558         0,0000         0,0000         0,0000
<END>          0,2115         0,1829         0,1543         0,1461         0,1481         0,1572         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0], 
   [0], 
   [0], 
   [0],
   [0]]]]
Attention Weights:
[[[[    1.0000,         0,         0,         0,         0,         0,         0,         0,         0],
   [    0.4823,    0.5177,         0,         0,         0,         0,         0,         0,         0],
   [    0.3464,    0.3759,    0.2777,         0,         0,         0,         0,         0,         0],
   [    0.2998,    0.3307,    0.2427,    0.1268,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Attention Weights:
[[[[    0.2556,    0.2170,    0.1828,    0.1708,    0.1738,         0,         0,         0,         0],
   [    0.2528,    0.2163,    0.1835,    0.1721,    0.1754,         0,         0,         0,         0],
   [    0.2493,    0.2154,    0.1845,    0.1737,    0.1770,         0,         0,         0,         0],
   [    0.2462,    0.2140,    0.1851,    0.1754,    0.1793,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Perte pour ce batch: 3.0834043
Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0], 
   [0], 
   [0],
   [0]]]]
Attention Weights:
[[[[    0.1414,    0.1417,    0.1720,    0.2350,    0.3099,         0,         0,         0,         0],
   [    0.1465,    0.1455,    0.1736,    0.2327,    0.3018,         0,         0,         0,         0],
   [    0.1486,    0.1488,    0.1761,    0.2316,    0.2949,         0,         0,         0,         0],
   [    0.1479,    0.1513,    0.1792,    0.2317,    0.2898,         0,         0,         0,         0],
   [    0.1455,    0.1520,    0.1813,    0.2328,    0.2884,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
===== Échantillon 1 =====
===== Encoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        le             chat           sol            <END>          <PAD>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,1414         0,1417         0,1720         0,2350         0,3099         0,0000         0,0000         0,0000         0,0000
le             0,1465         0,1455         0,1736         0,2327         0,3018         0,0000         0,0000         0,0000         0,0000
chat           0,1486         0,1488         0,1761         0,2316         0,2949         0,0000         0,0000         0,0000         0,0000
sol            0,1479         0,1513         0,1792         0,2317         0,2898         0,0000         0,0000         0,0000         0,0000
<END>          0,1455         0,1520         0,1813         0,2328         0,2884         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        le             chat           <END>          <PAD>          <PAD>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        1,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
le             0,4823         0,5177         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
chat           0,3464         0,3759         0,2777         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<END>          0,2998         0,3307         0,2427         0,1268         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Cross-Attention Weights =====
===== Tête 1 =====
Requête        <START>        le             chat           sol            <END>          <PAD>          <PAD>          <PAD>          <PAD>
-----------------------
-----------------------
-
-
------------------------------------------------------------------------------------------------------
<START>        0,2556         0,2170         0,1828         0,1708         0,1738         0,0000         0,0000         0,0000         0,0000
le             0,2528         0,2163         0,1835         0,1721         0,1754         0,0000         0,0000         0,0000         0,0000
chat           0,2493         0,2154         0,1845         0,1737         0,1770         0,0000         0,0000         0,0000         0,0000
<END>          0,2462         0,2140         0,1851         0,1754         0,1793         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0], 
   [0], 
   [0], 
   [0],
   [0]]]]
Attention Weights:
[[[[    1.0000,         0,         0,         0,         0,         0,         0,         0,         0],
   [    0.4811,    0.5189,         0,         0,         0,         0,         0,         0,         0],
   [    0.3429,    0.3743,    0.2828,         0,         0,         0,         0,         0,         0],
   [    0.2962,    0.3294,    0.2470,    0.1273,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Attention Weights:
[[[[    0.2533,    0.2175,    0.1828,    0.1725,    0.1739,         0,         0,         0,         0],
   [    0.2505,    0.2168,    0.1834,    0.1738,    0.1755,         0,         0,         0,         0],
   [    0.2470,    0.2156,    0.1842,    0.1756,    0.1775,         0,         0,         0,         0],
   [    0.2439,    0.2141,    0.1847,    0.1773,    0.1799,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Perte pour ce batch: 2.4082599
Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0]]]]
Attention Weights:
[[[[    0.0753,    0.0753,    0.0911,    0.1239,    0.1655,    0.1794,    0.1611,    0.1283,         0],
   [    0.0790,    0.0786,    0.0934,    0.1245,    0.1637,    0.1760,    0.1582,    0.1266,         0],
   [    0.0811,    0.0813,    0.0959,    0.1252,    0.1614,    0.1725,    0.1561,    0.1265,         0],
   [    0.0813,    0.0832,    0.0982,    0.1258,    0.1589,    0.1692,    0.1552,    0.1282,         0],
   [    0.0793,    0.0828,    0.0988,    0.1262,    0.1581,    0.1686,    0.1560,    0.1302,         0],
   [    0.0788,    0.0827,    0.0990,    0.1269,    0.1588,    0.1690,    0.1556,    0.1292,         0],
   [    0.0793,    0.0818,    0.0978,    0.1274,    0.1620,    0.1717,    0.1548,    0.1251,         0],
   [    0.0800,    0.0807,    0.0962,    0.1278,    0.1656,    0.1751,    0.1540,    0.1206,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
===== Échantillon 1 =====
===== Encoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        <UNK>          <UNK>          est            un             mauvais        film           <END>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,0753         0,0753         0,0911         0,1239         0,1655         0,1794         0,1611         0,1283         0,0000
<UNK>          0,0790         0,0786         0,0934         0,1245         0,1637         0,1760         0,1582         0,1266         0,0000
<UNK>          0,0811         0,0813         0,0959         0,1252         0,1614         0,1725         0,1561         0,1265         0,0000
est            0,0813         0,0832         0,0982         0,1258         0,1589         0,1692         0,1552         0,1282         0,0000
un             0,0793         0,0828         0,0988         0,1262         0,1581         0,1686         0,1560         0,1302         0,0000
mauvais        0,0788         0,0827         0,0990         0,1269         0,1588         0,1690         0,1556         0,1292         0,0000
film           0,0793         0,0818         0,0978         0,1274         0,1620         0,1717         0,1548         0,1251         0,0000
<END>          0,0800         0,0807         0,0962         0,1278         0,1656         0,1751         0,1540         0,1206         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        <UNK>          <UNK>          est            un             mauvais        <END>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        1,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<UNK>          0,4811         0,5189         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<UNK>          0,3429         0,3743         0,2828         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
est            0,2962         0,3294         0,2470         0,1273         0,0000         0,0000         0,0000         0,0000         0,0000
un             0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
mauvais        0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<END>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Cross-Attention Weights =====
===== Tête 1 =====
Requête        <START>        <UNK>          <UNK>          est            un             mauvais        film           <END>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,2533         0,2175         0,1828         0,1725         0,1739         0,0000         0,0000         0,0000         0,0000
<UNK>          0,2505         0,2168         0,1834         0,1738         0,1755         0,0000         0,0000         0,0000         0,0000
<UNK>          0,2470         0,2156         0,1842         0,1756         0,1775         0,0000         0,0000         0,0000         0,0000
est            0,2439         0,2141         0,1847         0,1773         0,1799         0,0000         0,0000         0,0000         0,0000
un             0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
mauvais        0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<END>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0],
   [0]]]]
Attention Weights:
[[[[    1.0000,         0,         0,         0,         0,         0,         0,         0,         0],
   [    0.4880,    0.5120,         0,         0,         0,         0,         0,         0,         0],
   [    0.3512,    0.3728,    0.2760,         0,         0,         0,         0,         0,         0],
   [    0.3023,    0.3264,    0.2394,    0.1319,         0,         0,         0,         0,         0],
   [    0.2792,    0.3068,    0.2245,    0.1219,    0.0675,         0,         0,         0,         0],
   [    0.2606,    0.2897,    0.2126,    0.1150,    0.0633,    0.0588,         0,         0,         0],
   [    0.2373,    0.2641,    0.1945,    0.1052,    0.0578,    0.0532,    0.0880,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Attention Weights:
[[[[    0.1557,    0.1333,    0.1119,    0.1042,    0.1068,    0.1130,    0.1252,    0.1498,         0],
   [    0.1537,    0.1327,    0.1120,    0.1046,    0.1072,    0.1136,    0.1258,    0.1504,         0],
   [    0.1515,    0.1318,    0.1123,    0.1055,    0.1085,    0.1147,    0.1262,    0.1495,         0],
   [    0.1497,    0.1307,    0.1126,    0.1066,    0.1102,    0.1162,    0.1265,    0.1475,         0],
   [    0.1480,    0.1298,    0.1128,    0.1077,    0.1119,    0.1175,    0.1266,    0.1455,         0],
   [    0.1474,    0.1298,    0.1132,    0.1081,    0.1123,    0.1176,    0.1265,    0.1451,         0],
   [    0.1488,    0.1311,    0.1136,    0.1073,    0.1103,    0.1156,    0.1259,    0.1474,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Perte pour ce batch: 3.6337411
Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0], 
   [0],
   [0]]]]
Attention Weights:
[[[[    0.1061,    0.1057,    0.1281,    0.1745,    0.2305,    0.2552,         0,         0,         0],
   [    0.1108,    0.1095,    0.1306,    0.1745,    0.2263,    0.2484,         0,         0,         0],
   [    0.1132,    0.1125,    0.1331,    0.1749,    0.2230,    0.2433,         0,         0,         0],
   [    0.1136,    0.1152,    0.1364,    0.1760,    0.2200,    0.2388,         0,         0,         0],
   [    0.1115,    0.1155,    0.1379,    0.1772,    0.2196,    0.2383,         0,         0,         0],
   [    0.1101,    0.1147,    0.1373,    0.1776,    0.2209,    0.2394,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
===== Échantillon 1 =====
===== Encoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        ce             livre          est            intéressant    <END>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,1061         0,1057         0,1281         0,1745         0,2305         0,2552         0,0000         0,0000         0,0000
ce             0,1108         0,1095         0,1306         0,1745         0,2263         0,2484         0,0000         0,0000         0,0000
livre          0,1132         0,1125         0,1331         0,1749         0,2230         0,2433         0,0000         0,0000         0,0000
est            0,1136         0,1152         0,1364         0,1760         0,2200         0,2388         0,0000         0,0000         0,0000
intéressant    0,1115         0,1155         0,1379         0,1772         0,2196         0,2383         0,0000         0,0000         0,0000
<END>          0,1101         0,1147         0,1373         0,1776         0,2209         0,2394         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        le             livre          est            intéressant    <END>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        1,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
le             0,4880         0,5120         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
livre          0,3512         0,3728         0,2760         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
est            0,3023         0,3264         0,2394         0,1319         0,0000         0,0000         0,0000         0,0000         0,0000
intéressant    0,2792         0,3068         0,2245         0,1219         0,0675         0,0000         0,0000         0,0000         0,0000
<END>          0,2606         0,2897         0,2126         0,1150         0,0633         0,0588         0,0000         0,0000         0,0000
<PAD>          0,2373         0,2641         0,1945         0,1052         0,0578         0,0532         0,0880         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Cross-Attention Weights =====
===== Tête 1 =====
Requête        <START>        ce             livre          est            intéressant    <END>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,1557         0,1333         0,1119         0,1042         0,1068         0,1130         0,1252         0,1498         0,0000
le             0,1537         0,1327         0,1120         0,1046         0,1072         0,1136         0,1258         0,1504         0,0000
livre          0,1515         0,1318         0,1123         0,1055         0,1085         0,1147         0,1262         0,1495         0,0000
est            0,1497         0,1307         0,1126         0,1066         0,1102         0,1162         0,1265         0,1475         0,0000
intéressant    0,1480         0,1298         0,1128         0,1077         0,1119         0,1175         0,1266         0,1455         0,0000
<END>          0,1474         0,1298         0,1132         0,1081         0,1123         0,1176         0,1265         0,1451         0,0000
<PAD>          0,1488         0,1311         0,1136         0,1073         0,1103         0,1156         0,1259         0,1474         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0], 
   [0],
   [0]]]]
Attention Weights:
[[[[    1.0000,         0,         0,         0,         0,         0,         0,         0,         0],
   [    0.4876,    0.5124,         0,         0,         0,         0,         0,         0,         0],
   [    0.3477,    0.3695,    0.2828,         0,         0,         0,         0,         0,         0],
   [    0.2962,    0.3224,    0.2447,    0.1367,         0,         0,         0,         0,         0],
   [    0.2723,    0.3025,    0.2285,    0.1259,    0.0706,         0,         0,         0,         0],
   [    0.2536,    0.2851,    0.2159,    0.1186,    0.0662,    0.0606,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Attention Weights:
[[[[    0.2104,    0.1823,    0.1545,    0.1453,    0.1487,    0.1588,         0,         0,         0],
   [    0.2077,    0.1815,    0.1547,    0.1461,    0.1498,    0.1602,         0,         0,         0],
   [    0.2047,    0.1802,    0.1549,    0.1471,    0.1514,    0.1616,         0,         0,         0],
   [    0.2015,    0.1782,    0.1546,    0.1483,    0.1537,    0.1637,         0,         0,         0],
   [    0.1985,    0.1764,    0.1544,    0.1495,    0.1558,    0.1654,         0,         0,         0],
   [    0.1976,    0.1761,    0.1547,    0.1500,    0.1562,    0.1655,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Perte pour ce batch: 3.0842896
Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [1.0000]]]]
Attention Weights:
[[[[    0.0680,    0.0674,    0.0821,    0.1129,    0.1495,    0.1635,    0.1437,    0.1141,    0.0988],
   [    0.0718,    0.0706,    0.0846,    0.1138,    0.1479,    0.1601,    0.1407,    0.1126,    0.0980],
   [    0.0732,    0.0726,    0.0865,    0.1145,    0.1464,    0.1570,    0.1389,    0.1126,    0.0984],
   [    0.0726,    0.0736,    0.0879,    0.1148,    0.1444,    0.1538,    0.1382,    0.1142,    0.1004],
   [    0.0710,    0.0736,    0.0885,    0.1150,    0.1434,    0.1523,    0.1385,    0.1157,    0.1021],
   [    0.0707,    0.0737,    0.0887,    0.1156,    0.1442,    0.1528,    0.1385,    0.1150,    0.1009],
   [    0.0713,    0.0735,    0.0882,    0.1165,    0.1472,    0.1561,    0.1384,    0.1120,    0.0969],
   [    0.0724,    0.0730,    0.0873,    0.1171,    0.1507,    0.1601,    0.1384,    0.1085,    0.0924],
   [    0.0718,    0.0718,    0.0860,    0.1173,    0.1532,    0.1635,    0.1393,    0.1069,    0.0901]]]]
===== Échantillon 1 =====
===== Encoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        je             <UNK>          <UNK>          <UNK>          pas            ce             repas          <END>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,0680         0,0674         0,0821         0,1129         0,1495         0,1635         0,1437         0,1141         0,0988
je             0,0718         0,0706         0,0846         0,1138         0,1479         0,1601         0,1407         0,1126         0,0980
<UNK>          0,0732         0,0726         0,0865         0,1145         0,1464         0,1570         0,1389         0,1126         0,0984
<UNK>          0,0726         0,0736         0,0879         0,1148         0,1444         0,1538         0,1382         0,1142         0,1004
<UNK>          0,0710         0,0736         0,0885         0,1150         0,1434         0,1523         0,1385         0,1157         0,1021
pas            0,0707         0,0737         0,0887         0,1156         0,1442         0,1528         0,1385         0,1150         0,1009
ce             0,0713         0,0735         0,0882         0,1165         0,1472         0,1561         0,1384         0,1120         0,0969
repas          0,0724         0,0730         0,0873         0,1171         0,1507         0,1601         0,1384         0,1085         0,0924
<END>          0,0718         0,0718         0,0860         0,1173         0,1532         0,1635         0,1393         0,1069         0,0901

===== Decoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        je             <UNK>          <UNK>          <UNK>          pas            <END>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        1,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
je             0,4876         0,5124         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<UNK>          0,3477         0,3695         0,2828         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<UNK>          0,2962         0,3224         0,2447         0,1367         0,0000         0,0000         0,0000         0,0000         0,0000
<UNK>          0,2723         0,3025         0,2285         0,1259         0,0706         0,0000         0,0000         0,0000         0,0000
pas            0,2536         0,2851         0,2159         0,1186         0,0662         0,0606         0,0000         0,0000         0,0000
<END>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Cross-Attention Weights =====
===== Tête 1 =====
Requête        <START>        je             <UNK>          <UNK>          <UNK>          pas            ce             repas          <END>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,2104         0,1823         0,1545         0,1453         0,1487         0,1588         0,0000         0,0000         0,0000
je             0,2077         0,1815         0,1547         0,1461         0,1498         0,1602         0,0000         0,0000         0,0000
<UNK>          0,2047         0,1802         0,1549         0,1471         0,1514         0,1616         0,0000         0,0000         0,0000
<UNK>          0,2015         0,1782         0,1546         0,1483         0,1537         0,1637         0,0000         0,0000         0,0000
<UNK>          0,1985         0,1764         0,1544         0,1495         0,1558         0,1654         0,0000         0,0000         0,0000
pas            0,1976         0,1761         0,1547         0,1500         0,1562         0,1655         0,0000         0,0000         0,0000
<END>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0],
   [0]]]]
Attention Weights:
[[[[    1.0000,         0,         0,         0,         0,         0,         0,         0,         0],
   [    0.4848,    0.5152,         0,         0,         0,         0,         0,         0,         0],
   [    0.3443,    0.3710,    0.2846,         0,         0,         0,         0,         0,         0],
   [    0.2947,    0.3238,    0.2457,    0.1358,         0,         0,         0,         0,         0],
   [    0.2709,    0.3036,    0.2295,    0.1246,    0.0715,         0,         0,         0,         0],
   [    0.2522,    0.2859,    0.2165,    0.1170,    0.0667,    0.0617,         0,         0,         0],
   [    0.2284,    0.2591,    0.1971,    0.1067,    0.0606,    0.0556,    0.0925,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Attention Weights:
[[[[    0.1275,    0.1113,    0.0941,    0.0882,    0.0913,    0.0977,    0.1079,    0.1255,    0.1564],
   [    0.1258,    0.1107,    0.0941,    0.0885,    0.0918,    0.0983,    0.1085,    0.1259,    0.1564],
   [    0.1242,    0.1100,    0.0945,    0.0896,    0.0933,    0.0997,    0.1093,    0.1253,    0.1542],
   [    0.1228,    0.1092,    0.0950,    0.0910,    0.0955,    0.1019,    0.1102,    0.1240,    0.1505],
   [    0.1217,    0.1087,    0.0955,    0.0922,    0.0974,    0.1036,    0.1108,    0.1228,    0.1472],
   [    0.1212,    0.1086,    0.0958,    0.0927,    0.0980,    0.1041,    0.1108,    0.1224,    0.1463],
   [    0.1218,    0.1093,    0.0958,    0.0917,    0.0960,    0.1018,    0.1099,    0.1238,    0.1498],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Perte pour ce batch: 3.7498877
Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0], 
   [0],
   [0]]]]
Attention Weights:
[[[[    0.1056,    0.1050,    0.1286,    0.1741,    0.2314,    0.2552,         0,         0,         0],
   [    0.1102,    0.1088,    0.1314,    0.1740,    0.2272,    0.2484,         0,         0,         0],
   [    0.1126,    0.1118,    0.1342,    0.1745,    0.2239,    0.2429,         0,         0,         0],
   [    0.1126,    0.1141,    0.1373,    0.1755,    0.2214,    0.2390,         0,         0,         0],
   [    0.1102,    0.1141,    0.1384,    0.1766,    0.2216,    0.2391,         0,         0,         0],
   [    0.1088,    0.1134,    0.1380,    0.1771,    0.2227,    0.2399,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
===== Échantillon 1 =====
===== Encoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        ce             film           est            excellent      <END>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,1056         0,1050         0,1286         0,1741         0,2314         0,2552         0,0000         0,0000         0,0000
ce             0,1102         0,1088         0,1314         0,1740         0,2272         0,2484         0,0000         0,0000         0,0000
film           0,1126         0,1118         0,1342         0,1745         0,2239         0,2429         0,0000         0,0000         0,0000
est            0,1126         0,1141         0,1373         0,1755         0,2214         0,2390         0,0000         0,0000         0,0000
excellent      0,1102         0,1141         0,1384         0,1766         0,2216         0,2391         0,0000         0,0000         0,0000
<END>          0,1088         0,1134         0,1380         0,1771         0,2227         0,2399         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        le             film           est            excellent      <END>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        1,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
le             0,4848         0,5152         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
film           0,3443         0,3710         0,2846         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
est            0,2947         0,3238         0,2457         0,1358         0,0000         0,0000         0,0000         0,0000         0,0000
excellent      0,2709         0,3036         0,2295         0,1246         0,0715         0,0000         0,0000         0,0000         0,0000
<END>          0,2522         0,2859         0,2165         0,1170         0,0667         0,0617         0,0000         0,0000         0,0000
<PAD>          0,2284         0,2591         0,1971         0,1067         0,0606         0,0556         0,0925         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Cross-Attention Weights =====
===== Tête 1 =====
Requête        <START>        ce             film           est            excellent      <END>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,1275         0,1113         0,0941         0,0882         0,0913         0,0977         0,1079         0,1255         0,1564
le             0,1258         0,1107         0,0941         0,0885         0,0918         0,0983         0,1085         0,1259         0,1564
film           0,1242         0,1100         0,0945         0,0896         0,0933         0,0997         0,1093         0,1253         0,1542
est            0,1228         0,1092         0,0950         0,0910         0,0955         0,1019         0,1102         0,1240         0,1505
excellent      0,1217         0,1087         0,0955         0,0922         0,0974         0,1036         0,1108         0,1228         0,1472
<END>          0,1212         0,1086         0,0958         0,0927         0,0980         0,1041         0,1108         0,1224         0,1463
<PAD>          0,1218         0,1093         0,0958         0,0917         0,0960         0,1018         0,1099         0,1238         0,1498
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0], 
   [0],
   [0]]]]
Attention Weights:
[[[[    1.0000,         0,         0,         0,         0,         0,         0,         0,         0],
   [    0.4836,    0.5164,         0,         0,         0,         0,         0,         0,         0],
   [    0.3394,    0.3681,    0.2924,         0,         0,         0,         0,         0,         0],
   [    0.2859,    0.3180,    0.2508,    0.1453,         0,         0,         0,         0,         0],
   [    0.2605,    0.2967,    0.2334,    0.1330,    0.0764,         0,         0,         0,         0],
   [    0.2419,    0.2785,    0.2193,    0.1244,    0.0710,    0.0650,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Attention Weights:
[[[[    0.2047,    0.1807,    0.1549,    0.1464,    0.1515,    0.1618,         0,         0,         0],
   [    0.2018,    0.1796,    0.1551,    0.1472,    0.1528,    0.1635,         0,         0,         0],
   [    0.1986,    0.1781,    0.1551,    0.1483,    0.1546,    0.1653,         0,         0,         0],
   [    0.1951,    0.1757,    0.1545,    0.1496,    0.1573,    0.1679,         0,         0,         0],
   [    0.1919,    0.1735,    0.1540,    0.1508,    0.1598,    0.1700,         0,         0,         0],
   [    0.1908,    0.1731,    0.1543,    0.1513,    0.1603,    0.1702,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Perte pour ce batch: 2.7134972
Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0]]]]
Attention Weights:
[[[[    0.0754,    0.0746,    0.0906,    0.1258,    0.1663,    0.1817,    0.1591,    0.1265,         0],
   [    0.0794,    0.0781,    0.0930,    0.1264,    0.1644,    0.1780,    0.1560,    0.1246,         0],
   [    0.0809,    0.0801,    0.0951,    0.1269,    0.1629,    0.1753,    0.1544,    0.1244,         0],
   [    0.0804,    0.0815,    0.0971,    0.1274,    0.1609,    0.1725,    0.1539,    0.1262,         0],
   [    0.0784,    0.0814,    0.0980,    0.1278,    0.1603,    0.1716,    0.1545,    0.1280,         0],
   [    0.0776,    0.0811,    0.0979,    0.1283,    0.1612,    0.1724,    0.1544,    0.1271,         0],
   [    0.0784,    0.0808,    0.0969,    0.1289,    0.1637,    0.1747,    0.1534,    0.1232,         0],
   [    0.0795,    0.0801,    0.0953,    0.1295,    0.1669,    0.1777,    0.1523,    0.1187,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
===== Échantillon 1 =====
===== Encoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        je             suis           triste         <UNK>          <UNK>          <UNK>          <END>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,0754         0,0746         0,0906         0,1258         0,1663         0,1817         0,1591         0,1265         0,0000
je             0,0794         0,0781         0,0930         0,1264         0,1644         0,1780         0,1560         0,1246         0,0000
suis           0,0809         0,0801         0,0951         0,1269         0,1629         0,1753         0,1544         0,1244         0,0000
triste         0,0804         0,0815         0,0971         0,1274         0,1609         0,1725         0,1539         0,1262         0,0000
<UNK>          0,0784         0,0814         0,0980         0,1278         0,1603         0,1716         0,1545         0,1280         0,0000
<UNK>          0,0776         0,0811         0,0979         0,1283         0,1612         0,1724         0,1544         0,1271         0,0000
<UNK>          0,0784         0,0808         0,0969         0,1289         0,1637         0,1747         0,1534         0,1232         0,0000
<END>          0,0795         0,0801         0,0953         0,1295         0,1669         0,1777         0,1523         0,1187         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        je             suis           triste         <END>          <PAD>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        1,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
je             0,4836         0,5164         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
suis           0,3394         0,3681         0,2924         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
triste         0,2859         0,3180         0,2508         0,1453         0,0000         0,0000         0,0000         0,0000         0,0000
<END>          0,2605         0,2967         0,2334         0,1330         0,0764         0,0000         0,0000         0,0000         0,0000
<PAD>          0,2419         0,2785         0,2193         0,1244         0,0710         0,0650         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Cross-Attention Weights =====
===== Tête 1 =====
Requête        <START>        je             suis           triste         <UNK>          <UNK>          <UNK>          <END>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,2047         0,1807         0,1549         0,1464         0,1515         0,1618         0,0000         0,0000         0,0000
je             0,2018         0,1796         0,1551         0,1472         0,1528         0,1635         0,0000         0,0000         0,0000
suis           0,1986         0,1781         0,1551         0,1483         0,1546         0,1653         0,0000         0,0000         0,0000
triste         0,1951         0,1757         0,1545         0,1496         0,1573         0,1679         0,0000         0,0000         0,0000
<END>          0,1919         0,1735         0,1540         0,1508         0,1598         0,1700         0,0000         0,0000         0,0000
<PAD>          0,1908         0,1731         0,1543         0,1513         0,1603         0,1702         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0], 
   [0], 
   [0],
   [0]]]]
Attention Weights:
[[[[    1.0000,         0,         0,         0,         0,         0,         0,         0,         0],
   [    0.4814,    0.5186,         0,         0,         0,         0,         0,         0,         0],
   [    0.3356,    0.3674,    0.2970,         0,         0,         0,         0,         0,         0],
   [    0.2827,    0.3170,    0.2543,    0.1459,         0,         0,         0,         0,         0],
   [    0.2573,    0.2950,    0.2361,    0.1328,    0.0788,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Attention Weights:
[[[[    0.1474,    0.1301,    0.1119,    0.1047,    0.1081,    0.1157,    0.1282,    0.1538,         0],
   [    0.1453,    0.1293,    0.1119,    0.1051,    0.1089,    0.1167,    0.1289,    0.1540,         0],
   [    0.1429,    0.1280,    0.1120,    0.1060,    0.1104,    0.1182,    0.1295,    0.1530,         0],
   [    0.1404,    0.1262,    0.1117,    0.1073,    0.1129,    0.1206,    0.1302,    0.1506,         0],
   [    0.1384,    0.1249,    0.1117,    0.1084,    0.1150,    0.1225,    0.1306,    0.1485,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Perte pour ce batch: 3.2826965
Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0], 
   [0],
   [0]]]]
Attention Weights:
[[[[    0.1053,    0.1047,    0.1275,    0.1742,    0.2327,    0.2555,         0,         0,         0],
   [    0.1098,    0.1085,    0.1304,    0.1741,    0.2285,    0.2487,         0,         0,         0],
   [    0.1122,    0.1113,    0.1330,    0.1745,    0.2255,    0.2435,         0,         0,         0],
   [    0.1118,    0.1135,    0.1361,    0.1756,    0.2234,    0.2396,         0,         0,         0],
   [    0.1092,    0.1132,    0.1370,    0.1766,    0.2240,    0.2399,         0,         0,         0],
   [    0.1078,    0.1124,    0.1366,    0.1772,    0.2252,    0.2408,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
===== Échantillon 1 =====
===== Encoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        ce             temps          est            agréable       <END>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,1053         0,1047         0,1275         0,1742         0,2327         0,2555         0,0000         0,0000         0,0000
ce             0,1098         0,1085         0,1304         0,1741         0,2285         0,2487         0,0000         0,0000         0,0000
temps          0,1122         0,1113         0,1330         0,1745         0,2255         0,2435         0,0000         0,0000         0,0000
est            0,1118         0,1135         0,1361         0,1756         0,2234         0,2396         0,0000         0,0000         0,0000
agréable       0,1092         0,1132         0,1370         0,1766         0,2240         0,2399         0,0000         0,0000         0,0000
<END>          0,1078         0,1124         0,1366         0,1772         0,2252         0,2408         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        le             temps          est            agréable       <END>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        1,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
le             0,4814         0,5186         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
temps          0,3356         0,3674         0,2970         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
est            0,2827         0,3170         0,2543         0,1459         0,0000         0,0000         0,0000         0,0000         0,0000
agréable       0,2573         0,2950         0,2361         0,1328         0,0788         0,0000         0,0000         0,0000         0,0000
<END>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Cross-Attention Weights =====
===== Tête 1 =====
Requête        <START>        ce             temps          est            agréable       <END>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,1474         0,1301         0,1119         0,1047         0,1081         0,1157         0,1282         0,1538         0,0000
le             0,1453         0,1293         0,1119         0,1051         0,1089         0,1167         0,1289         0,1540         0,0000
temps          0,1429         0,1280         0,1120         0,1060         0,1104         0,1182         0,1295         0,1530         0,0000
est            0,1404         0,1262         0,1117         0,1073         0,1129         0,1206         0,1302         0,1506         0,0000
agréable       0,1384         0,1249         0,1117         0,1084         0,1150         0,1225         0,1306         0,1485         0,0000
<END>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0], 
   [0],
   [0]]]]
Attention Weights:
[[[[    1.0000,         0,         0,         0,         0,         0,         0,         0,         0],
   [    0.4790,    0.5210,         0,         0,         0,         0,         0,         0,         0],
   [    0.3308,    0.3660,    0.3033,         0,         0,         0,         0,         0,         0],
   [    0.2750,    0.3131,    0.2582,    0.1538,         0,         0,         0,         0,         0],
   [    0.2486,    0.2906,    0.2389,    0.1395,    0.0825,         0,         0,         0,         0],
   [    0.2294,    0.2712,    0.2234,    0.1299,    0.0764,    0.0697,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Attention Weights:
[[[[    0.2013,    0.1806,    0.1556,    0.1470,    0.1516,    0.1639,         0,         0,         0],
   [    0.1982,    0.1793,    0.1555,    0.1478,    0.1533,    0.1658,         0,         0,         0],
   [    0.1948,    0.1774,    0.1553,    0.1489,    0.1555,    0.1680,         0,         0,         0],
   [    0.1909,    0.1746,    0.1546,    0.1503,    0.1586,    0.1710,         0,         0,         0],
   [    0.1871,    0.1719,    0.1539,    0.1516,    0.1618,    0.1738,         0,         0,         0],
   [    0.1861,    0.1715,    0.1541,    0.1521,    0.1623,    0.1740,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Perte pour ce batch: 2.9994001
Epoch 2 completed with average loss: 0.036653385
Epoch 3 / 3
Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0], 
   [0],
   [0]]]]
Attention Weights:
[[[[    0.1048,    0.1050,    0.1261,    0.1752,    0.2332,    0.2557,         0,         0,         0],
   [    0.1092,    0.1085,    0.1283,    0.1749,    0.2296,    0.2494,         0,         0,         0],
   [    0.1116,    0.1119,    0.1314,    0.1757,    0.2261,    0.2434,         0,         0,         0],
   [    0.1106,    0.1134,    0.1338,    0.1773,    0.2244,    0.2405,         0,         0,         0],
   [    0.1082,    0.1133,    0.1351,    0.1788,    0.2244,    0.2401,         0,         0,         0],
   [    0.1068,    0.1125,    0.1348,    0.1792,    0.2256,    0.2410,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
===== Échantillon 1 =====
===== Encoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        chat           mange          la             souris         <END>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,1048         0,1050         0,1261         0,1752         0,2332         0,2557         0,0000         0,0000         0,0000
chat           0,1092         0,1085         0,1283         0,1749         0,2296         0,2494         0,0000         0,0000         0,0000
mange          0,1116         0,1119         0,1314         0,1757         0,2261         0,2434         0,0000         0,0000         0,0000
la             0,1106         0,1134         0,1338         0,1773         0,2244         0,2405         0,0000         0,0000         0,0000
souris         0,1082         0,1133         0,1351         0,1788         0,2244         0,2401         0,0000         0,0000         0,0000
<END>          0,1068         0,1125         0,1348         0,1792         0,2256         0,2410         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        le             chat           mange          <END>          <PAD>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        1,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
le             0,4790         0,5210         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
chat           0,3308         0,3660         0,3033         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
mange          0,2750         0,3131         0,2582         0,1538         0,0000         0,0000         0,0000         0,0000         0,0000
<END>          0,2486         0,2906         0,2389         0,1395         0,0825         0,0000         0,0000         0,0000         0,0000
<PAD>          0,2294         0,2712         0,2234         0,1299         0,0764         0,0697         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Cross-Attention Weights =====
===== Tête 1 =====
Requête        <START>        chat           mange          la             souris         <END>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,2013         0,1806         0,1556         0,1470         0,1516         0,1639         0,0000         0,0000         0,0000
le             0,1982         0,1793         0,1555         0,1478         0,1533         0,1658         0,0000         0,0000         0,0000
chat           0,1948         0,1774         0,1553         0,1489         0,1555         0,1680         0,0000         0,0000         0,0000
mange          0,1909         0,1746         0,1546         0,1503         0,1586         0,1710         0,0000         0,0000         0,0000
<END>          0,1871         0,1719         0,1539         0,1516         0,1618         0,1738         0,0000         0,0000         0,0000
<PAD>          0,1861         0,1715         0,1541         0,1521         0,1623         0,1740         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0], 
   [0], 
   [0],
   [0]]]]
Attention Weights:
[[[[    1.0000,         0,         0,         0,         0,         0,         0,         0,         0],
   [    0.4746,    0.5254,         0,         0,         0,         0,         0,         0,         0],
   [    0.3260,    0.3680,    0.3061,         0,         0,         0,         0,         0,         0],
   [    0.2703,    0.3146,    0.2609,    0.1542,         0,         0,         0,         0,         0],
   [    0.2440,    0.2908,    0.2408,    0.1394,    0.0850,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Attention Weights:
[[[[    0.1985,    0.1786,    0.1550,    0.1479,    0.1539,    0.1661,         0,         0,         0],
   [    0.1956,    0.1772,    0.1548,    0.1487,    0.1556,    0.1681,         0,         0,         0],
   [    0.1917,    0.1749,    0.1547,    0.1501,    0.1581,    0.1706,         0,         0,         0],
   [    0.1875,    0.1718,    0.1538,    0.1514,    0.1615,    0.1741,         0,         0,         0],
   [    0.1836,    0.1690,    0.1530,    0.1526,    0.1647,    0.1771,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Perte pour ce batch: 2.7919774
Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0],
   [0]]]]
Attention Weights:
[[[[    0.0856,    0.0850,    0.1042,    0.1448,    0.1908,    0.2075,    0.1820,         0,         0],
   [    0.0901,    0.0886,    0.1067,    0.1453,    0.1882,    0.2030,    0.1781,         0,         0],
   [    0.0919,    0.0910,    0.1091,    0.1462,    0.1865,    0.1995,    0.1758,         0,         0],
   [    0.0911,    0.0922,    0.1113,    0.1473,    0.1853,    0.1971,    0.1758,         0,         0],
   [    0.0887,    0.0916,    0.1119,    0.1480,    0.1855,    0.1971,    0.1771,         0,         0],
   [    0.0878,    0.0911,    0.1117,    0.1485,    0.1864,    0.1979,    0.1767,         0,         0],
   [    0.0883,    0.0903,    0.1103,    0.1486,    0.1882,    0.1997,    0.1746,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
===== Échantillon 1 =====
===== Encoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        chien          court          dans           le             jardin         <END>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,0856         0,0850         0,1042         0,1448         0,1908         0,2075         0,1820         0,0000         0,0000
chien          0,0901         0,0886         0,1067         0,1453         0,1882         0,2030         0,1781         0,0000         0,0000
court          0,0919         0,0910         0,1091         0,1462         0,1865         0,1995         0,1758         0,0000         0,0000
dans           0,0911         0,0922         0,1113         0,1473         0,1853         0,1971         0,1758         0,0000         0,0000
le             0,0887         0,0916         0,1119         0,1480         0,1855         0,1971         0,1771         0,0000         0,0000
jardin         0,0878         0,0911         0,1117         0,1485         0,1864         0,1979         0,1767         0,0000         0,0000
<END>          0,0883         0,0903         0,1103         0,1486         0,1882         0,1997         0,1746         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        le             chien          court          <END>          <PAD>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        1,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
le             0,4746         0,5254         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
chien          0,3260         0,3680         0,3061         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
court          0,2703         0,3146         0,2609         0,1542         0,0000         0,0000         0,0000         0,0000         0,0000
<END>          0,2440         0,2908         0,2408         0,1394         0,0850         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Cross-Attention Weights =====
===== Tête 1 =====
Requête        <START>        chien          court          dans           le             jardin         <END>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,1985         0,1786         0,1550         0,1479         0,1539         0,1661         0,0000         0,0000         0,0000
le             0,1956         0,1772         0,1548         0,1487         0,1556         0,1681         0,0000         0,0000         0,0000
chien          0,1917         0,1749         0,1547         0,1501         0,1581         0,1706         0,0000         0,0000         0,0000
court          0,1875         0,1718         0,1538         0,1514         0,1615         0,1741         0,0000         0,0000         0,0000
<END>          0,1836         0,1690         0,1530         0,1526         0,1647         0,1771         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0], 
   [0], 
   [0],
   [0]]]]
Attention Weights:
[[[[    1.0000,         0,         0,         0,         0,         0,         0,         0,         0],
   [    0.4777,    0.5223,         0,         0,         0,         0,         0,         0,         0],
   [    0.3267,    0.3648,    0.3084,         0,         0,         0,         0,         0,         0],
   [    0.2693,    0.3099,    0.2607,    0.1601,         0,         0,         0,         0,         0],
   [    0.2419,    0.2856,    0.2395,    0.1441,    0.0889,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Attention Weights:
[[[[    0.1668,    0.1497,    0.1305,    0.1237,    0.1316,    0.1409,    0.1568,         0,         0],
   [    0.1640,    0.1482,    0.1303,    0.1242,    0.1329,    0.1424,    0.1579,         0,         0],
   [    0.1606,    0.1461,    0.1300,    0.1252,    0.1349,    0.1445,    0.1587,         0,         0],
   [    0.1566,    0.1431,    0.1290,    0.1264,    0.1380,    0.1476,    0.1594,         0,         0],
   [    0.1531,    0.1406,    0.1281,    0.1275,    0.1406,    0.1502,    0.1598,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Perte pour ce batch: 2.6654644
Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0],
   [0]]]]
Attention Weights:
[[[[    0.0858,    0.0852,    0.1037,    0.1432,    0.1902,    0.2095,    0.1824,         0,         0],
   [    0.0897,    0.0885,    0.1062,    0.1439,    0.1878,    0.2052,    0.1787,         0,         0],
   [    0.0921,    0.0913,    0.1086,    0.1447,    0.1857,    0.2015,    0.1762,         0,         0],
   [    0.0910,    0.0924,    0.1106,    0.1457,    0.1844,    0.1995,    0.1764,         0,         0],
   [    0.0884,    0.0920,    0.1116,    0.1467,    0.1844,    0.1994,    0.1775,         0,         0],
   [    0.0877,    0.0916,    0.1114,    0.1470,    0.1853,    0.2001,    0.1770,         0,         0],
   [    0.0883,    0.0907,    0.1100,    0.1469,    0.1874,    0.2018,    0.1749,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
===== Échantillon 1 =====
===== Encoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        les            chats          aiment         les            chiens         <END>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,0858         0,0852         0,1037         0,1432         0,1902         0,2095         0,1824         0,0000         0,0000
les            0,0897         0,0885         0,1062         0,1439         0,1878         0,2052         0,1787         0,0000         0,0000
chats          0,0921         0,0913         0,1086         0,1447         0,1857         0,2015         0,1762         0,0000         0,0000
aiment         0,0910         0,0924         0,1106         0,1457         0,1844         0,1995         0,1764         0,0000         0,0000
les            0,0884         0,0920         0,1116         0,1467         0,1844         0,1994         0,1775         0,0000         0,0000
chiens         0,0877         0,0916         0,1114         0,1470         0,1853         0,2001         0,1770         0,0000         0,0000
<END>          0,0883         0,0907         0,1100         0,1469         0,1874         0,2018         0,1749         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        les            chats          aiment         <END>          <PAD>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        1,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
les            0,4777         0,5223         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
chats          0,3267         0,3648         0,3084         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
aiment         0,2693         0,3099         0,2607         0,1601         0,0000         0,0000         0,0000         0,0000         0,0000
<END>          0,2419         0,2856         0,2395         0,1441         0,0889         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Cross-Attention Weights =====
===== Tête 1 =====
Requête        <START>        les            chats          aiment         les            chiens         <END>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,1668         0,1497         0,1305         0,1237         0,1316         0,1409         0,1568         0,0000         0,0000
les            0,1640         0,1482         0,1303         0,1242         0,1329         0,1424         0,1579         0,0000         0,0000
chats          0,1606         0,1461         0,1300         0,1252         0,1349         0,1445         0,1587         0,0000         0,0000
aiment         0,1566         0,1431         0,1290         0,1264         0,1380         0,1476         0,1594         0,0000         0,0000
<END>          0,1531         0,1406         0,1281         0,1275         0,1406         0,1502         0,1598         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0], 
   [0], 
   [0],
   [0]]]]
Attention Weights:
[[[[    1.0000,         0,         0,         0,         0,         0,         0,         0,         0],
   [    0.4752,    0.5248,         0,         0,         0,         0,         0,         0,         0],
   [    0.3219,    0.3640,    0.3142,         0,         0,         0,         0,         0,         0],
   [    0.2634,    0.3076,    0.2644,    0.1646,         0,         0,         0,         0,         0],
   [    0.2364,    0.2831,    0.2424,    0.1477,    0.0903,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Attention Weights:
[[[[    0.1659,    0.1503,    0.1302,    0.1248,    0.1314,    0.1403,    0.1570,         0,         0],
   [    0.1633,    0.1489,    0.1299,    0.1253,    0.1327,    0.1419,    0.1582,         0,         0],
   [    0.1594,    0.1463,    0.1294,    0.1263,    0.1351,    0.1444,    0.1591,         0,         0],
   [    0.1552,    0.1429,    0.1283,    0.1275,    0.1383,    0.1478,    0.1600,         0,         0],
   [    0.1515,    0.1401,    0.1275,    0.1285,    0.1410,    0.1507,    0.1606,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Perte pour ce batch: 3.013808
Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0], 
   [0],
   [0]]]]
Attention Weights:
[[[[    0.1046,    0.1048,    0.1255,    0.1757,    0.2338,    0.2556,         0,         0,         0],
   [    0.1089,    0.1083,    0.1279,    0.1756,    0.2301,    0.2492,         0,         0,         0],
   [    0.1112,    0.1114,    0.1309,    0.1763,    0.2266,    0.2436,         0,         0,         0],
   [    0.1096,    0.1123,    0.1330,    0.1780,    0.2255,    0.2416,         0,         0,         0],
   [    0.1071,    0.1121,    0.1343,    0.1795,    0.2256,    0.2415,         0,         0,         0],
   [    0.1058,    0.1113,    0.1338,    0.1799,    0.2268,    0.2423,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
===== Échantillon 1 =====
===== Encoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        tapis          sur            le             sol            <END>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,1046         0,1048         0,1255         0,1757         0,2338         0,2556         0,0000         0,0000         0,0000
tapis          0,1089         0,1083         0,1279         0,1756         0,2301         0,2492         0,0000         0,0000         0,0000
sur            0,1112         0,1114         0,1309         0,1763         0,2266         0,2436         0,0000         0,0000         0,0000
le             0,1096         0,1123         0,1330         0,1780         0,2255         0,2416         0,0000         0,0000         0,0000
sol            0,1071         0,1121         0,1343         0,1795         0,2256         0,2415         0,0000         0,0000         0,0000
<END>          0,1058         0,1113         0,1338         0,1799         0,2268         0,2423         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        le             tapis          sur            <END>          <PAD>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        1,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
le             0,4752         0,5248         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
tapis          0,3219         0,3640         0,3142         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
sur            0,2634         0,3076         0,2644         0,1646         0,0000         0,0000         0,0000         0,0000         0,0000
<END>          0,2364         0,2831         0,2424         0,1477         0,0903         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Cross-Attention Weights =====
===== Tête 1 =====
Requête        <START>        tapis          sur            le             sol            <END>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,1659         0,1503         0,1302         0,1248         0,1314         0,1403         0,1570         0,0000         0,0000
le             0,1633         0,1489         0,1299         0,1253         0,1327         0,1419         0,1582         0,0000         0,0000
tapis          0,1594         0,1463         0,1294         0,1263         0,1351         0,1444         0,1591         0,0000         0,0000
sur            0,1552         0,1429         0,1283         0,1275         0,1383         0,1478         0,1600         0,0000         0,0000
<END>          0,1515         0,1401         0,1275         0,1285         0,1410         0,1507         0,1606         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0], 
   [0], 
   [0],
   [0]]]]
Attention Weights:
[[[[    1.0000,         0,         0,         0,         0,         0,         0,         0,         0],
   [    0.4670,    0.5330,         0,         0,         0,         0,         0,         0,         0],
   [    0.3140,    0.3673,    0.3187,         0,         0,         0,         0,         0,         0],
   [    0.2547,    0.3095,    0.2678,    0.1679,         0,         0,         0,         0,         0],
   [    0.2274,    0.2845,    0.2455,    0.1502,    0.0923,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Attention Weights:
[[[[    0.1965,    0.1784,    0.1548,    0.1472,    0.1557,    0.1674,         0,         0,         0],
   [    0.1929,    0.1764,    0.1545,    0.1481,    0.1580,    0.1701,         0,         0,         0],
   [    0.1884,    0.1734,    0.1539,    0.1495,    0.1613,    0.1736,         0,         0,         0],
   [    0.1828,    0.1691,    0.1525,    0.1511,    0.1660,    0.1785,         0,         0,         0],
   [    0.1780,    0.1656,    0.1515,    0.1525,    0.1700,    0.1824,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Perte pour ce batch: 3.1426098
Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0], 
   [0],
   [0]]]]
Attention Weights:
[[[[    0.1044,    0.1042,    0.1279,    0.1742,    0.2342,    0.2552,         0,         0,         0],
   [    0.1086,    0.1078,    0.1306,    0.1742,    0.2304,    0.2485,         0,         0,         0],
   [    0.1105,    0.1104,    0.1332,    0.1747,    0.2276,    0.2435,         0,         0,         0],
   [    0.1097,    0.1122,    0.1360,    0.1759,    0.2257,    0.2405,         0,         0,         0],
   [    0.1065,    0.1114,    0.1367,    0.1772,    0.2266,    0.2415,         0,         0,         0],
   [    0.1053,    0.1108,    0.1364,    0.1777,    0.2276,    0.2423,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
===== Échantillon 1 =====
===== Encoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        ce             film           est            fantastique    <END>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,1044         0,1042         0,1279         0,1742         0,2342         0,2552         0,0000         0,0000         0,0000
ce             0,1086         0,1078         0,1306         0,1742         0,2304         0,2485         0,0000         0,0000         0,0000
film           0,1105         0,1104         0,1332         0,1747         0,2276         0,2435         0,0000         0,0000         0,0000
est            0,1097         0,1122         0,1360         0,1759         0,2257         0,2405         0,0000         0,0000         0,0000
fantastique    0,1065         0,1114         0,1367         0,1772         0,2266         0,2415         0,0000         0,0000         0,0000
<END>          0,1053         0,1108         0,1364         0,1777         0,2276         0,2423         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        le             film           est            fantastique    <END>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        1,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
le             0,4670         0,5330         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
film           0,3140         0,3673         0,3187         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
est            0,2547         0,3095         0,2678         0,1679         0,0000         0,0000         0,0000         0,0000         0,0000
fantastique    0,2274         0,2845         0,2455         0,1502         0,0923         0,0000         0,0000         0,0000         0,0000
<END>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Cross-Attention Weights =====
===== Tête 1 =====
Requête        <START>        ce             film           est            fantastique    <END>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,1965         0,1784         0,1548         0,1472         0,1557         0,1674         0,0000         0,0000         0,0000
le             0,1929         0,1764         0,1545         0,1481         0,1580         0,1701         0,0000         0,0000         0,0000
film           0,1884         0,1734         0,1539         0,1495         0,1613         0,1736         0,0000         0,0000         0,0000
est            0,1828         0,1691         0,1525         0,1511         0,1660         0,1785         0,0000         0,0000         0,0000
fantastique    0,1780         0,1656         0,1515         0,1525         0,1700         0,1824         0,0000         0,0000         0,0000
<END>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0], 
   [0],
   [0]]]]
Attention Weights:
[[[[    1.0000,         0,         0,         0,         0,         0,         0,         0,         0],
   [    0.4636,    0.5364,         0,         0,         0,         0,         0,         0,         0],
   [    0.3071,    0.3652,    0.3276,         0,         0,         0,         0,         0,         0],
   [    0.2469,    0.3050,    0.2725,    0.1755,         0,         0,         0,         0,         0],
   [    0.2193,    0.2795,    0.2488,    0.1565,    0.0960,         0,         0,         0,         0],
   [    0.1999,    0.2584,    0.2302,    0.1439,    0.0875,    0.0801,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Attention Weights:
[[[[    0.1958,    0.1788,    0.1549,    0.1471,    0.1544,    0.1690,         0,         0,         0],
   [    0.1918,    0.1766,    0.1545,    0.1482,    0.1569,    0.1721,         0,         0,         0],
   [    0.1870,    0.1735,    0.1537,    0.1496,    0.1603,    0.1759,         0,         0,         0],
   [    0.1811,    0.1690,    0.1521,    0.1514,    0.1652,    0.1812,         0,         0,         0],
   [    0.1759,    0.1650,    0.1507,    0.1530,    0.1696,    0.1858,         0,         0,         0],
   [    0.1741,    0.1639,    0.1505,    0.1537,    0.1709,    0.1869,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Perte pour ce batch: 2.6677637
Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0], 
   [0],
   [0]]]]
Attention Weights:
[[[[    0.1041,    0.1036,    0.1276,    0.1750,    0.2344,    0.2554,         0,         0,         0],
   [    0.1087,    0.1075,    0.1303,    0.1748,    0.2305,    0.2482,         0,         0,         0],
   [    0.1106,    0.1102,    0.1327,    0.1752,    0.2279,    0.2434,         0,         0,         0],
   [    0.1084,    0.1113,    0.1351,    0.1769,    0.2270,    0.2414,         0,         0,         0],
   [    0.1059,    0.1112,    0.1360,    0.1781,    0.2272,    0.2416,         0,         0,         0],
   [    0.1046,    0.1104,    0.1357,    0.1787,    0.2282,    0.2424,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
===== Échantillon 1 =====
===== Encoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        je             déteste        ce             temps          <END>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,1041         0,1036         0,1276         0,1750         0,2344         0,2554         0,0000         0,0000         0,0000
je             0,1087         0,1075         0,1303         0,1748         0,2305         0,2482         0,0000         0,0000         0,0000
déteste        0,1106         0,1102         0,1327         0,1752         0,2279         0,2434         0,0000         0,0000         0,0000
ce             0,1084         0,1113         0,1351         0,1769         0,2270         0,2414         0,0000         0,0000         0,0000
temps          0,1059         0,1112         0,1360         0,1781         0,2272         0,2416         0,0000         0,0000         0,0000
<END>          0,1046         0,1104         0,1357         0,1787         0,2282         0,2424         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        je             déteste        <END>          <PAD>          <PAD>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        1,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
je             0,4636         0,5364         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
déteste        0,3071         0,3652         0,3276         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<END>          0,2469         0,3050         0,2725         0,1755         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,2193         0,2795         0,2488         0,1565         0,0960         0,0000         0,0000         0,0000         0,0000
<PAD>          0,1999         0,2584         0,2302         0,1439         0,0875         0,0801         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Cross-Attention Weights =====
===== Tête 1 =====
Requête        <START>        je             déteste        ce             temps          <END>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,1958         0,1788         0,1549         0,1471         0,1544         0,1690         0,0000         0,0000         0,0000
je             0,1918         0,1766         0,1545         0,1482         0,1569         0,1721         0,0000         0,0000         0,0000
déteste        0,1870         0,1735         0,1537         0,1496         0,1603         0,1759         0,0000         0,0000         0,0000
<END>          0,1811         0,1690         0,1521         0,1514         0,1652         0,1812         0,0000         0,0000         0,0000
<PAD>          0,1759         0,1650         0,1507         0,1530         0,1696         0,1858         0,0000         0,0000         0,0000
<PAD>          0,1741         0,1639         0,1505         0,1537         0,1709         0,1869         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0], 
   [0], 
   [0], 
   [0],
   [0]]]]
Attention Weights:
[[[[    1.0000,         0,         0,         0,         0,         0,         0,         0,         0],
   [    0.4593,    0.5407,         0,         0,         0,         0,         0,         0,         0],
   [    0.3012,    0.3657,    0.3330,         0,         0,         0,         0,         0,         0],
   [    0.2410,    0.3055,    0.2767,    0.1768,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Attention Weights:
[[[[    0.1959,    0.1789,    0.1535,    0.1467,    0.1550,    0.1699,         0,         0,         0],
   [    0.1919,    0.1767,    0.1530,    0.1480,    0.1574,    0.1731,         0,         0,         0],
   [    0.1866,    0.1730,    0.1519,    0.1497,    0.1613,    0.1775,         0,         0,         0],
   [    0.1796,    0.1675,    0.1500,    0.1518,    0.1671,    0.1839,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Perte pour ce batch: 2.806715
Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0], 
   [0], 
   [0],
   [0]]]]
Attention Weights:
[[[[    0.1391,    0.1400,    0.1697,    0.2368,    0.3145,         0,         0,         0,         0],
   [    0.1435,    0.1436,    0.1713,    0.2345,    0.3070,         0,         0,         0,         0],
   [    0.1449,    0.1467,    0.1742,    0.2339,    0.3004,         0,         0,         0,         0],
   [    0.1422,    0.1474,    0.1769,    0.2352,    0.2982,         0,         0,         0,         0],
   [    0.1385,    0.1467,    0.1785,    0.2373,    0.2990,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
===== Échantillon 1 =====
===== Encoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        quelle         belle          journée        <END>          <PAD>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,1391         0,1400         0,1697         0,2368         0,3145         0,0000         0,0000         0,0000         0,0000
quelle         0,1435         0,1436         0,1713         0,2345         0,3070         0,0000         0,0000         0,0000         0,0000
belle          0,1449         0,1467         0,1742         0,2339         0,3004         0,0000         0,0000         0,0000         0,0000
journée        0,1422         0,1474         0,1769         0,2352         0,2982         0,0000         0,0000         0,0000         0,0000
<END>          0,1385         0,1467         0,1785         0,2373         0,2990         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        quelle         belle          <END>          <PAD>          <PAD>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        1,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
quelle         0,4593         0,5407         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
belle          0,3012         0,3657         0,3330         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<END>          0,2410         0,3055         0,2767         0,1768         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Cross-Attention Weights =====
===== Tête 1 =====
Requête        <START>        quelle         belle          journée        <END>          <PAD>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,1959         0,1789         0,1535         0,1467         0,1550         0,1699         0,0000         0,0000         0,0000
quelle         0,1919         0,1767         0,1530         0,1480         0,1574         0,1731         0,0000         0,0000         0,0000
belle          0,1866         0,1730         0,1519         0,1497         0,1613         0,1775         0,0000         0,0000         0,0000
<END>          0,1796         0,1675         0,1500         0,1518         0,1671         0,1839         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0], 
   [0], 
   [0], 
   [0],
   [0]]]]
Attention Weights:
[[[[    1.0000,         0,         0,         0,         0,         0,         0,         0,         0],
   [    0.4478,    0.5522,         0,         0,         0,         0,         0,         0,         0],
   [    0.2906,    0.3708,    0.3385,         0,         0,         0,         0,         0,         0],
   [    0.2312,    0.3089,    0.2810,    0.1788,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Attention Weights:
[[[[    0.2344,    0.2128,    0.1859,    0.1780,    0.1888,         0,         0,         0,         0],
   [    0.2302,    0.2102,    0.1858,    0.1804,    0.1934,         0,         0,         0,         0],
   [    0.2240,    0.2061,    0.1854,    0.1840,    0.2005,         0,         0,         0,         0],
   [    0.2170,    0.2009,    0.1846,    0.1881,    0.2094,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Perte pour ce batch: 2.8243055
Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0], 
   [0], 
   [0],
   [0]]]]
Attention Weights:
[[[[    0.1394,    0.1398,    0.1699,    0.2356,    0.3153,         0,         0,         0,         0],
   [    0.1438,    0.1433,    0.1715,    0.2337,    0.3077,         0,         0,         0,         0],
   [    0.1447,    0.1457,    0.1739,    0.2333,    0.3024,         0,         0,         0,         0],
   [    0.1422,    0.1468,    0.1765,    0.2345,    0.2999,         0,         0,         0,         0],
   [    0.1385,    0.1464,    0.1782,    0.2364,    0.3005,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
===== Échantillon 1 =====
===== Encoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        le             chat           sol            <END>          <PAD>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,1394         0,1398         0,1699         0,2356         0,3153         0,0000         0,0000         0,0000         0,0000
le             0,1438         0,1433         0,1715         0,2337         0,3077         0,0000         0,0000         0,0000         0,0000
chat           0,1447         0,1457         0,1739         0,2333         0,3024         0,0000         0,0000         0,0000         0,0000
sol            0,1422         0,1468         0,1765         0,2345         0,2999         0,0000         0,0000         0,0000         0,0000
<END>          0,1385         0,1464         0,1782         0,2364         0,3005         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        le             chat           <END>          <PAD>          <PAD>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        1,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
le             0,4478         0,5522         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
chat           0,2906         0,3708         0,3385         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<END>          0,2312         0,3089         0,2810         0,1788         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Cross-Attention Weights =====
===== Tête 1 =====
Requête        <START>        le             chat           sol            <END>          <PAD>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,2344         0,2128         0,1859         0,1780         0,1888         0,0000         0,0000         0,0000         0,0000
le             0,2302         0,2102         0,1858         0,1804         0,1934         0,0000         0,0000         0,0000         0,0000
chat           0,2240         0,2061         0,1854         0,1840         0,2005         0,0000         0,0000         0,0000         0,0000
<END>          0,2170         0,2009         0,1846         0,1881         0,2094         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0], 
   [0], 
   [0], 
   [0],
   [0]]]]
Attention Weights:
[[[[    1.0000,         0,         0,         0,         0,         0,         0,         0,         0],
   [    0.4470,    0.5530,         0,         0,         0,         0,         0,         0,         0],
   [    0.2868,    0.3672,    0.3460,         0,         0,         0,         0,         0,         0],
   [    0.2272,    0.3050,    0.2865,    0.1812,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Attention Weights:
[[[[    0.2340,    0.2139,    0.1853,    0.1787,    0.1880,         0,         0,         0,         0],
   [    0.2294,    0.2111,    0.1850,    0.1814,    0.1931,         0,         0,         0,         0],
   [    0.2231,    0.2068,    0.1845,    0.1851,    0.2005,         0,         0,         0,         0],
   [    0.2156,    0.2011,    0.1835,    0.1896,    0.2101,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Perte pour ce batch: 2.3340094
Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0]]]]
Attention Weights:
[[[[    0.0749,    0.0748,    0.0907,    0.1254,    0.1691,    0.1821,    0.1595,    0.1235,         0],
   [    0.0783,    0.0779,    0.0931,    0.1261,    0.1676,    0.1788,    0.1566,    0.1217,         0],
   [    0.0795,    0.0800,    0.0953,    0.1270,    0.1661,    0.1760,    0.1548,    0.1212,         0],
   [    0.0785,    0.0810,    0.0972,    0.1278,    0.1645,    0.1739,    0.1544,    0.1226,         0],
   [    0.0757,    0.0799,    0.0973,    0.1283,    0.1645,    0.1743,    0.1557,    0.1244,         0],
   [    0.0750,    0.0796,    0.0973,    0.1289,    0.1654,    0.1749,    0.1554,    0.1234,         0],
   [    0.0761,    0.0793,    0.0965,    0.1295,    0.1680,    0.1769,    0.1542,    0.1195,         0],
   [    0.0776,    0.0788,    0.0952,    0.1298,    0.1709,    0.1792,    0.1530,    0.1154,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
===== Échantillon 1 =====
===== Encoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        <UNK>          <UNK>          est            un             mauvais        film           <END>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,0749         0,0748         0,0907         0,1254         0,1691         0,1821         0,1595         0,1235         0,0000
<UNK>          0,0783         0,0779         0,0931         0,1261         0,1676         0,1788         0,1566         0,1217         0,0000
<UNK>          0,0795         0,0800         0,0953         0,1270         0,1661         0,1760         0,1548         0,1212         0,0000
est            0,0785         0,0810         0,0972         0,1278         0,1645         0,1739         0,1544         0,1226         0,0000
un             0,0757         0,0799         0,0973         0,1283         0,1645         0,1743         0,1557         0,1244         0,0000
mauvais        0,0750         0,0796         0,0973         0,1289         0,1654         0,1749         0,1554         0,1234         0,0000
film           0,0761         0,0793         0,0965         0,1295         0,1680         0,1769         0,1542         0,1195         0,0000
<END>          0,0776         0,0788         0,0952         0,1298         0,1709         0,1792         0,1530         0,1154         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        <UNK>          <UNK>          est            un             mauvais        <END>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        1,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<UNK>          0,4470         0,5530         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<UNK>          0,2868         0,3672         0,3460         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
est            0,2272         0,3050         0,2865         0,1812         0,0000         0,0000         0,0000         0,0000         0,0000
un             0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
mauvais        0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<END>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Cross-Attention Weights =====
===== Tête 1 =====
Requête        <START>        <UNK>          <UNK>          est            un             mauvais        film           <END>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,2340         0,2139         0,1853         0,1787         0,1880         0,0000         0,0000         0,0000         0,0000
<UNK>          0,2294         0,2111         0,1850         0,1814         0,1931         0,0000         0,0000         0,0000         0,0000
<UNK>          0,2231         0,2068         0,1845         0,1851         0,2005         0,0000         0,0000         0,0000         0,0000
est            0,2156         0,2011         0,1835         0,1896         0,2101         0,0000         0,0000         0,0000         0,0000
un             0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
mauvais        0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<END>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0],
   [0]]]]
Attention Weights:
[[[[    1.0000,         0,         0,         0,         0,         0,         0,         0,         0],
   [    0.4508,    0.5492,         0,         0,         0,         0,         0,         0,         0],
   [    0.2889,    0.3656,    0.3455,         0,         0,         0,         0,         0,         0],
   [    0.2266,    0.3004,    0.2823,    0.1907,         0,         0,         0,         0,         0],
   [    0.1995,    0.2741,    0.2563,    0.1674,    0.1027,         0,         0,         0,         0],
   [    0.1804,    0.2515,    0.2354,    0.1525,    0.0928,    0.0874,         0,         0,         0],
   [    0.1597,    0.2196,    0.2066,    0.1362,    0.0843,    0.0789,    0.1145,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Attention Weights:
[[[[    0.1386,    0.1245,    0.1072,    0.1030,    0.1098,    0.1195,    0.1338,    0.1635,         0],
   [    0.1349,    0.1222,    0.1065,    0.1037,    0.1120,    0.1223,    0.1353,    0.1631,         0],
   [    0.1303,    0.1190,    0.1055,    0.1051,    0.1157,    0.1265,    0.1371,    0.1608,         0],
   [    0.1249,    0.1147,    0.1041,    0.1070,    0.1209,    0.1323,    0.1393,    0.1566,         0],
   [    0.1202,    0.1111,    0.1030,    0.1088,    0.1257,    0.1374,    0.1411,    0.1529,         0],
   [    0.1186,    0.1100,    0.1028,    0.1094,    0.1270,    0.1387,    0.1416,    0.1519,         0],
   [    0.1220,    0.1129,    0.1039,    0.1081,    0.1231,    0.1343,    0.1402,    0.1556,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Perte pour ce batch: 3.3769307
Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0], 
   [0],
   [0]]]]
Attention Weights:
[[[[    0.1045,    0.1044,    0.1265,    0.1748,    0.2329,    0.2569,         0,         0,         0],
   [    0.1084,    0.1078,    0.1290,    0.1750,    0.2293,    0.2505,         0,         0,         0],
   [    0.1098,    0.1101,    0.1313,    0.1757,    0.2268,    0.2464,         0,         0,         0],
   [    0.1086,    0.1116,    0.1339,    0.1771,    0.2251,    0.2437,         0,         0,         0],
   [    0.1054,    0.1109,    0.1348,    0.1784,    0.2259,    0.2446,         0,         0,         0],
   [    0.1036,    0.1097,    0.1341,    0.1790,    0.2275,    0.2460,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
===== Échantillon 1 =====
===== Encoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        ce             livre          est            intéressant    <END>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,1045         0,1044         0,1265         0,1748         0,2329         0,2569         0,0000         0,0000         0,0000
ce             0,1084         0,1078         0,1290         0,1750         0,2293         0,2505         0,0000         0,0000         0,0000
livre          0,1098         0,1101         0,1313         0,1757         0,2268         0,2464         0,0000         0,0000         0,0000
est            0,1086         0,1116         0,1339         0,1771         0,2251         0,2437         0,0000         0,0000         0,0000
intéressant    0,1054         0,1109         0,1348         0,1784         0,2259         0,2446         0,0000         0,0000         0,0000
<END>          0,1036         0,1097         0,1341         0,1790         0,2275         0,2460         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        le             livre          est            intéressant    <END>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        1,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
le             0,4508         0,5492         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
livre          0,2889         0,3656         0,3455         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
est            0,2266         0,3004         0,2823         0,1907         0,0000         0,0000         0,0000         0,0000         0,0000
intéressant    0,1995         0,2741         0,2563         0,1674         0,1027         0,0000         0,0000         0,0000         0,0000
<END>          0,1804         0,2515         0,2354         0,1525         0,0928         0,0874         0,0000         0,0000         0,0000
<PAD>          0,1597         0,2196         0,2066         0,1362         0,0843         0,0789         0,1145         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Cross-Attention Weights =====
===== Tête 1 =====
Requête        <START>        ce             livre          est            intéressant    <END>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,1386         0,1245         0,1072         0,1030         0,1098         0,1195         0,1338         0,1635         0,0000
le             0,1349         0,1222         0,1065         0,1037         0,1120         0,1223         0,1353         0,1631         0,0000
livre          0,1303         0,1190         0,1055         0,1051         0,1157         0,1265         0,1371         0,1608         0,0000
est            0,1249         0,1147         0,1041         0,1070         0,1209         0,1323         0,1393         0,1566         0,0000
intéressant    0,1202         0,1111         0,1030         0,1088         0,1257         0,1374         0,1411         0,1529         0,0000
<END>          0,1186         0,1100         0,1028         0,1094         0,1270         0,1387         0,1416         0,1519         0,0000
<PAD>          0,1220         0,1129         0,1039         0,1081         0,1231         0,1343         0,1402         0,1556         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0], 
   [0],
   [0]]]]
Attention Weights:
[[[[    1.0000,         0,         0,         0,         0,         0,         0,         0,         0],
   [    0.4478,    0.5522,         0,         0,         0,         0,         0,         0,         0],
   [    0.2839,    0.3640,    0.3522,         0,         0,         0,         0,         0,         0],
   [    0.2198,    0.2972,    0.2865,    0.1966,         0,         0,         0,         0,         0],
   [    0.1917,    0.2696,    0.2585,    0.1722,    0.1079,         0,         0,         0,         0],
   [    0.1732,    0.2469,    0.2368,    0.1567,    0.0974,    0.0889,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Attention Weights:
[[[[    0.1933,    0.1771,    0.1535,    0.1476,    0.1565,    0.1719,         0,         0,         0],
   [    0.1877,    0.1736,    0.1522,    0.1492,    0.1603,    0.1769,         0,         0,         0],
   [    0.1812,    0.1688,    0.1504,    0.1510,    0.1655,    0.1830,         0,         0,         0],
   [    0.1725,    0.1619,    0.1477,    0.1535,    0.1730,    0.1914,         0,         0,         0],
   [    0.1650,    0.1559,    0.1453,    0.1555,    0.1797,    0.1986,         0,         0,         0],
   [    0.1626,    0.1542,    0.1449,    0.1563,    0.1816,    0.2004,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Perte pour ce batch: 2.8504264
Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [1.0000]]]]
Attention Weights:
[[[[    0.0683,    0.0679,    0.0825,    0.1146,    0.1528,    0.1661,    0.1433,    0.1103,    0.0943],
   [    0.0717,    0.0709,    0.0850,    0.1157,    0.1516,    0.1630,    0.1403,    0.1087,    0.0931],
   [    0.0724,    0.0724,    0.0867,    0.1167,    0.1509,    0.1606,    0.1388,    0.1085,    0.0930],
   [    0.0707,    0.0727,    0.0878,    0.1174,    0.1499,    0.1586,    0.1387,    0.1098,    0.0943],
   [    0.0684,    0.0722,    0.0881,    0.1178,    0.1497,    0.1579,    0.1394,    0.1112,    0.0954],
   [    0.0679,    0.0721,    0.0882,    0.1183,    0.1506,    0.1587,    0.1395,    0.1105,    0.0942],
   [    0.0690,    0.0722,    0.0879,    0.1191,    0.1532,    0.1614,    0.1392,    0.1076,    0.0907],
   [    0.0709,    0.0723,    0.0872,    0.1193,    0.1556,    0.1644,    0.1387,    0.1045,    0.0871],
   [    0.0708,    0.0715,    0.0861,    0.1193,    0.1575,    0.1672,    0.1394,    0.1031,    0.0852]]]]
===== Échantillon 1 =====
===== Encoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        je             <UNK>          <UNK>          <UNK>          pas            ce             repas          <END>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,0683         0,0679         0,0825         0,1146         0,1528         0,1661         0,1433         0,1103         0,0943
je             0,0717         0,0709         0,0850         0,1157         0,1516         0,1630         0,1403         0,1087         0,0931
<UNK>          0,0724         0,0724         0,0867         0,1167         0,1509         0,1606         0,1388         0,1085         0,0930
<UNK>          0,0707         0,0727         0,0878         0,1174         0,1499         0,1586         0,1387         0,1098         0,0943
<UNK>          0,0684         0,0722         0,0881         0,1178         0,1497         0,1579         0,1394         0,1112         0,0954
pas            0,0679         0,0721         0,0882         0,1183         0,1506         0,1587         0,1395         0,1105         0,0942
ce             0,0690         0,0722         0,0879         0,1191         0,1532         0,1614         0,1392         0,1076         0,0907
repas          0,0709         0,0723         0,0872         0,1193         0,1556         0,1644         0,1387         0,1045         0,0871
<END>          0,0708         0,0715         0,0861         0,1193         0,1575         0,1672         0,1394         0,1031         0,0852

===== Decoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        je             <UNK>          <UNK>          <UNK>          pas            <END>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        1,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
je             0,4478         0,5522         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<UNK>          0,2839         0,3640         0,3522         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<UNK>          0,2198         0,2972         0,2865         0,1966         0,0000         0,0000         0,0000         0,0000         0,0000
<UNK>          0,1917         0,2696         0,2585         0,1722         0,1079         0,0000         0,0000         0,0000         0,0000
pas            0,1732         0,2469         0,2368         0,1567         0,0974         0,0889         0,0000         0,0000         0,0000
<END>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Cross-Attention Weights =====
===== Tête 1 =====
Requête        <START>        je             <UNK>          <UNK>          <UNK>          pas            ce             repas          <END>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,1933         0,1771         0,1535         0,1476         0,1565         0,1719         0,0000         0,0000         0,0000
je             0,1877         0,1736         0,1522         0,1492         0,1603         0,1769         0,0000         0,0000         0,0000
<UNK>          0,1812         0,1688         0,1504         0,1510         0,1655         0,1830         0,0000         0,0000         0,0000
<UNK>          0,1725         0,1619         0,1477         0,1535         0,1730         0,1914         0,0000         0,0000         0,0000
<UNK>          0,1650         0,1559         0,1453         0,1555         0,1797         0,1986         0,0000         0,0000         0,0000
pas            0,1626         0,1542         0,1449         0,1563         0,1816         0,2004         0,0000         0,0000         0,0000
<END>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0],
   [0]]]]
Attention Weights:
[[[[    1.0000,         0,         0,         0,         0,         0,         0,         0,         0],
   [    0.4450,    0.5550,         0,         0,         0,         0,         0,         0,         0],
   [    0.2783,    0.3622,    0.3594,         0,         0,         0,         0,         0,         0],
   [    0.2151,    0.2951,    0.2910,    0.1988,         0,         0,         0,         0,         0],
   [    0.1876,    0.2674,    0.2619,    0.1721,    0.1110,         0,         0,         0,         0],
   [    0.1689,    0.2440,    0.2390,    0.1557,    0.0997,    0.0927,         0,         0,         0],
   [    0.1487,    0.2118,    0.2085,    0.1383,    0.0898,    0.0832,    0.1197,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Attention Weights:
[[[[    0.1137,    0.1036,    0.0889,    0.0839,    0.0908,    0.0996,    0.1125,    0.1348,    0.1723],
   [    0.1105,    0.1016,    0.0882,    0.0848,    0.0930,    0.1023,    0.1143,    0.1346,    0.1707],
   [    0.1068,    0.0990,    0.0877,    0.0867,    0.0971,    0.1070,    0.1169,    0.1331,    0.1657],
   [    0.1025,    0.0957,    0.0871,    0.0893,    0.1032,    0.1139,    0.1204,    0.1304,    0.1575],
   [    0.0989,    0.0930,    0.0866,    0.0915,    0.1084,    0.1197,    0.1232,    0.1281,    0.1507],
   [    0.0976,    0.0921,    0.0866,    0.0923,    0.1100,    0.1215,    0.1240,    0.1273,    0.1486],
   [    0.0997,    0.0939,    0.0869,    0.0906,    0.1059,    0.1167,    0.1220,    0.1296,    0.1545],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Perte pour ce batch: 3.4237359
Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0], 
   [0],
   [0]]]]
Attention Weights:
[[[[    0.1045,    0.1044,    0.1277,    0.1745,    0.2327,    0.2562,         0,         0,         0],
   [    0.1083,    0.1077,    0.1303,    0.1747,    0.2291,    0.2499,         0,         0,         0],
   [    0.1096,    0.1099,    0.1328,    0.1754,    0.2268,    0.2455,         0,         0,         0],
   [    0.1079,    0.1110,    0.1353,    0.1767,    0.2258,    0.2432,         0,         0,         0],
   [    0.1044,    0.1100,    0.1358,    0.1780,    0.2272,    0.2447,         0,         0,         0],
   [    0.1027,    0.1090,    0.1352,    0.1786,    0.2286,    0.2459,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
===== Échantillon 1 =====
===== Encoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        ce             film           est            excellent      <END>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,1045         0,1044         0,1277         0,1745         0,2327         0,2562         0,0000         0,0000         0,0000
ce             0,1083         0,1077         0,1303         0,1747         0,2291         0,2499         0,0000         0,0000         0,0000
film           0,1096         0,1099         0,1328         0,1754         0,2268         0,2455         0,0000         0,0000         0,0000
est            0,1079         0,1110         0,1353         0,1767         0,2258         0,2432         0,0000         0,0000         0,0000
excellent      0,1044         0,1100         0,1358         0,1780         0,2272         0,2447         0,0000         0,0000         0,0000
<END>          0,1027         0,1090         0,1352         0,1786         0,2286         0,2459         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        le             film           est            excellent      <END>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        1,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
le             0,4450         0,5550         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
film           0,2783         0,3622         0,3594         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
est            0,2151         0,2951         0,2910         0,1988         0,0000         0,0000         0,0000         0,0000         0,0000
excellent      0,1876         0,2674         0,2619         0,1721         0,1110         0,0000         0,0000         0,0000         0,0000
<END>          0,1689         0,2440         0,2390         0,1557         0,0997         0,0927         0,0000         0,0000         0,0000
<PAD>          0,1487         0,2118         0,2085         0,1383         0,0898         0,0832         0,1197         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Cross-Attention Weights =====
===== Tête 1 =====
Requête        <START>        ce             film           est            excellent      <END>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,1137         0,1036         0,0889         0,0839         0,0908         0,0996         0,1125         0,1348         0,1723
le             0,1105         0,1016         0,0882         0,0848         0,0930         0,1023         0,1143         0,1346         0,1707
film           0,1068         0,0990         0,0877         0,0867         0,0971         0,1070         0,1169         0,1331         0,1657
est            0,1025         0,0957         0,0871         0,0893         0,1032         0,1139         0,1204         0,1304         0,1575
excellent      0,0989         0,0930         0,0866         0,0915         0,1084         0,1197         0,1232         0,1281         0,1507
<END>          0,0976         0,0921         0,0866         0,0923         0,1100         0,1215         0,1240         0,1273         0,1486
<PAD>          0,0997         0,0939         0,0869         0,0906         0,1059         0,1167         0,1220         0,1296         0,1545
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0], 
   [0],
   [0]]]]
Attention Weights:
[[[[    1.0000,         0,         0,         0,         0,         0,         0,         0,         0],
   [    0.4411,    0.5589,         0,         0,         0,         0,         0,         0,         0],
   [    0.2721,    0.3611,    0.3669,         0,         0,         0,         0,         0,         0],
   [    0.2066,    0.2903,    0.2947,    0.2084,         0,         0,         0,         0,         0],
   [    0.1776,    0.2609,    0.2645,    0.1808,    0.1161,         0,         0,         0,         0],
   [    0.1598,    0.2376,    0.2406,    0.1631,    0.1038,    0.0950,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Attention Weights:
[[[[    0.1910,    0.1764,    0.1541,    0.1477,    0.1580,    0.1728,         0,         0,         0],
   [    0.1846,    0.1723,    0.1526,    0.1494,    0.1625,    0.1786,         0,         0,         0],
   [    0.1771,    0.1666,    0.1505,    0.1515,    0.1685,    0.1858,         0,         0,         0],
   [    0.1673,    0.1587,    0.1472,    0.1541,    0.1772,    0.1956,         0,         0,         0],
   [    0.1589,    0.1517,    0.1441,    0.1562,    0.1849,    0.2041,         0,         0,         0],
   [    0.1563,    0.1500,    0.1437,    0.1571,    0.1869,    0.2061,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Perte pour ce batch: 2.5763314
Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0]]]]
Attention Weights:
[[[[    0.0756,    0.0751,    0.0910,    0.1272,    0.1684,    0.1829,    0.1571,    0.1228,         0],
   [    0.0791,    0.0782,    0.0934,    0.1280,    0.1670,    0.1795,    0.1540,    0.1206,         0],
   [    0.0796,    0.0797,    0.0952,    0.1288,    0.1663,    0.1776,    0.1528,    0.1201,         0],
   [    0.0780,    0.0802,    0.0967,    0.1294,    0.1654,    0.1761,    0.1528,    0.1214,         0],
   [    0.0751,    0.0794,    0.0973,    0.1300,    0.1654,    0.1761,    0.1538,    0.1230,         0],
   [    0.0742,    0.0789,    0.0970,    0.1305,    0.1665,    0.1770,    0.1537,    0.1222,         0],
   [    0.0756,    0.0791,    0.0962,    0.1310,    0.1684,    0.1786,    0.1524,    0.1186,         0],
   [    0.0775,    0.0791,    0.0950,    0.1314,    0.1708,    0.1808,    0.1509,    0.1144,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
===== Échantillon 1 =====
===== Encoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        je             suis           triste         <UNK>          <UNK>          <UNK>          <END>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,0756         0,0751         0,0910         0,1272         0,1684         0,1829         0,1571         0,1228         0,0000
je             0,0791         0,0782         0,0934         0,1280         0,1670         0,1795         0,1540         0,1206         0,0000
suis           0,0796         0,0797         0,0952         0,1288         0,1663         0,1776         0,1528         0,1201         0,0000
triste         0,0780         0,0802         0,0967         0,1294         0,1654         0,1761         0,1528         0,1214         0,0000
<UNK>          0,0751         0,0794         0,0973         0,1300         0,1654         0,1761         0,1538         0,1230         0,0000
<UNK>          0,0742         0,0789         0,0970         0,1305         0,1665         0,1770         0,1537         0,1222         0,0000
<UNK>          0,0756         0,0791         0,0962         0,1310         0,1684         0,1786         0,1524         0,1186         0,0000
<END>          0,0775         0,0791         0,0950         0,1314         0,1708         0,1808         0,1509         0,1144         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        je             suis           triste         <END>          <PAD>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        1,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
je             0,4411         0,5589         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
suis           0,2721         0,3611         0,3669         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
triste         0,2066         0,2903         0,2947         0,2084         0,0000         0,0000         0,0000         0,0000         0,0000
<END>          0,1776         0,2609         0,2645         0,1808         0,1161         0,0000         0,0000         0,0000         0,0000
<PAD>          0,1598         0,2376         0,2406         0,1631         0,1038         0,0950         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Cross-Attention Weights =====
===== Tête 1 =====
Requête        <START>        je             suis           triste         <UNK>          <UNK>          <UNK>          <END>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,1910         0,1764         0,1541         0,1477         0,1580         0,1728         0,0000         0,0000         0,0000
je             0,1846         0,1723         0,1526         0,1494         0,1625         0,1786         0,0000         0,0000         0,0000
suis           0,1771         0,1666         0,1505         0,1515         0,1685         0,1858         0,0000         0,0000         0,0000
triste         0,1673         0,1587         0,1472         0,1541         0,1772         0,1956         0,0000         0,0000         0,0000
<END>          0,1589         0,1517         0,1441         0,1562         0,1849         0,2041         0,0000         0,0000         0,0000
<PAD>          0,1563         0,1500         0,1437         0,1571         0,1869         0,2061         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0], 
   [0], 
   [0],
   [0]]]]
Attention Weights:
[[[[    1.0000,         0,         0,         0,         0,         0,         0,         0,         0],
   [    0.4384,    0.5616,         0,         0,         0,         0,         0,         0,         0],
   [    0.2667,    0.3589,    0.3744,         0,         0,         0,         0,         0,         0],
   [    0.2005,    0.2869,    0.2993,    0.2133,         0,         0,         0,         0,         0],
   [    0.1725,    0.2572,    0.2672,    0.1825,    0.1205,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Attention Weights:
[[[[    0.1364,    0.1248,    0.1080,    0.1009,    0.1080,    0.1188,    0.1345,    0.1687,         0],
   [    0.1316,    0.1218,    0.1067,    0.1018,    0.1109,    0.1225,    0.1366,    0.1680,         0],
   [    0.1259,    0.1175,    0.1052,    0.1036,    0.1157,    0.1280,    0.1392,    0.1651,         0],
   [    0.1187,    0.1116,    0.1029,    0.1060,    0.1227,    0.1360,    0.1424,    0.1596,         0],
   [    0.1129,    0.1070,    0.1012,    0.1081,    0.1285,    0.1425,    0.1448,    0.1550,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Perte pour ce batch: 3.165151
Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0], 
   [0],
   [0]]]]
Attention Weights:
[[[[    0.1048,    0.1047,    0.1271,    0.1743,    0.2333,    0.2557,         0,         0,         0],
   [    0.1085,    0.1079,    0.1299,    0.1746,    0.2298,    0.2494,         0,         0,         0],
   [    0.1096,    0.1099,    0.1321,    0.1751,    0.2279,    0.2454,         0,         0,         0],
   [    0.1076,    0.1108,    0.1347,    0.1766,    0.2273,    0.2430,         0,         0,         0],
   [    0.1038,    0.1096,    0.1350,    0.1778,    0.2291,    0.2447,         0,         0,         0],
   [    0.1021,    0.1085,    0.1344,    0.1785,    0.2307,    0.2458,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
===== Échantillon 1 =====
===== Encoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        ce             temps          est            agréable       <END>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,1048         0,1047         0,1271         0,1743         0,2333         0,2557         0,0000         0,0000         0,0000
ce             0,1085         0,1079         0,1299         0,1746         0,2298         0,2494         0,0000         0,0000         0,0000
temps          0,1096         0,1099         0,1321         0,1751         0,2279         0,2454         0,0000         0,0000         0,0000
est            0,1076         0,1108         0,1347         0,1766         0,2273         0,2430         0,0000         0,0000         0,0000
agréable       0,1038         0,1096         0,1350         0,1778         0,2291         0,2447         0,0000         0,0000         0,0000
<END>          0,1021         0,1085         0,1344         0,1785         0,2307         0,2458         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Self-Attention Weights =====
===== Tête 1 =====
Requête        <START>        le             temps          est            agréable       <END>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        1,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
le             0,4384         0,5616         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
temps          0,2667         0,3589         0,3744         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
est            0,2005         0,2869         0,2993         0,2133         0,0000         0,0000         0,0000         0,0000         0,0000
agréable       0,1725         0,2572         0,2672         0,1825         0,1205         0,0000         0,0000         0,0000         0,0000
<END>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

===== Decoder Layer 1 Cross-Attention Weights =====
===== Tête 1 =====
Requête        <START>        ce             temps          est            agréable       <END>          <PAD>          <PAD>          <PAD>
------------------------------------------------------------------------------------------------------------------------------------------------------
<START>        0,1364         0,1248         0,1080         0,1009         0,1080         0,1188         0,1345         0,1687         0,0000
le             0,1316         0,1218         0,1067         0,1018         0,1109         0,1225         0,1366         0,1680         0,0000
temps          0,1259         0,1175         0,1052         0,1036         0,1157         0,1280         0,1392         0,1651         0,0000
est            0,1187         0,1116         0,1029         0,1060         0,1227         0,1360         0,1424         0,1596         0,0000
agréable       0,1129         0,1070         0,1012         0,1081         0,1285         0,1425         0,1448         0,1550         0,0000
<END>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         
<PAD>          0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000         0,0000

Generated Query Padding Mask:
[[[[1.0000],
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000], 
   [1.0000],
   [0], 
   [0],
   [0]]]]
Attention Weights:
[[[[    1.0000,         0,         0,         0,         0,         0,         0,         0,         0],
   [    0.4351,    0.5649,         0,         0,         0,         0,         0,         0,         0],
   [    0.2620,    0.3584,    0.3796,         0,         0,         0,         0,         0,         0],
   [    0.1949,    0.2842,    0.3015,    0.2194,         0,         0,         0,         0,         0],
   [    0.1656,    0.2535,    0.2687,    0.1879,    0.1243,         0,         0,         0,         0],
   [    0.1480,    0.2293,    0.2430,    0.1687,    0.1109,    0.1000,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Attention Weights:
[[[[    0.1933,    0.1791,    0.1553,    0.1466,    0.1547,    0.1710,         0,         0,         0],
   [    0.1860,    0.1742,    0.1534,    0.1486,    0.1601,    0.1777,         0,         0,         0],
   [    0.1774,    0.1677,    0.1508,    0.1509,    0.1672,    0.1860,         0,         0,         0],
   [    0.1662,    0.1585,    0.1469,    0.1538,    0.1773,    0.1972,         0,         0,         0],
   [    0.1563,    0.1503,    0.1433,    0.1562,    0.1866,    0.2072,         0,         0,         0],
   [    0.1537,    0.1485,    0.1428,    0.1570,    0.1887,    0.2092,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0], 
   [         0,         0,         0,         0,         0,         0,         0,         0,         0],
   [         0,         0,         0,         0,         0,         0,         0,         0,         0]]]]
Perte pour ce batch: 2.9057925
Epoch 3 completed with average loss: 0.03441852
Training completed.
Embedding After:
[   -0.0074,    0.0026,   -0.0020,   -0.0067,    0.0044,    0.0116,-6.9037e-5,   -0.0058,   -0.0140,    0.0113,    0.0042,    0.0009,   -0.0026,    0.0022,    0.0090,   -0.0082,    0.0105,    0.0089,   -0.0021,    0.0040,    0.0076,   -0.0053,   -0.0002,   -0.0035,    0.0037,   -0.0141,    0.0126,    0.0020,    0.0068,   -0.0066,    0.0011,    0.0079,    0.0085,   -0.0105,   -0.0067,    0.0058,    0.0118,   -0.0020,   -0.0166,   -0.0051,   -0.0023,    0.0030,    0.0015,    0.0120,    0.0096,    0.0011,   -0.0111,    0.0018,   -0.0030,    0.0148]
Embeddings Changed: true
