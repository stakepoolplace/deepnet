package RN.transformer;

import java.util.List;

public class Batch {
    private List<String> data;
    private List<String> target;

    public Batch(List<String> data, List<String> target) {
        this.data = data;
        this.target = target;
    }

    public List<String> getData() {
        return data;
    }

    public List<String> getTarget() {
        return target;
    }
}
package RN.transformer;

import java.io.Serializable;
import java.util.List;

import org.nd4j.linalg.api.ndarray.INDArray;
import org.nd4j.linalg.factory.Nd4j;
import org.nd4j.linalg.learning.AdamUpdater;
import org.nd4j.linalg.learning.config.Adam;

public class CustomAdamOptimizer implements Serializable {
    /**
	 * 
	 */
	private static final long serialVersionUID = 3031098044411623634L;
	private transient Adam adamConfig;
    private transient AdamUpdater adamUpdater;
    private double initialLr;
    private int modelSize;
    private int warmupSteps;
    private int currentStep;
    private int epoch;
    private double learningRate;
    private INDArray stateViewArray; // Utilisé pour stocker l'état interne de l'optimiseur.

    // Constructeur avec initialisation de l'état de l'optimiseur.
    public CustomAdamOptimizer(double initialLr, int dmodel, int warmupSteps, long numberOfParameters) {
        this.initialLr = initialLr;
        this.modelSize = dmodel;
        this.learningRate = initialLr; // Initialisation explicite du taux d'apprentissage à la valeur initiale.
        this.warmupSteps = warmupSteps;
        this.currentStep = 0;
        this.adamConfig = new Adam(initialLr);
        
        // Initialisation de stateViewArray en fonction du nombre total de paramètres.
        this.stateViewArray = Nd4j.zeros(1, 2 * numberOfParameters); // 2 * car Adam utilise m et v.
        
        this.adamUpdater = new AdamUpdater(adamConfig);
        this.adamUpdater.setStateViewArray(this.stateViewArray, new long[]{numberOfParameters}, 'c', true);
    }


    
    public void update(List<INDArray> params, List<INDArray> grads) {
        if (params.size() != grads.size()) {
            throw new IllegalArgumentException("La taille de la liste des paramètres et des gradients doit être la même.");
        }
        
        for (int i = 0; i < params.size(); i++) {
            INDArray param = params.get(i);
            INDArray grad = grads.get(i); // Récupération du gradient correspondant au paramètre.
            
            calculateLearningRate(); // Calcul du taux d'apprentissage actuel.
            adamConfig.setLearningRate(this.learningRate);
            adamUpdater.setConfig(adamConfig);

            // L'ajustement et l'application du gradient spécifique au paramètre courant.
            adamUpdater.applyUpdater(grad, currentStep, epoch); // Notez que grad est utilisé directement ici.

            // Mise à jour du paramètre en soustrayant le gradient ajusté.
            param.subi(grad);
        }
        
        currentStep++; // Incrémentation du nombre de pas après la mise à jour.
    }


    // Calcul du taux d'apprentissage en fonction du pas actuel.
    public double calculateLearningRate() {
        double step = Math.max(1.0, (double) currentStep);  // Commencer à 1 pour éviter la division par zéro
        
        // Learning rate de base
        double lr = initialLr;
        
        // Calculer le facteur de warmup (0 à 1 pendant la période de warmup)
        double warmupFactor = Math.min(1.0, step / warmupSteps);
        
        // Calculer le facteur de décroissance (diminue après la période de warmup)
        double decayFactor;
        if (step <= warmupSteps) {
            decayFactor = 1.0;
        } else {
            // Décroissance en racine carrée inverse après le warmup
            decayFactor = Math.sqrt(warmupSteps / step);
        }
        
        // Calculer le learning rate final
        this.learningRate = lr * warmupFactor * decayFactor;
        
        // Appliquer des limites pour éviter des valeurs extrêmes
        this.learningRate = Math.max(this.learningRate, initialLr * 0.1);   // Pas moins de 10% du lr initial
        this.learningRate = Math.min(this.learningRate, initialLr);         // Pas plus que le lr initial
        
        // Log des valeurs pour debugging
        if (currentStep % 100 == 0) {
            System.out.printf("Step: %d, Warmup: %.3f, Decay: %.3f, LR: %.6f%n", 
                currentStep, warmupFactor, decayFactor, this.learningRate);
        }
        
        return this.learningRate;
    }


    // Getters et setters.
    public void setCurrentStep(int step) {
        this.currentStep = step;
    }

    public int getCurrentStep() {
        return this.currentStep;
    }

    public void setEpoch(int epoch) {
        this.epoch = epoch;
    }

    public int getEpoch() {
        return this.epoch;
    }

    public double getLearningRate() {
        return learningRate;
    }

    public void setLearningRate(double learningRate) {
        this.learningRate = learningRate;
    }
}
package RN.transformer;

import java.io.BufferedReader;
import java.io.FileReader;
import java.io.IOException;
import java.util.ArrayList;
import java.util.List;

public class DataGenerator {
    private BufferedReader dataReader;
    private BufferedReader targetReader;
    protected Tokenizer tokenizer;
    private int batchSize;
    private int maxTokensPerBatch;
	private String targetFilePath;
	private String dataFilePath;

    public DataGenerator(String dataFilePath, String targetFilePath, Tokenizer tokenizer, int batchSize, int maxTokensPerBatch) throws IOException {
        this.dataReader = new BufferedReader(new FileReader(dataFilePath));
        this.targetReader = new BufferedReader(new FileReader(targetFilePath));
        this.targetFilePath = targetFilePath;
        this.dataFilePath = dataFilePath;
        this.tokenizer = tokenizer;
        this.batchSize = batchSize;
        this.maxTokensPerBatch = maxTokensPerBatch;
    }

    public boolean hasNextBatch() throws IOException {
        return dataReader.ready() && targetReader.ready();
    }

    public Batch nextBatch() throws IOException {
        List<String> dataBatch = new ArrayList<>();
        List<String> targetBatch = new ArrayList<>();
        StringBuilder dataBuffer = new StringBuilder();
        StringBuilder targetBuffer = new StringBuilder();

        while (dataBatch.size() < batchSize && hasNextBatch()) {
            int dataChar;
            while ((dataChar = dataReader.read()) != -1) {
                dataBuffer.append((char) dataChar);
                if (dataBuffer.length() >= maxTokensPerBatch) break;
            }

            int targetChar;
            while ((targetChar = targetReader.read()) != -1) {
                targetBuffer.append((char) targetChar);
                if (targetBuffer.length() >= maxTokensPerBatch) break;
            }

            if (dataBuffer.length() > 0 && targetBuffer.length() > 0) {
                List<String> dataTokens = tokenizer.tokenize(dataBuffer.toString());
                List<String> targetTokens = tokenizer.tokenize(targetBuffer.toString());
                dataBatch.add(String.join(" ", dataTokens));
                targetBatch.add(String.join(" ", targetTokens));
                dataBuffer = new StringBuilder(); // Réinitialiser les buffers pour le prochain segment
                targetBuffer = new StringBuilder();
            }
        }

        return new Batch(dataBatch, targetBatch);
    }

    public void close() throws IOException {
        dataReader.close();
        targetReader.close();
    }

    public void init() throws IOException {
        this.dataReader = new BufferedReader(new FileReader(dataFilePath));
        this.targetReader = new BufferedReader(new FileReader(targetFilePath));
    }    
}
package RN.transformer;

import java.io.Serializable;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

import org.nd4j.linalg.api.ndarray.INDArray;
import org.nd4j.linalg.factory.Nd4j;

public class Decoder implements Serializable {
    private static final long serialVersionUID = 283129978055526337L;
    private List<DecoderLayer> layers;
    private LayerNorm layerNorm;
    private int numLayers;
    protected int dModel;
    private int numHeads;
    private double dropoutRate;
    private LinearProjection linearProjection; // Projection linéaire vers la taille du vocabulaire

    // Cache pour stocker les entrées des couches pendant la passe forward
    private List<INDArray> forwardCache;

    public Decoder(int numLayers, int dModel, int numHeads, int dff, double dropoutRate) {
        this.numLayers = numLayers;
        this.dModel = dModel;
        this.numHeads = numHeads;
        this.dropoutRate = dropoutRate;
        this.layers = new ArrayList<>();
        this.layerNorm = new LayerNorm(dModel);
        this.linearProjection = new LinearProjection(dModel, TransformerModel.getVocabSize()); // Initialiser avec la taille du vocabulaire
        this.forwardCache = new ArrayList<>();

        for (int i = 0; i < numLayers; i++) {
            this.layers.add(new DecoderLayer(dModel, numHeads, dff, dropoutRate));
            this.forwardCache.add(null); // Initialiser le cache avec des valeurs nulles
        }
    }

    /**
     * Réinitialise le cache. Appelé avant une nouvelle passe forward.
     */
    public void resetCache() {
        for (int i = 0; i < forwardCache.size(); i++) {
            forwardCache.set(i, null);
        }
    }

    public INDArray decode(boolean isTraining, INDArray x, INDArray encoderOutput, INDArray lookAheadMask, INDArray paddingMask) {
        // Réinitialiser le cache avant une nouvelle passe forward
        resetCache();

        // Traitement par les couches de décodeur
        for (int i = 0; i < layers.size(); i++) {
            DecoderLayer layer = layers.get(i);
            x = layer.forward(isTraining, x, encoderOutput, lookAheadMask, paddingMask, forwardCache.get(i));
            forwardCache.set(i, x.dup()); // Stocker l'entrée actuelle dans le cache
        }

        // Normalisation finale
        x = layerNorm.forward(x);

        // Application d'une projection linéaire, si nécessaire
        if (this.linearProjection != null) {
            x = linearProjection.project(x); // Transforme la sortie du décodeur pour les logits de vocabulaire
        }

        return x;
    }

    public Map<String, INDArray> backward(INDArray gradOutput) {
        // Récupérer l'entrée de la dernière couche à partir du cache
        INDArray lastLayerInput = forwardCache.get(layers.size() - 1);
        if (lastLayerInput == null) {
            throw new IllegalStateException("Cache de la dernière couche non initialisé. Assurez-vous d'effectuer une passe forward avant.");
        }

        // Propager le gradient à travers LinearProjection
        Map<String, INDArray> gradLinearProjection = linearProjection.backward(lastLayerInput, gradOutput);

        // Propager le gradient à travers LayerNorm
        Map<String, INDArray> gradLayerNorm = layerNorm.backward(gradLinearProjection.get("input"));

        // Transformer gradOutput en un Map pour correspondre à la signature de DecoderLayer.backward
        Map<String, INDArray> gradMap = new HashMap<>();
        gradMap.put("input", gradLayerNorm.get("input")); // Utiliser "input" comme clé est arbitraire mais doit correspondre à ce que s'attend à recevoir DecoderLayer.backward

        // Commencer avec le gradient à la sortie du Decoder
        for (int i = layers.size() - 1; i >= 0; i--) {
            DecoderLayer layer = layers.get(i);
            // Passer le cache correspondant à cette couche
            INDArray layerInput = forwardCache.get(i);
            gradMap = layer.backward(gradMap, layerInput);
        }
        // À ce stade, gradMap contiendrait le gradient à propager à l'Encoder
        // Vous pouvez ensuite extraire le gradient à passer à l'encodeur ou à d'autres parties du modèle si nécessaire.

        return gradMap;
    }

    // Méthode pour obtenir tous les paramètres du décodeur
    public List<INDArray> getParameters() {
        List<INDArray> params = new ArrayList<>();

        // Collecter les paramètres de chaque couche du décodeur
        for (DecoderLayer layer : layers) {
            params.addAll(layer.getParameters());
        }

        // Collecter les paramètres de la normalisation de couche finale
        params.addAll(layerNorm.getParameters());

        // Collecter les paramètres de la projection linéaire
        params.addAll(linearProjection.getParameters());

        return params;
    }

    // Méthode pour obtenir tous les gradients du décodeur
    public List<INDArray> getGradients() {
        List<INDArray> grads = new ArrayList<>();

        // Collecter les gradients de chaque couche du décodeur
        for (DecoderLayer layer : layers) {
            grads.addAll(layer.getGradients());
        }

        // Collecter les gradients de la normalisation de couche finale
        grads.addAll(layerNorm.getGradients());

        // Collecter les gradients de la projection linéaire
        grads.addAll(linearProjection.getGradients());

        return grads;
    }

    // Méthode pour calculer les gradients basés sur la perte
    public INDArray calculateGradients(double loss) {
        // La logique réelle de calcul des gradients serait beaucoup plus complexe
        // et dépendrait des détails spécifiques de votre implémentation et de votre bibliothèque d'autograd.
        INDArray gradients = Nd4j.rand(1, 100); // Assumer des dimensions hypothétiques pour l'exemple
        return gradients;
    }

    public int getNumberOfParameters() {
        int numParams = 0;

        // Parcourir toutes les couches de décodeur pour compter leurs paramètres
        for (DecoderLayer layer : layers) {
            numParams += layer.getNumberOfParameters();
        }

        // Ajouter les paramètres de la normalisation de couche et de la projection linéaire
        numParams += layerNorm.getNumberOfParameters();
        numParams += linearProjection.getNumberOfParameters();

        return numParams;
    }

    static class DecoderLayer implements Serializable {
        private static final long serialVersionUID = 4450374170745550258L;
        MultiHeadAttention selfAttention;
        MultiHeadAttention encoderDecoderAttention;
        PositionwiseFeedForward feedForward;
        LayerNorm layerNorm1;
        LayerNorm layerNorm2;
        LayerNorm layerNorm3;
        Dropout dropout1;
        Dropout dropout2;
        Dropout dropout3;

        public DecoderLayer(int dModel, int numHeads, int dff, double dropoutRate) {
            this.selfAttention = new MultiHeadAttention(dModel, numHeads);
            this.encoderDecoderAttention = new MultiHeadAttention(dModel, numHeads);
            this.feedForward = new PositionwiseFeedForward(dModel, dff);
            this.layerNorm1 = new LayerNorm(dModel);
            this.layerNorm2 = new LayerNorm(dModel);
            this.layerNorm3 = new LayerNorm(dModel);
            this.dropout1 = new Dropout(dropoutRate);
            this.dropout2 = new Dropout(dropoutRate);
            this.dropout3 = new Dropout(dropoutRate);
        }

        /**
         * Passe forward avec gestion du cache.
         *
         * @param isTraining       Indique si le modèle est en mode entraînement
         * @param x                Entrée actuelle
         * @param encoderOutput    Sortie de l'encodeur
         * @param lookAheadMask    Masque de look-ahead
         * @param paddingMask      Masque de padding
         * @param cachedInput      Entrée mise en cache de la passe forward précédente
         * @return Sortie après cette couche
         */
        public INDArray forward(boolean isTraining, INDArray x, INDArray encoderOutput, INDArray lookAheadMask, INDArray paddingMask, INDArray cachedInput) {
            INDArray attn1 = selfAttention.forward(x, x, x, lookAheadMask);
            attn1 = dropout1.apply(isTraining, attn1);
            x = layerNorm1.forward(x.add(attn1));

            INDArray attn2 = encoderDecoderAttention.forward(x, encoderOutput, encoderOutput, paddingMask);
            attn2 = dropout2.apply(isTraining, attn2);
            x = layerNorm2.forward(x.add(attn2));

            INDArray ffOutput = feedForward.forward(x);
            ffOutput = dropout3.apply(isTraining, ffOutput);
            return layerNorm3.forward(x.add(ffOutput));
        }

        /**
         * Passe backward avec utilisation du cache.
         *
         * @param gradOutput Gradients provenant de la couche suivante
         * @param cachedInput Entrée mise en cache de la passe forward précédente
         * @return Gradients à propager vers les couches précédentes
         */
        public Map<String, INDArray> backward(Map<String, INDArray> gradOutput, INDArray cachedInput) {
            
            Map<String, INDArray> gradients = new HashMap<>();

        	// Rétropropagation à travers LayerNorm3
            Map<String, INDArray> gradLayerNorm3 = layerNorm3.backward(gradOutput.get("input"));
            gradients.putAll(gradLayerNorm3);

            // Rétropropagation à travers PositionwiseFeedForward
            Map<String, INDArray> gradFeedForward = feedForward.backward(gradLayerNorm3.get("input"));
            gradients.putAll(gradFeedForward);

            // Rétropropagation à travers LayerNorm2
            Map<String, INDArray> gradLayerNorm2 = layerNorm2.backward(gradFeedForward.get("input"));
            gradients.putAll(gradLayerNorm2);

            // Rétropropagation à travers encoderDecoderAttention
            Map<String, INDArray> gradEncoderDecoderAttention = encoderDecoderAttention.backward(gradLayerNorm2.get("input"));
            gradients.putAll(gradEncoderDecoderAttention);

            // Rétropropagation à travers LayerNorm1
            Map<String, INDArray> gradLayerNorm1 = layerNorm1.backward(gradEncoderDecoderAttention.get("inputQ"));
            gradients.putAll(gradLayerNorm1);

            // Rétropropagation à travers selfAttention
            Map<String, INDArray> gradSelfAttention = selfAttention.backward(gradLayerNorm1.get("input"));
            gradients.putAll(gradSelfAttention);

            // Retourner les gradients accumulés pour mise à jour des paramètres
            return gradients;
        }

        public List<INDArray> getParameters() {
            List<INDArray> layerParams = new ArrayList<>();

            layerParams.addAll(selfAttention.getParameters());
            layerParams.addAll(encoderDecoderAttention.getParameters());
            layerParams.addAll(feedForward.getParameters());
            layerParams.addAll(layerNorm1.getParameters());
            layerParams.addAll(layerNorm2.getParameters());
            layerParams.addAll(layerNorm3.getParameters());

            return layerParams;
        }

        public List<INDArray> getGradients() {
            List<INDArray> layerGrads = new ArrayList<>();

            layerGrads.addAll(selfAttention.getGradients());
            layerGrads.addAll(encoderDecoderAttention.getGradients());
            layerGrads.addAll(feedForward.getGradients());
            layerGrads.addAll(layerNorm1.getGradients());
            layerGrads.addAll(layerNorm2.getGradients());
            layerGrads.addAll(layerNorm3.getGradients());

            return layerGrads;
        }

        public long getNumberOfParameters() {
            return selfAttention.getNumberOfParameters() +
                   encoderDecoderAttention.getNumberOfParameters() +
                   feedForward.getNumberOfParameters() +
                   layerNorm1.getNumberOfParameters() +
                   layerNorm2.getNumberOfParameters() +
                   layerNorm3.getNumberOfParameters();
        }

        public long getNumberOfGradients() {
            return selfAttention.getNumberOfGradients() +
                   encoderDecoderAttention.getNumberOfGradients() +
                   feedForward.getNumberOfGradients() +
                   layerNorm1.getNumberOfGradients() +
                   layerNorm2.getNumberOfGradients() +
                   layerNorm3.getNumberOfGradients();
        }
    }
}
package RN.transformer;

import java.io.Serializable;

import org.nd4j.linalg.api.ndarray.INDArray;
import org.nd4j.linalg.factory.Nd4j;

public class Dropout implements Serializable {
    /**
	 * 
	 */
	private static final long serialVersionUID = -61325399079678110L;
	private double rate;
    private INDArray mask;


    public Dropout(double rate) {
        this.rate = rate;
    }

    
    public INDArray apply(boolean isTraining, INDArray input) {
        if(isTraining) {
            // Création du masque d'activation basée sur le taux de dropout
            this.mask = Nd4j.rand(input.shape()).gt(rate);
            // Application du masque aux données d'entrée
            return input.mul(mask);
        } else {
            // Pendant l'inférence, dropout n'est pas appliqué mais les activations sont ajustées
            return input.mul(1.0 - rate);
        }
    }
    
    public INDArray backward(INDArray gradOutput) {
        // Pendant la rétropropagation, simplement passer le gradient à travers le masque
        return gradOutput.mul(mask);
    }    
    
}
package RN.transformer;


import java.io.Serializable;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

import org.nd4j.linalg.api.ndarray.INDArray;
import org.nd4j.linalg.factory.Nd4j;


/**
 * 	numLayers: Le nombre de couches répétitives dans l'encodeur.
 *	dModel: La dimensionnalité des embeddings de tokens et des sorties de toutes les couches dans le modèle.
 *	numHeads: Le nombre de têtes d'attention dans les mécanismes d'attention multi-têtes.
 *	dff: La dimensionnalité des couches feed-forward internes dans chaque couche d'encodeur.
 *	vocabSize: La taille du vocabulaire, nécessaire pour les embeddings de tokens.
 *	maxSeqLength: La longueur maximale de séquence, utilisée pour les embeddings positionnels.
 */

public class Encoder implements Serializable  {
	
    /**
	 * 
	 */
	private static final long serialVersionUID = -5716799542280937448L;
	private List<EncoderLayer> layers;
    private PositionalEncoding positionalEncoding;
    private LayerNorm layerNorm;
    private Tokenizer tokenizer;

    public Encoder() {
	}
    
    public Encoder(int numLayers, int dModel, int numHeads, int dff, double dropoutRate, Tokenizer tokenizer) {
    	this.positionalEncoding = new PositionalEncoding(dModel);
        this.layers = new ArrayList<>();
        this.layerNorm = new LayerNorm(dModel);
        this.tokenizer = tokenizer;
        
        for (int i = 0; i < numLayers; i++) {
            this.layers.add(new EncoderLayer(dModel, numHeads, dff, dropoutRate));
        }
    }



	public List<List<Float>> encode(boolean isTraining, String text) {
        // Tokenization du texte
        List<String> tokens = tokenizer.tokenize(text);
        // Conversion des tokens en IDs
        List<Integer> tokenIds = tokenizer.tokensToIds(tokens);

        // Encodage des IDs de tokens à travers les couches de l'encodeur
        INDArray inputEmbeddings = lookupEmbeddings(tokenIds);
        INDArray encoded = forward(isTraining, inputEmbeddings, null);
        
        // Conversion des embeddings encodés en logits
        return convertToLogits(encoded);
    }
    
    public INDArray encode(boolean isTraining, List<Integer> tokenIds, INDArray paddingMask) {
    	
        // Utiliser la matrice d'embeddings pré-entraînée pour récupérer les embeddings correspondants aux IDs de tokens
        INDArray inputEmbeddings = lookupEmbeddings(tokenIds);

        // Appliquer les transformations de l'encodeur sur les embeddings
        INDArray encoded = forward(isTraining, inputEmbeddings, paddingMask);

        return encoded;
    }
    

    
   

    private INDArray lookupEmbeddings(List<Integer> tokenIds) {
        // Utiliser la matrice d'embeddings pré-entraînée pour récupérer les embeddings correspondants aux IDs de tokens
        int maxSeqLength = tokenIds.size();
        int dModel = layers.get(0).selfAttention.getdModel();
        INDArray embeddings = Nd4j.zeros(maxSeqLength, dModel);

        for (int i = 0; i < tokenIds.size(); i++) {
            int tokenId = tokenIds.get(i) - 1;
            
            // Vérifier que tokenId est valide
            if (tokenId >= TransformerModel.getPretrainedEmbeddings().rows()) {
                throw new IllegalArgumentException("Token ID " + tokenId + " est hors des limites de la matrice d'embeddings qui a " + TransformerModel.getPretrainedEmbeddings().rows() + " lignes.");
            }
            
            
            // Récupérer l'embedding correspondant au token ID à partir de la matrice d'embeddings pré-entraînée
            embeddings.putRow(i, TransformerModel.getPretrainedEmbeddings().getRow(tokenId));
        }

        return embeddings;
    }

    private List<List<Float>> convertToLogits(INDArray encoded) {
        // Convertir les embeddings encodés en logits
        List<List<Float>> logits = new ArrayList<>();
        int seqLength = (int) encoded.size(0);
        for (int i = 0; i < seqLength; i++) {
            INDArray row = encoded.getRow(i);
            List<Float> rowList = new ArrayList<>();
            for (int j = 0; j < row.length(); j++) {
                rowList.add(row.getFloat(j)); // Ajouter la valeur de l'élément à la liste des logits
            }
            logits.add(rowList);
        }
        return logits;
    }

    private INDArray forward(boolean isTraining, INDArray x, INDArray paddingMask) {
    	
        // Appliquer les embeddings positionnels
        INDArray posEncoding = positionalEncoding.getPositionalEncoding(x.shape()[0]);
        x = x.add(posEncoding);

        for (EncoderLayer layer : layers) {
            x = layer.forward(isTraining, x, paddingMask);
        }
        
        x =  layerNorm.forward(x);

        return x;
    }
    

    public void backward(Map<String, INDArray> gradOutput) {
    	
    	INDArray gradK = gradOutput.get("gradK");
    	INDArray gradV = gradOutput.get("gradV");
    	
    	INDArray gradFromDecoder = gradK.add(gradV);  // Cette étape suppose que gradK et gradV sont adaptés pour être sommés ainsi
        // Backpropagation à travers la normalisation de couche finale
    	Map<String, INDArray> gradientsFromLayerNorm = layerNorm.backward(gradFromDecoder);

        // Récupération du gradient par rapport aux entrées de LayerNorm qui sera utilisé comme gradient initial pour les couches de l'Encoder
        INDArray gradInput = gradientsFromLayerNorm.get("input");

        // Propagation des gradients à travers chaque couche d'Encoder en ordre inverse
        for (int i = layers.size() - 1; i >= 0; i--) {
            // Chaque couche retourne le gradient par rapport à ses entrées qui est passé à la couche précédente
            gradInput = layers.get(i).backward(gradInput);
        }

        // Mettre à jour ou enregistrer les gradients pour gamma et beta si nécessaire
        // Par exemple, si ces paramètres sont appris :
        // updateGammaBeta(gradFromLayerNorm.get("gamma"), gradFromLayerNorm.get("beta"));
    }

    // Méthode hypothétique pour mettre à jour ou enregistrer les gradients de gamma et beta
    private void updateGammaBeta(INDArray gradGamma, INDArray gradBeta) {
        // Mettre à jour ou enregistrer les gradients de gamma et beta
        // Ceci pourrait inclure l'application d'un taux d'apprentissage ou l'enregistrement pour une utilisation dans un pas d'optimisation
    }




    
    // Méthode pour calculer les gradients basés sur la perte
    public INDArray calculateGradients(double loss) {
        // Dans un cas réel, cette méthode impliquerait le calcul du gradient de la perte par rapport à chaque paramètre
        // Pour cet exemple, simuler un gradient comme un INDArray de mêmes dimensions que les paramètres
        INDArray gradients = Nd4j.rand(1, 100); // Assumer les mêmes dimensions hypothétiques que les paramètres
        return gradients;
    }
    
    
    public List<INDArray> getParameters() {
        List<INDArray> params = new ArrayList<>();
        // Collecter les poids et biais de multiHeadAttention et positionwiseFeedForward
        for (EncoderLayer layer : layers) {
            params.addAll(layer.getParameters());
        }

        // Inclure les paramètres de la normalisation de couche finale
        if(layerNorm != null) {
            params.addAll(layerNorm.getParameters());
        }
        
        return params;
    }
    
    public List<INDArray> getGradients() {
        List<INDArray> grads = new ArrayList<>();
        // Collecter les poids et biais de multiHeadAttention et positionwiseFeedForward
        for (EncoderLayer layer : layers) {
        	grads.addAll(layer.getGradients());
        }

        // Inclure les gradients de la normalisation de couche finale
        if(layerNorm != null) {
        	grads.addAll(layerNorm.getGradients());
        }
        
        return grads;
    }
    

    
    public int getNumberOfParameters() {
        int numParams = 0;

        // Parcourir toutes les couches d'encodeur pour compter leurs paramètres
        for (EncoderLayer layer : layers) {
            numParams += layer.getNumberOfParameters();
        }

        // Ajouter les paramètres de la normalisation de couche et des embeddings positionnels
        numParams += layerNorm.getNumberOfParameters();

        return numParams;
    }
    
    
    public int getNumberOfGradients() {
        int numGrads = 0;

        // Parcourir toutes les couches d'encodeur pour compter leurs gradients
        for (EncoderLayer layer : layers) {
        	numGrads += layer.getNumberOfGradients();
        }

        // Ajouter les gradients de la normalisation de couche et des embeddings positionnels
        numGrads += layerNorm.getNumberOfGradients();

        return numGrads;
    }



    static class EncoderLayer implements Serializable {
    	
        /**
		 * 
		 */
		private static final long serialVersionUID = -88886021425567141L;
		
		MultiHeadAttention selfAttention;
        PositionwiseFeedForward feedForward;
        LayerNorm layerNorm1;
        LayerNorm layerNorm2;
        Dropout dropout1;
        Dropout dropout2;

        public EncoderLayer(int dModel, int numHeads, int dff, double dropoutRate) {
        	
            this.selfAttention = new MultiHeadAttention(dModel, numHeads);
            this.feedForward = new PositionwiseFeedForward(dModel, dff);
            this.layerNorm1 = new LayerNorm(dModel);
            this.layerNorm2 = new LayerNorm(dModel);
            this.dropout1 = new Dropout(dropoutRate);
            this.dropout2 = new Dropout(dropoutRate);
        }
        
        public INDArray forward(boolean isTraining, INDArray x, INDArray paddingMask) {
        	
            INDArray attnOutput = selfAttention.forward(x, x, x, paddingMask);
            attnOutput = dropout1.apply(isTraining, attnOutput);
            x = layerNorm1.forward(x.add(attnOutput)); // Add & norm

            INDArray ffOutput = feedForward.forward(x);
            ffOutput = dropout2.apply(isTraining, ffOutput);
            return layerNorm2.forward(x.add(ffOutput)); // Add & norm again
        }
        
        public INDArray backward(INDArray gradOutput) {
        	
            // Backward à travers la deuxième normalisation de couche
            Map<String, INDArray> gradLayerNorm2 = layerNorm2.backward(gradOutput);
            INDArray gradToFeedForward = gradLayerNorm2.get("input");

            // Backward à travers la couche PositionwiseFeedForward
            Map<String, INDArray> gradFeedForward = feedForward.backward(gradToFeedForward);
            INDArray gradToLayerNorm1 = gradFeedForward.get("input");

            // Backward à travers la première normalisation de couche
            Map<String, INDArray> gradLayerNorm1 = layerNorm1.backward(gradToLayerNorm1);
            INDArray gradToSelfAttention = gradLayerNorm1.get("input");

            // Backward à travers SelfAttention
            Map<String, INDArray> gradSelfAttention = selfAttention.backward(gradToSelfAttention);

            // Préparer les gradients pour les étapes suivantes si nécessaire
//            INDArray gradInput = gradSelfAttention.get("inputQ");  // Utilisation de 'inputQ' comme exemple de gradient retourné
//            INDArray gradK = gradSelfAttention.get("inputK");
//            INDArray gradV = gradSelfAttention.get("inputV");
//
//            Map<String, INDArray> gradInputs = new HashMap<>();
//            gradInputs.put("input", gradInput);
//            gradInputs.put("gradK", gradK);
//            gradInputs.put("gradV", gradV);

            return gradSelfAttention.get("inputQ");
        }


        
        public List<INDArray> getParameters() {
            List<INDArray> layerParams = new ArrayList<>();
            
            // Collecter les paramètres des composants de la couche d'encodeur
            layerParams.addAll(selfAttention.getParameters());
            layerParams.addAll(feedForward.getParameters());
            layerParams.addAll(layerNorm1.getParameters());
            layerParams.addAll(layerNorm2.getParameters());

            return layerParams;
        }
        
        public List<INDArray> getGradients() {
            List<INDArray> layerGrads = new ArrayList<>();
            
            // Collecter les paramètres des composants de la couche d'encodeur
            layerGrads.addAll(selfAttention.getGradients());
            layerGrads.addAll(feedForward.getGradients());
            layerGrads.addAll(layerNorm1.getGradients());
            layerGrads.addAll(layerNorm2.getGradients());

            return layerGrads;
        }
        

        public long getNumberOfParameters() {
            return selfAttention.getNumberOfParameters() +
                   feedForward.getNumberOfParameters() +
                   layerNorm1.getNumberOfParameters() +
                   layerNorm2.getNumberOfParameters();
        }
        
        

        public long getNumberOfGradients() {
            return selfAttention.getNumberOfGradients() +
                   feedForward.getNumberOfGradients() +
                   layerNorm1.getNumberOfGradients() +
                   layerNorm2.getNumberOfGradients();
        }



    }
    
    
    
}
package RN.transformer;

import java.io.Serializable;
import java.util.Map;

import org.nd4j.linalg.api.ndarray.INDArray;

class Layer implements Serializable {
    /**
	 * 
	 */
	private static final long serialVersionUID = 6248679804921861953L;
	// Paramètres de la couche (poids, biais, etc.)
    INDArray weights;
    INDArray bias;

    // Méthode pour calculer la passe en avant
    public INDArray forward(INDArray input) {
        // Implémentation spécifique de la couche
        return null;
    }

    // Méthode pour calculer la rétropropagation
    public Map<String, INDArray> backward(INDArray incomingGradient) {
        // Calculer les gradients par rapport aux paramètres de la couche
        // et propager le gradient de la perte à la couche précédente
        return null;
    }
}
package RN.transformer;

import java.io.Serializable;
import java.util.Arrays;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

import org.nd4j.linalg.api.ndarray.INDArray;
import org.nd4j.linalg.factory.Nd4j;
import org.nd4j.linalg.ops.transforms.Transforms;

/**
 * Classe représentant une normalisation de couche (LayerNorm).
 * Les tenseurs sont supposés avoir la forme [seqLength, dModel].
 */
public class LayerNorm extends Layer implements Serializable {
    private static final long serialVersionUID = 941772045774041840L;
    private INDArray gamma, beta;
    private final double epsilon = 1e-6;
    private INDArray inputCache; // Cache pour le forward
    private Map<String, INDArray> gradients = new HashMap<>();

    /**
     * Constructeur de la classe LayerNorm.
     * 
     * @param dModel Dimension du modèle (dModel)
     */
    public LayerNorm(int dModel) {
        // Initialisation de gamma à des uns et de beta à des zéros avec la forme [1, dModel]
        gamma = Nd4j.ones(1, dModel); // [1, dModel]
        beta = Nd4j.zeros(1, dModel); // [1, dModel]
    }

    /**
     * Passe forward de la normalisation de couche.
     * 
     * @param x Entrée de forme [seqLength, dModel]
     * @return Sortie normalisée de forme [seqLength, dModel]
     */
    @Override
    public INDArray forward(INDArray x) {
        // Vérification des valeurs NaN ou Inf dans l'entrée
        if (x.isNaN().any() || x.isInfinite().any()) {
            throw new RuntimeException("LayerNorm.forward received NaN or Infinite values in input.");
        }

        // Stockage de l'entrée pour la rétropropagation
        this.inputCache = x.dup();

        // Calcul de la moyenne et de la variance sur la dimension dModel (axis=1)
        INDArray mean = x.mean(1).reshape(x.rows(), 1); // [seqLength, 1]
        INDArray variance = x.var(false, 1).reshape(x.rows(), 1); // [seqLength, 1]

        // Ajout de epsilon pour éviter la division par zéro
        INDArray std = Transforms.sqrt(variance.add(epsilon)); // [seqLength, 1]

        // Vérification des valeurs NaN ou Inf dans les résultats intermédiaires
        if (mean.isNaN().any() || mean.isInfinite().any()) {
            throw new RuntimeException("NaN or Infinite values encountered in mean calculation.");
        }
        if (std.isNaN().any() || std.isInfinite().any()) {
            throw new RuntimeException("NaN or Infinite values encountered in standard deviation calculation.");
        }

        // Normalisation : (x - mean) / std
        INDArray normalized = x.sub(mean).div(std); // [seqLength, dModel]

        // Mise à l'échelle et décalage : normalized * gamma + beta
        INDArray output = normalized.mulRowVector(gamma).addRowVector(beta); // [seqLength, dModel]

        // Vérification des valeurs NaN ou Inf dans la sortie
        if (output.isNaN().any() || output.isInfinite().any()) {
            throw new RuntimeException("NaN or Infinite values produced by LayerNorm normalization.");
        }

        return output;
    }

    /**
     * Passe backward de la normalisation de couche.
     * 
     * @param gradOutput Gradient provenant de la couche suivante de forme [seqLength, dModel]
     * @return Map contenant les gradients pour les paramètres 'gamma', 'beta' et 'input'
     */
    @Override
    public Map<String, INDArray> backward(INDArray gradOutput) {
        INDArray input = this.inputCache; // [seqLength, dModel]
        long seqLength = input.shape()[0];
        long dModel = input.shape()[1];

        // Recalcul de la moyenne et de la variance comme dans le forward
        INDArray mean = input.mean(1).reshape(seqLength, 1); // [seqLength, 1]
        INDArray variance = input.var(false, 1).reshape(seqLength, 1); // [seqLength, 1]
        INDArray stdInv = Transforms.pow(variance.add(epsilon), -0.5); // [seqLength, 1]
        INDArray normalized = input.sub(mean).mul(stdInv); // [seqLength, dModel]

        // Reshape de gamma pour le broadcasting : [1, dModel]
        INDArray gammaReshaped = gamma.reshape(1, dModel); // [1, dModel]

        // Calcul des gradients pour gamma et beta
        INDArray gradGamma = gradOutput.mul(normalized).sum(0); // [dModel]
        INDArray gradBeta = gradOutput.sum(0); // [dModel]

        // Calcul du gradient par rapport à la normalisation
        INDArray gradNormalized = gradOutput.mul(gammaReshaped); // [seqLength, dModel]

        // Calcul du gradient par rapport à l'entrée
        INDArray sumGradNorm = gradNormalized.mul(normalized).sum(1).reshape(seqLength, 1); // [seqLength, 1]
        INDArray gradInput = gradNormalized.mul(stdInv).sub(normalized.mul(sumGradNorm).mul(stdInv)); // [seqLength, dModel]

        // Stockage des gradients dans la map
        gradients.put("gamma", gradGamma);
        gradients.put("beta", gradBeta);
        gradients.put("input", gradInput); // Gradient à propager vers les couches précédentes

        return gradients;
    }

    /**
     * Obtient les gradients des paramètres.
     * 
     * @return Liste des gradients dans l'ordre [gamma, beta]
     */
    public List<INDArray> getGradients() {
        return Arrays.asList(gradients.get("gamma"), gradients.get("beta"));
    }

    /**
     * Obtient les paramètres de la normalisation de couche.
     * 
     * @return Liste des paramètres dans l'ordre [gamma, beta]
     */
    public List<INDArray> getParameters() {
        return Arrays.asList(gamma, beta);
    }

    /**
     * Définit (met à jour) les paramètres de la normalisation de couche.
     * 
     * @param newGamma Nouvelles valeurs pour gamma
     * @param newBeta  Nouvelles valeurs pour beta
     */
    public void setParameters(INDArray newGamma, INDArray newBeta) {
        this.gamma = newGamma;
        this.beta = newBeta;
    }

    /**
     * Obtient le nombre total de paramètres.
     * 
     * @return Nombre total de paramètres
     */
    public long getNumberOfParameters() {
        // gamma et beta ont chacun une taille de dModel
        return gamma.length() + beta.length();
    }

    /**
     * Obtient le nombre total de gradients.
     * 
     * @return Nombre total de gradients
     */
    public long getNumberOfGradients() {
        return gradients.get("gamma").length() + gradients.get("beta").length();
    }
}
package RN.transformer;

import java.io.Serializable;
import java.util.Arrays;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

import org.nd4j.linalg.api.ndarray.INDArray;
import org.nd4j.linalg.factory.Nd4j;
import org.nd4j.linalg.ops.transforms.Transforms;

/**
 * Classe représentant une projection linéaire avec normalisation de couche (LayerNorm).
 * Les tenseurs sont supposés avoir la forme [seqLength, dModel].
 */
public class LinearProjection implements Serializable {
    
    private static final long serialVersionUID = -6601830517666118676L;
    private INDArray weights; // Poids de la projection linéaire [dModel, outputSize]
    private INDArray bias;    // Biais de la projection linéaire [1, outputSize]
    private INDArray gamma;   // Paramètre de scale pour LayerNorm [1, dModel]
    private INDArray beta;    // Paramètre de shift pour LayerNorm [1, dModel]
    private final double epsilon = 1e-7; // Petite constante pour éviter la division par zéro
    
    // Gradients calculés lors de la passe backward
    private Map<String, INDArray> gradients = new HashMap<>();
    
    /**
     * Constructeur de la classe LinearProjection.
     * 
     * @param inputSize  Taille de l'entrée (dModel)
     * @param outputSize Taille de la sortie (par exemple, la taille du vocabulaire)
     */
    public LinearProjection(int inputSize, int outputSize) {
        // Initialisation des poids avec une distribution normale divisée par sqrt(inputSize) pour l'initialisation de He
        this.weights = Nd4j.randn(inputSize, outputSize).div(Math.sqrt(inputSize));
        // Initialisation des biais à zéro
        this.bias = Nd4j.zeros(1, outputSize);
        // Initialisation de gamma à un et beta à zéro pour la normalisation de couche
        this.gamma = Nd4j.ones(1, inputSize); // [1, dModel]
        this.beta = Nd4j.zeros(1, inputSize); // [1, dModel]
    }

    /**
     * Effectue la projection linéaire.
     * 
     * @param input Entrée de forme [seqLength, dModel]
     * @return Sortie projetée de forme [seqLength, outputSize]
     */
    public INDArray project(INDArray input) {
        // Projection linéaire: [seqLength, dModel] * [dModel, outputSize] + [1, outputSize] = [seqLength, outputSize]
        return input.mmul(weights).addRowVector(bias);
    }

    /**
     * Passe forward avec normalisation de couche et projection linéaire.
     * 
     * @param input Entrée de forme [seqLength, dModel]
     * @return Sortie projetée de forme [seqLength, outputSize]
     */
    public INDArray forward(INDArray input) {
        // Calcul de la moyenne et de la variance sur la dimension dModel (axis=1)
        INDArray mean = input.mean(1).reshape(input.rows(), 1); // [seqLength, 1]
        INDArray variance = input.var(false, 1).reshape(input.rows(), 1); // [seqLength, 1]
        INDArray stdInv = Transforms.pow(variance.add(epsilon), -0.5); // [seqLength, 1]
        
        // Normalisation: (input - mean) / sqrt(variance + epsilon)
        INDArray normalized = input.sub(mean).mul(stdInv); // [seqLength, dModel]
        
        // Mise à l'échelle et décalage: normalized * gamma + beta
        INDArray scaled = normalized.mul(gamma).add(beta); // [seqLength, dModel]
        
        // Projection linéaire
        INDArray output = scaled.mmul(weights).addRowVector(bias); // [seqLength, outputSize]
        return output;
    }

    /**
     * Passe backward pour calculer les gradients.
     * 
     * @param input      Entrée originale utilisée dans la passe forward de forme [seqLength, dModel]
     * @param gradOutput Gradient provenant de la couche suivante de forme [seqLength, outputSize]
     * @return Map contenant les gradients pour les paramètres 'weights', 'bias', 'gamma' et 'beta', ainsi que 'input'
     */
    public Map<String, INDArray> backward(INDArray input, INDArray gradOutput) {
        // Calcul des moyennes et variances
        INDArray mean = input.mean(1).reshape(input.rows(), 1); // [seqLength, 1]
        INDArray variance = input.var(false, 1).reshape(input.rows(), 1); // [seqLength, 1]
        INDArray stdInv = Transforms.pow(variance.add(epsilon), -0.5); // [seqLength, 1]

        // Calcul de normalized = (input - mean) / sqrt(var + epsilon)
        INDArray normalized = input.sub(mean).mul(stdInv); // [seqLength, dModel]

        // Gradients pour la projection linéaire
        INDArray gradScaled = gradOutput.mmul(weights.transpose()); // [seqLength, dModel]

        // Gradients pour gamma et beta de LayerNorm
        INDArray gradGamma = normalized.mul(gradScaled).sum(0); // [dModel]
        INDArray gradBeta = gradScaled.sum(0); // [dModel]

        // Gradients pour la normalisation
        INDArray gradNormalized = gradScaled.mul(gamma); // [seqLength, dModel]
        
        // Calcul des gradients pour l'entrée
        INDArray sumGradNormInput = gradNormalized.mul(normalized).sum(1).reshape(input.rows(), 1); // [seqLength, 1]
        INDArray gradInput = gradNormalized.mul(stdInv).sub(normalized.mul(sumGradNormInput).mul(stdInv)); // [seqLength, dModel]

        // Gradients pour les poids et les biais
        INDArray gradWeights = input.transpose().mmul(gradOutput); // [dModel, outputSize]
        INDArray gradBias = gradOutput.sum(0); // [outputSize]

        // Stockage des gradients dans la map
        gradients.put("weights", gradWeights);
        gradients.put("bias", gradBias);
        gradients.put("gamma", gradGamma);
        gradients.put("beta", gradBeta);
        gradients.put("input", gradInput); // Gradient à propager vers les couches précédentes

        return gradients;
    }

    /**
     * Obtient les gradients des paramètres.
     * 
     * @return Liste des gradients dans l'ordre [weights, bias, gamma, beta]
     */
    public List<INDArray> getGradients() {
        return Arrays.asList(gradients.get("weights"), gradients.get("bias"), gradients.get("gamma"), gradients.get("beta"));
    }

    /**
     * Obtient les paramètres de la projection linéaire.
     * 
     * @return Liste des paramètres dans l'ordre [weights, bias, gamma, beta]
     */
    public List<INDArray> getParameters() {
        return Arrays.asList(weights, bias, gamma, beta);
    }

    /**
     * Définit (met à jour) les paramètres de la projection linéaire.
     * 
     * @param newWeights Nouvelles valeurs pour les poids
     * @param newBias    Nouvelles valeurs pour les biais
     * @param newGamma   Nouvelles valeurs pour gamma
     * @param newBeta    Nouvelles valeurs pour beta
     */
    public void setParameters(INDArray newWeights, INDArray newBias, INDArray newGamma, INDArray newBeta) {
        this.weights = newWeights;
        this.bias = newBias;
        this.gamma = newGamma;
        this.beta = newBeta;
    }

    /**
     * Obtient le nombre total de paramètres.
     * 
     * @return Nombre total de paramètres
     */
    public long getNumberOfParameters() {
        return weights.length() + bias.length() + gamma.length() + beta.length();
    }

    /**
     * Obtient le nombre total de gradients.
     * 
     * @return Nombre total de gradients
     */
    public long getNumberOfGradients() {
        return gradients.get("weights").length() + gradients.get("bias").length() + gradients.get("gamma").length() + gradients.get("beta").length();
    }
}
package RN.transformer;

import java.io.Serializable;
import java.util.Arrays;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

import org.nd4j.linalg.api.ndarray.INDArray;
import org.nd4j.linalg.factory.Nd4j;
import org.nd4j.linalg.ops.transforms.Transforms;

public class MultiHeadAttention implements Serializable {

	/**
	 * 
	 */
	private static final long serialVersionUID = -7153801764592720027L;
	private int dModel;
	private int numHeads;
	private int depth;
	private INDArray inputQ, inputK, inputV; // Inputs cachés pour le backward
	private INDArray Wq, Wk, Wv, Wo; // Poids pour les requêtes, clés, valeurs et sortie
	private INDArray attentionWeights; // Poids d'attention cachés pour le backward
	private INDArray attentionOutput;
	private Map<String, INDArray> gradients = new HashMap<>();

	public MultiHeadAttention(int dModel, int numHeads) {
		if (dModel % numHeads != 0) {
			throw new IllegalArgumentException("dModel must be divisible by numHeads");
		}
		this.dModel = dModel;
		this.numHeads = numHeads;
		this.depth = dModel / numHeads;
		// Initialisation des poids. Les dimensions réelles dépendent de l'architecture
		// spécifique.
		Wq = Nd4j.rand(dModel, numHeads * depth);
		Wk = Nd4j.rand(dModel, numHeads * depth);
		Wv = Nd4j.rand(dModel, numHeads * depth);
		Wo = Nd4j.rand(dModel, dModel);
	}

	public INDArray forward(INDArray query, INDArray key, INDArray value, INDArray mask) {
		// Cacher les inputs pour une utilisation dans backward
		this.inputQ = query.dup();
		this.inputK = key.dup();
		this.inputV = value.dup();

		// Trouver la longueur maximale des séquences dans le lot
		int sequenceLength = (int) Math.max(query.shape()[0], Math.max(key.shape()[0], value.shape()[0]));

		// Remplir (padding) les séquences plus courtes avec des zéros (si nécessaire)
		query = padSequence(query, sequenceLength);
		key = padSequence(key, sequenceLength);
		value = padSequence(value, sequenceLength);

		// Projection linéaire des requêtes, clés et valeurs
		INDArray q = query.mmul(Wq).reshape(sequenceLength, numHeads, depth).permute(1, 0, 2); // [numHeads, seqLength,
																								// depth]
		INDArray k = key.mmul(Wk).reshape(sequenceLength, numHeads, depth).permute(1, 0, 2); // [numHeads, seqLength,
																								// depth]
		INDArray v = value.mmul(Wv).reshape(sequenceLength, numHeads, depth).permute(1, 0, 2); // [numHeads, seqLength,
																								// depth]

		// Transposer k pour obtenir [numHeads, depth, seqLength]
		k = k.permute(0, 2, 1);

		// Calcul des scores d'attention (produit matriciel Q x K^T)
		INDArray attentionScores = Nd4j.matmul(q, k).div(Math.sqrt(depth)); // [numHeads, seqLength, seqLength]

		// Application du masque, si fourni
		if (mask != null) {
			attentionScores.addi(mask); // Masque les scores non pertinents
		}

		// Calcul des poids d'attention avec softmax
		INDArray attentionWeights = Transforms.softmax(attentionScores);
		this.attentionWeights = attentionWeights;

		// Calcul de l'output avec V
		INDArray attentionOutput = Nd4j.matmul(attentionWeights, v); // [numHeads, seqLength, depth]

		// Permutation et reshape pour combiner les têtes
		this.attentionOutput = attentionOutput.permute(1, 0, 2) // [seqLength, numHeads, depth]
				.reshape(sequenceLength, numHeads * depth); // [seqLength, numHeads * depth]

		// Appliquer Wo pour la transformation linéaire finale
		return this.attentionOutput.mmul(Wo); // [seqLength, dModel]
	}

	private INDArray padSequence(INDArray sequence, int maxSeqLength) {
		int batchSize = (int) sequence.shape()[0];
		int seqLength = (int) sequence.shape()[1];

		if (seqLength < maxSeqLength) {
			INDArray paddingTensor = Nd4j.zeros(batchSize, maxSeqLength - seqLength, dModel);
			sequence = Nd4j.hstack(sequence, paddingTensor);
		}

		return sequence;
	}

	public Map<String, INDArray> backward(INDArray gradOutput) {
	    if (this.attentionOutput == null) {
	        throw new IllegalStateException("attentionOutput est null. Assurez-vous d'appeler la méthode forward avant backward.");
	    }

	    // 1. Définir les variables nécessaires
	    int seqLength = (int) attentionOutput.shape()[0];
	    int numHeads = this.numHeads;
	    int depth = this.depth;

	    // 2. Calcul du gradient par rapport à Wo
	    INDArray gradWo = attentionOutput.transpose().mmul(gradOutput); // [numHeads * depth, dModel]
	    gradients.put("Wo", gradWo);

	    // 3. Calcul du gradient par rapport à attentionOutput
	    INDArray gradAttentionOutput = gradOutput.mmul(Wo.transpose()); // [seqLength, numHeads * depth]

	    // 4. Reshape gradAttentionOutput de [seqLength, numHeads * depth] à [numHeads, seqLength, depth]
	    INDArray gradAttentionOutputReshaped = gradAttentionOutput.reshape(numHeads, seqLength, depth); // [numHeads, seqLength, depth]

	    // 5. Calcul de gradV = attentionWeights [numHeads, seqLength, seqLength] mmul gradAttentionOutputReshaped [numHeads, seqLength, depth] = [numHeads, seqLength, depth]
	    INDArray gradV = Nd4j.create(numHeads, seqLength, depth); // [numHeads, seqLength, depth]
	    for (int h = 0; h < numHeads; h++) {
	        // Remplacer getRow par slice
	        INDArray attentionWeightsHead = attentionWeights.slice(h); // [seqLength, seqLength]
	        
	        // Remplacer getRow par slice
	        INDArray gradAttentionOutputHead = gradAttentionOutputReshaped.slice(h); // [seqLength, depth]
	        
	        // Multiplication matricielle : [seqLength, seqLength] mmul [seqLength, depth] = [seqLength, depth]
	        INDArray gradVHead = attentionWeightsHead.mmul(gradAttentionOutputHead); // [seqLength, depth]
	        
	        // Remplacer putRow par putSlice
	        gradV.putSlice(h, gradVHead); // Assigner gradVHead à gradV[h]
	    }

	    // 6. Reshape gradV de [numHeads, seqLength, depth] à [seqLength, numHeads * depth]
	    INDArray gradVReshaped = gradV.reshape(seqLength, numHeads * depth); // [seqLength, numHeads * depth]

	    // 7. Calcul de gradWv
	    INDArray gradWv = inputV.transpose().mmul(gradVReshaped); // [dModel, numHeads * depth]
	    gradients.put("Wv", gradWv);

	    // 8. Reshape V de [seqLength, numHeads * depth] à [numHeads, seqLength, depth]
	    INDArray V = inputV.mmul(Wv); // [seqLength, numHeads * depth]
	    INDArray VReshaped = V.reshape(numHeads, seqLength, depth); // [numHeads, seqLength, depth]

	    // 9. Calcul de gradScores = gradAttentionOutputReshaped [numHeads, seqLength, depth] mmul VReshaped.transpose() [numHeads, depth, seqLength] = [numHeads, seqLength, seqLength]
	    INDArray gradScores = Nd4j.create(numHeads, seqLength, seqLength); // [numHeads, seqLength, seqLength]
	    for (int h = 0; h < numHeads; h++) {
	        // VReshaped[h].transpose() : [depth, seqLength]
	        INDArray VhT = VReshaped.slice(h).transpose(); // [depth, seqLength]

	        // gradAttentionOutputReshaped[h] : [seqLength, depth]
	        INDArray gradAttentionOutputHead = gradAttentionOutputReshaped.slice(h); // [seqLength, depth]

	        // Calcul de gradScores[h] : [seqLength, depth] mmul [depth, seqLength] = [seqLength, seqLength]
	        INDArray gradScoresHead = gradAttentionOutputHead.mmul(VhT); // [seqLength, seqLength]

	        // Remplacer putRow par putSlice
	        gradScores.putSlice(h, gradScoresHead); // Assigner gradScoresHead à gradScores[h]
	    }

	    // 10. Calcul du gradient de la softmax
	    INDArray gradAttentionScoresFinal = softmaxGrad(attentionWeights, gradScores); // [numHeads, seqLength, seqLength]

	    // 11. Calcul de Q : [seqLength, numHeads * depth] = [seqLength, numHeads * depth]
	    INDArray Q = inputQ.mmul(Wq); // [seqLength, numHeads * depth]
	    INDArray QReshaped = Q.reshape(numHeads, seqLength, depth); // [numHeads, seqLength, depth]

	    // 12. Calcul de gradQ = gradScoresFinal [numHeads, seqLength, seqLength] mmul (inputK * Wk) [numHeads, seqLength, depth] = [numHeads, seqLength, depth]
	    INDArray inputK_proj = inputK.mmul(Wk); // [seqLength, numHeads * depth]
	    INDArray inputK_projReshaped = inputK_proj.reshape(numHeads, seqLength, depth); // [numHeads, seqLength, depth]
	    INDArray gradQ_full = Nd4j.create(numHeads, seqLength, depth); // [numHeads, seqLength, depth]
	    for (int h = 0; h < numHeads; h++) {
	        // gradAttentionScoresFinal[h] : [seqLength, seqLength]
	        INDArray gradScoresHead = gradAttentionScoresFinal.slice(h); // [seqLength, seqLength]
	        
	        // inputK_projReshaped[h] : [seqLength, depth]
	        INDArray inputK_projHead = inputK_projReshaped.slice(h); // [seqLength, depth]
	        
	        // Calcul de gradQ_full[h] : [seqLength, depth] = [seqLength, seqLength] mmul [seqLength, depth]
	        INDArray gradQHead = gradScoresHead.mmul(inputK_projHead); // [seqLength, depth]
	        
	        // Remplacer putRow par putSlice
	        gradQ_full.putSlice(h, gradQHead); // Assigner gradQHead à gradQ_full[h]
	    }

	    // Reshape gradQ_full de [numHeads, seqLength, depth] à [seqLength, numHeads * depth]
	    INDArray gradQReshaped = gradQ_full.reshape(seqLength, numHeads * depth); // [seqLength, numHeads * depth]

	    // 13. Calcul de gradK = gradScoresFinal.transpose() [numHeads, seqLength, seqLength] mmul Q [numHeads, seqLength, depth] = [numHeads, seqLength, depth]
	    INDArray gradK_full = Nd4j.create(numHeads, seqLength, depth); // [numHeads, seqLength, depth]
	    for (int h = 0; h < numHeads; h++) {
	        // gradAttentionScoresFinal[h].transpose() : [seqLength, seqLength]
	        INDArray gradScoresHeadTrans = gradAttentionScoresFinal.slice(h).transpose(); // [seqLength, seqLength]
	        
	        // QReshaped[h] : [seqLength, depth]
	        INDArray QHead = QReshaped.slice(h); // [seqLength, depth]
	        
	        // Calcul de gradK_full[h] : [seqLength, depth] = [seqLength, seqLength] mmul [seqLength, depth]
	        INDArray gradKHead = gradScoresHeadTrans.mmul(QHead); // [seqLength, depth]
	        
	        // Remplacer putRow par putSlice
	        gradK_full.putSlice(h, gradKHead); // Assigner gradKHead à gradK_full[h]
	    }

	    // Reshape gradK_full de [numHeads, seqLength, depth] à [seqLength, numHeads * depth]
	    INDArray gradKReshaped = gradK_full.reshape(seqLength, numHeads * depth); // [seqLength, numHeads * depth]

	    // 14. Calcul de gradWq et gradWk
	    INDArray gradWq = inputQ.transpose().mmul(gradQReshaped); // [dModel, numHeads * depth]
	    INDArray gradWk = inputK.transpose().mmul(gradKReshaped); // [dModel, numHeads * depth]
	    gradients.put("Wq", gradWq);
	    gradients.put("Wk", gradWk);

	    // 15. Calcul des gradients par rapport aux entrées Q, K, V
	    INDArray gradInputQ = gradQReshaped.mmul(Wq.transpose()); // [seqLength, dModel]
	    INDArray gradInputK = gradKReshaped.mmul(Wk.transpose()); // [seqLength, dModel]
	    INDArray gradInputV = gradVReshaped.mmul(Wv.transpose()); // [seqLength, dModel]

	    // Ajouter les gradients spécifiques
	    gradients.put("inputQ", gradInputQ);
	    gradients.put("inputK", gradInputK);
	    gradients.put("inputV", gradInputV);

	    // 16. Calcul du gradient à propager vers les couches précédentes
	    INDArray gradInput = gradInputQ.add(gradInputK).add(gradInputV); // [seqLength, dModel]
	    gradients.put("input", gradInput);

	    return gradients;
	}


	/**
	 * Calcule le gradient de la softmax.
	 *
	 * @param softmax Résultats de la softmax de forme [numHeads, seqLength, seqLength]
	 * @param gradA   Gradients provenant de la couche suivante de la même forme que softmax
	 * @return Gradient par rapport aux scores d'attention de la même forme que softmax
	 */
	private INDArray softmaxGrad(INDArray softmax, INDArray gradA) {
	    // softmax: [numHeads, seqLength, seqLength]
	    // gradA: [numHeads, seqLength, seqLength]

	    // Calcul de dL/dS = softmax * (gradA - sum(gradA * softmax, axis=2, keepdims=true))
	    // ND4J M2.1 ne supporte pas directement les opérations sur les axes multiples, donc effectuer itération manuelle

	    INDArray gradS = Nd4j.create(softmax.shape());

	    int numHeads = (int) softmax.shape()[0];
	    int seqLength = (int) softmax.shape()[1];
	    int seqLength2 = (int) softmax.shape()[2];

	    for (int h = 0; h < numHeads; h++) {
	        for (int i = 0; i < seqLength; i++) {
	            double sum = 0.0;
	            for (int j = 0; j < seqLength2; j++) {
	                sum += softmax.getDouble(h, i, j) * gradA.getDouble(h, i, j);
	            }
	            for (int j = 0; j < seqLength2; j++) {
	                double grad = softmax.getDouble(h, i, j) * (gradA.getDouble(h, i, j) - sum);
	                gradS.putScalar(new int[]{h, i, j}, grad);
	            }
	        }
	    }

	    return gradS;
	}


	public List<INDArray> getParameters() {
		// Retourner les matrices de poids comme une liste d'INDArray
		return Arrays.asList(Wq, Wk, Wv, Wo);
	}

	public List<INDArray> getGradients() {
		return Arrays.asList(gradients.get("Wq"), gradients.get("Wk"), gradients.get("Wv"), gradients.get("Wo"));
	}

	public long getNumberOfParameters() {
		return Wq.length() + Wk.length() + Wv.length() + Wo.length();
	}

	public long getNumberOfGradients() {
		return gradients.get("Wq").length() + gradients.get("Wk").length() + gradients.get("Wv").length()
				+ gradients.get("Wo").length();
	}

	public int getdModel() {
		return dModel;
	}

	public void setdModel(int dModel) {
		this.dModel = dModel;
	}

	public int getNumHeads() {
		return numHeads;
	}

	public void setNumHeads(int numHeads) {
		this.numHeads = numHeads;
	}

	public INDArray getWq() {
		return Wq;
	}

	public void setWq(INDArray wq) {
		Wq = wq;
	}

	public INDArray getWk() {
		return Wk;
	}

	public void setWk(INDArray wk) {
		Wk = wk;
	}

	public INDArray getWv() {
		return Wv;
	}

	public void setWv(INDArray wv) {
		Wv = wv;
	}

	public INDArray getWo() {
		return Wo;
	}

	public void setWo(INDArray wo) {
		Wo = wo;
	}

}package RN.transformer;

import java.io.Serializable;

import org.nd4j.linalg.api.ndarray.INDArray;
import org.nd4j.linalg.factory.Nd4j;
import org.nd4j.linalg.indexing.NDArrayIndex;
import org.nd4j.linalg.ops.transforms.Transforms;

public class PositionalEncoding implements Serializable {
    /**
	 * 
	 */
	private static final long serialVersionUID = 7621854975948659411L;
	private final int dModel; // Dimensionnalité des embeddings
    
    public PositionalEncoding(int dModel) {
        this.dModel = dModel;
    }

    public INDArray getPositionalEncoding(long sequenceLength) {
        INDArray positions = Nd4j.arange(sequenceLength).reshape(sequenceLength, 1);
        INDArray i = Nd4j.arange(dModel).reshape(1, dModel);
        INDArray angleRates = Transforms.pow(i.divi(dModel).muli(-2).divi(2), 10000.0);

        INDArray angles = positions.mmul(angleRates);
        angles.get(NDArrayIndex.all(), NDArrayIndex.interval(0, 2, dModel)).assign(Transforms.sin(angles.get(NDArrayIndex.all(), NDArrayIndex.interval(0, 2, dModel))));
        angles.get(NDArrayIndex.all(), NDArrayIndex.interval(1, 2, dModel)).assign(Transforms.cos(angles.get(NDArrayIndex.all(), NDArrayIndex.interval(1, 2, dModel))));

        return angles;
    }


}


package RN.transformer;

import java.io.Serializable;
import java.util.Arrays;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

import org.nd4j.linalg.api.ndarray.INDArray;
import org.nd4j.linalg.factory.Nd4j;
import org.nd4j.linalg.ops.transforms.Transforms;

/**
 * Classe représentant le réseau Feed-Forward positionnel dans le Transformer.
 * Utilise deux couches linéaires avec une activation ReLU entre elles.
 * Les tenseurs sont supposés avoir la forme [seqLength, dModel].
 */
public class PositionwiseFeedForward implements Serializable {
    private static final long serialVersionUID = 4036365276693483563L;
    private INDArray W1, b1, W2, b2;
    private INDArray inputCache, reluCache; // Cache pour le forward
    private Map<String, INDArray> gradients = new HashMap<>();

    /**
     * Constructeur de la classe PositionwiseFeedForward.
     * 
     * @param modelSize Taille du modèle (dModel)
     * @param ffSize    Taille de la couche Feed-Forward (ffSize)
     */
    public PositionwiseFeedForward(int modelSize, int ffSize) {
        // Initialisation des poids avec une distribution uniforme ou normale selon le choix
        this.W1 = Nd4j.randn(modelSize, ffSize).div(Math.sqrt(modelSize)); // He Initialization
        this.b1 = Nd4j.zeros(1, ffSize);
        this.W2 = Nd4j.randn(ffSize, modelSize).div(Math.sqrt(ffSize)); // He Initialization
        this.b2 = Nd4j.zeros(1, modelSize);
    }

    /**
     * Passe forward du réseau Feed-Forward positionnel.
     * 
     * @param input Entrée de forme [seqLength, dModel]
     * @return Sortie de forme [seqLength, dModel]
     */
    public INDArray forward(INDArray input) {
        // Stockage de l'entrée pour la rétropropagation
        this.inputCache = input.dup();

        // Première couche linéaire suivie de ReLU
        INDArray hidden = input.mmul(W1).addRowVector(b1); // [seqLength, ffSize]
        this.reluCache = hidden.dup();
        INDArray reluOutput = Transforms.relu(hidden); // [seqLength, ffSize]

        // Deuxième couche linéaire
        INDArray output = reluOutput.mmul(W2).addRowVector(b2); // [seqLength, dModel]
        return output;
    }

    /**
     * Passe backward pour calculer les gradients.
     * 
     * @param gradOutput Gradient provenant de la couche suivante de forme [seqLength, dModel]
     * @return Map contenant les gradients pour les paramètres 'W1', 'b1', 'W2', 'b2', et 'input'
     */
    public Map<String, INDArray> backward(INDArray gradOutput) {
        // 1. Calcul des gradients par rapport à W2 et b2
        INDArray reluOutput = Transforms.relu(reluCache); // [seqLength, ffSize]
        INDArray gradW2 = reluOutput.transpose().mmul(gradOutput); // [ffSize, dModel]
        INDArray gradB2 = gradOutput.sum(0); // [dModel]

        // 2. Propagation du gradient à travers la deuxième couche linéaire
        INDArray gradHidden = gradOutput.mmul(W2.transpose()); // [seqLength, ffSize]

        // 3. Application de la dérivée de ReLU
        INDArray reluGrad = reluCache.gt(0).castTo(reluOutput.dataType()); // [seqLength, ffSize]
        INDArray gradThroughRelu = gradHidden.mul(reluGrad); // [seqLength, ffSize]

        // 4. Calcul des gradients par rapport à W1 et b1
        INDArray gradW1 = inputCache.transpose().mmul(gradThroughRelu); // [dModel, ffSize]
        INDArray gradB1 = gradThroughRelu.sum(0); // [ffSize]

        // 5. Calcul du gradient à propager à la couche précédente
        INDArray gradInput = gradThroughRelu.mmul(W1.transpose()); // [seqLength, dModel]

        // Stockage des gradients dans la map
        gradients.put("W1", gradW1);
        gradients.put("b1", gradB1);
        gradients.put("W2", gradW2);
        gradients.put("b2", gradB2);
        gradients.put("input", gradInput);

        return gradients;
    }

    /**
     * Obtient les gradients des paramètres.
     * 
     * @return Liste des gradients dans l'ordre [W1, b1, W2, b2]
     */
    public List<INDArray> getGradients() {
        return Arrays.asList(gradients.get("W1"), gradients.get("b1"), gradients.get("W2"), gradients.get("b2"));
    }

    /**
     * Obtient les paramètres du réseau Feed-Forward positionnel.
     * 
     * @return Liste des paramètres dans l'ordre [W1, b1, W2, b2]
     */
    public List<INDArray> getParameters() {
        return Arrays.asList(W1, b1, W2, b2);
    }

    /**
     * Définit (met à jour) les paramètres du réseau Feed-Forward positionnel.
     * 
     * @param newW1 Nouvelles valeurs pour W1
     * @param newB1 Nouvelles valeurs pour b1
     * @param newW2 Nouvelles valeurs pour W2
     * @param newB2 Nouvelles valeurs pour b2
     */
    public void setParameters(INDArray newW1, INDArray newB1, INDArray newW2, INDArray newB2) {
        this.W1 = newW1;
        this.b1 = newB1;
        this.W2 = newW2;
        this.b2 = newB2;
    }

    /**
     * Obtient le nombre total de paramètres.
     * 
     * @return Nombre total de paramètres
     */
    public long getNumberOfParameters() {
        return W1.length() + b1.length() + W2.length() + b2.length();
    }

    /**
     * Obtient le nombre total de gradients.
     * 
     * @return Nombre total de gradients
     */
    public long getNumberOfGradients() {
        return gradients.get("W1").length() + gradients.get("b1").length() +
               gradients.get("W2").length() + gradients.get("b2").length();
    }
}
package RN.transformer;


import java.io.Serializable;
import java.util.ArrayList;
import java.util.Collection;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.stream.Collectors;

import org.deeplearning4j.models.embeddings.wordvectors.WordVectors;
import org.nd4j.linalg.api.ndarray.INDArray;
import org.nd4j.linalg.factory.Nd4j;

public class Tokenizer implements Serializable {
    private static final long serialVersionUID = -1691008595018974489L;
    private Map<String, Integer> tokenToId;
    private Map<Integer, String> idToToken;
    private int vocabSize;
    private INDArray pretrainedEmbeddings;

    // Tokens spéciaux
    private static final String PAD_TOKEN = "<PAD>";
    private static final String UNK_TOKEN = "<UNK>";
    private static final String START_TOKEN = "<START>";
    private static final String END_TOKEN = "<END>";
    
    // Les embeddings pour les tokens spéciaux seront initialisés de manière spécifique
    private static final int EMBEDDING_SIZE = 300; // dModel

    public Tokenizer(WordVectors wordVectors) {
        this.tokenToId = new HashMap<>();
        this.idToToken = new HashMap<>();
        
        // Étape 1: Initialiser les tokens spéciaux en premier
        // Cela garantit que leurs IDs sont constants (0, 1, 2, 3)
        initializeSpecialTokens();
        
        // Étape 2: Ajouter tous les mots du vocabulaire Word2Vec
        Collection<String> words = wordVectors.vocab().words();
        for (String word : words) {
            if (!tokenToId.containsKey(word)) {
                addToken(word);
            }
        }
        
        this.vocabSize = tokenToId.size();
        
        // Étape 3: Créer la matrice d'embeddings avec les tokens spéciaux
        initializeEmbeddings(wordVectors);
    }
    
    public Tokenizer(List<String> words) {
        this.tokenToId = new HashMap<>();
        this.idToToken = new HashMap<>();
        
        // Étape 1: Initialiser les tokens spéciaux en premier
        // Cela garantit que leurs IDs sont constants (0, 1, 2, 3)
        initializeSpecialTokens();
        
        // Étape 2: Ajouter tous les mots du vocabulaire Word2Vec
        for (String word : words) {
            if (!tokenToId.containsKey(word)) {
                addToken(word);
            }
        }
        
        this.vocabSize = tokenToId.size();
        
        // Étape 3: Créer la matrice d'embeddings avec les tokens spéciaux
//        initializeEmbeddings(words);
    }

    private void initializeSpecialTokens() {
        // Les tokens spéciaux sont toujours ajoutés dans le même ordre
        addSpecialToken(PAD_TOKEN);    // ID = 0
        addSpecialToken(UNK_TOKEN);    // ID = 1
        addSpecialToken(START_TOKEN);  // ID = 2
        addSpecialToken(END_TOKEN);    // ID = 3
    }

    private void initializeEmbeddings(WordVectors wordVectors) {
        // Créer une nouvelle matrice d'embeddings avec la taille du vocabulaire complet
        pretrainedEmbeddings = Nd4j.zeros(vocabSize, EMBEDDING_SIZE);
        
        // Calculer le vecteur moyen pour l'initialisation des tokens spéciaux
        INDArray meanVector = calculateMeanVector(wordVectors);
        
        // Initialiser les embeddings des tokens spéciaux
        // PAD_TOKEN: vecteur de zéros (déjà initialisé par défaut)
        // UNK_TOKEN: vecteur moyen
        pretrainedEmbeddings.putRow(getUnkTokenId(), meanVector);
        // START_TOKEN: vecteur moyen + bruit gaussien
        pretrainedEmbeddings.putRow(getStartTokenId(), meanVector.add(Nd4j.randn(1, EMBEDDING_SIZE).mul(0.1)));
        // END_TOKEN: vecteur moyen + bruit gaussien
        pretrainedEmbeddings.putRow(getEndTokenId(), meanVector.add(Nd4j.randn(1, EMBEDDING_SIZE).mul(0.1)));
        
        // Copier les embeddings pour tous les autres tokens
        for (Map.Entry<String, Integer> entry : tokenToId.entrySet()) {
            String token = entry.getKey();
            int id = entry.getValue();
            
            // Ignorer les tokens spéciaux déjà traités
            if (isSpecialToken(token)) continue;
            
            if (wordVectors.hasWord(token)) {
                INDArray wordVector = wordVectors.getWordVectorMatrix(token);
                pretrainedEmbeddings.putRow(id, wordVector);
            } else {
                // Utiliser le vecteur moyen pour les mots inconnus
                pretrainedEmbeddings.putRow(id, meanVector);
            }
        }
    }

    private INDArray calculateMeanVector(WordVectors wordVectors) {
        INDArray sum = Nd4j.zeros(EMBEDDING_SIZE);
        int count = 0;
        
        Collection<String> words = wordVectors.vocab().words();
        for (String word : words) {
            sum.addi(wordVectors.getWordVectorMatrix(word));
            count++;
        }
        
        return sum.div(count);
    }

    private boolean isSpecialToken(String token) {
        return token.equals(PAD_TOKEN) || token.equals(UNK_TOKEN) || 
               token.equals(START_TOKEN) || token.equals(END_TOKEN);
    }

    private void addSpecialToken(String token) {
        int id = tokenToId.size();
        tokenToId.put(token, id);
        idToToken.put(id, token);
    }

    private void addToken(String token) {
        if (!tokenToId.containsKey(token)) {
            int id = tokenToId.size();
            tokenToId.put(token, id);
            idToToken.put(id, token);
        }
    }

    public List<String> tokenize(String text) {
        // Cette regex simple sépare les mots et la ponctuation, ce qui est une amélioration par rapport à la séparation par espace.
        // Pour des règles plus complexes, envisagez d'utiliser une librairie de tokenisation spécialisée.
        String[] tokens = text.split("\\s+|(?=\\p{Punct})|(?<=\\p{Punct})");
        List<String> tokenList = new ArrayList<>();
        for (String token : tokens) {
            if (!token.trim().isEmpty()) { // Ignorer les chaînes vides
                tokenList.add(token);
            }
        }
        return tokenList;
    }


    private boolean isPunctuation(String token) {
        // Une vérification simple de la ponctuation basée sur regex; ajustez selon vos besoins
        return token.matches("\\p{Punct}");
    }

    public List<Integer> tokensToIds(List<String> tokens) {
        return tokens.stream()
                .map(token -> tokenToId.getOrDefault(token, tokenToId.get(UNK_TOKEN)))
                .collect(Collectors.toList());
    }

    public String idsToTokens(List<Integer> ids) {
        return ids.stream()
                .map(id -> idToToken.getOrDefault(id, UNK_TOKEN))
                .collect(Collectors.joining(" "));
    }

    // Nouvelles méthodes pour gérer les tokens spéciaux
    public int getPadTokenId() {
        return tokenToId.get(PAD_TOKEN);
    }

    public int getUnkTokenId() {
        return tokenToId.get(UNK_TOKEN);
    }

    public int getStartTokenId() {
        return tokenToId.get(START_TOKEN);
    }

    public int getEndTokenId() {
        return tokenToId.get(END_TOKEN);
    }

    public int getVocabSize() {
        return vocabSize;
    }

    public String getToken(int id) {
        return idToToken.getOrDefault(id, UNK_TOKEN);
    }
    
    public INDArray getPretrainedEmbeddings() {
        return pretrainedEmbeddings;
    }
    
}



package RN.transformer;

import java.io.File;
import java.io.FileInputStream;
import java.io.FileOutputStream;
import java.io.IOException;
import java.io.ObjectInputStream;
import java.io.ObjectOutputStream;
import java.io.Serializable;
import java.util.List;

import org.deeplearning4j.models.embeddings.loader.WordVectorSerializer;
import org.nd4j.linalg.api.ndarray.INDArray;

public class Transformer implements Serializable {
	
    private static final long serialVersionUID = 1L;
    
    private Encoder encoder;
    private Decoder decoder;
    private CustomAdamOptimizer optimizer;
    // Autres attributs du Transformer...

    public Transformer(/* paramètres du constructeur */) {
        // Initialisation du Transformer...
    }

    // Méthodes existantes du Transformer...

    public void saveState(String filePath) throws IOException {
        try (ObjectOutputStream oos = new ObjectOutputStream(new FileOutputStream(filePath))) {
            // Sauvegarder l'état de l'encodeur et du décodeur
            oos.writeObject(encoder);
            oos.writeObject(decoder);
            
            // Sauvegarder l'état de l'optimiseur
            oos.writeObject(optimizer.getCurrentStep());
            oos.writeObject(optimizer.getEpoch());
            oos.writeObject(optimizer.getLearningRate());
            
            // Sauvegarder les paramètres du modèle
            List<INDArray> parameters = getParameters();
            oos.writeObject(parameters.size());
            for (INDArray param : parameters) {
                oos.writeObject(param);
            }
        }
    }

    public void loadState(String filePath) throws IOException, ClassNotFoundException {
        try (ObjectInputStream ois = new ObjectInputStream(new FileInputStream(filePath))) {
            // Charger l'état de l'encodeur et du décodeur
            this.encoder = (Encoder) ois.readObject();
            this.decoder = (Decoder) ois.readObject();
            
            // Charger l'état de l'optimiseur
            int currentStep = (int) ois.readObject();
            int epoch = (int) ois.readObject();
            double learningRate = (double) ois.readObject();
            optimizer.setCurrentStep(currentStep);
            optimizer.setEpoch(epoch);
            optimizer.setLearningRate(learningRate);
            
            // Charger les paramètres du modèle
            int numParams = (int) ois.readObject();
            List<INDArray> parameters = getParameters();
            for (int i = 0; i < numParams; i++) {
                INDArray param = (INDArray) ois.readObject();
                parameters.get(i).assign(param);
            }
        }
    }
    


    private List<INDArray> getParameters() {
        // Méthode pour obtenir tous les paramètres du modèle
        // Combinez les paramètres de l'encodeur et du décodeur
        List<INDArray> params = encoder.getParameters();
        params.addAll(decoder.getParameters());
        return params;
    }
}package RN.transformer;

import java.io.File;
import java.io.FileInputStream;
import java.io.FileOutputStream;
import java.io.IOException;
import java.io.ObjectInputStream;
import java.io.ObjectOutputStream;
import java.io.Serializable;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

import org.apache.commons.lang3.tuple.Pair;
import org.deeplearning4j.models.embeddings.loader.WordVectorSerializer;
import org.deeplearning4j.models.embeddings.wordvectors.WordVectors;
import org.nd4j.linalg.api.ndarray.INDArray;
import org.nd4j.linalg.factory.Nd4j;
import org.nd4j.linalg.indexing.NDArrayIndex;
import org.nd4j.linalg.ops.transforms.Transforms;

public class TransformerModel  implements Serializable {
	
    /**
	 * 
	 */
	private static final long serialVersionUID = -4799769434788429831L;
	
	private static String W2VECPATH = "pretrained-embeddings/mon_model_word2vec.txt";
	private boolean isTrained = false;
    public Encoder encoder;
    public Decoder decoder;
    public CustomAdamOptimizer optimizer;
    public Tokenizer tokenizer;
    private double dropoutRate = 0.1; // Exemple de taux de dropout fixe
    private transient static WordVectors wordVectors; // Chargé une fois, accessible statiquement
    private static int dModel = 300; // dmodel must be divisible by numHeads
    private static int numLayers = 6;
    private static int numHeads = 6; 
    private static int dff = 2048;
    private static INDArray pretrainedEmbeddings = null;
    private List<INDArray> combinedParameters = new ArrayList<>();
    private List<INDArray> combinedGradients = new ArrayList<INDArray>();
    
    static {
        try {
        	wordVectors = WordVectorSerializer.readWord2VecModel(new File(W2VECPATH), true);
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
    
    
    public TransformerModel(int numLayers, int dModel, int numHeads, int dff, double dropoutRate) {
    	this.numLayers = numLayers;
    	this.dModel = dModel;
    	this.numHeads = numHeads;
    	this.dff = dff;
    	this.dropoutRate = dropoutRate;
    	
        // Charger Word2Vec
        WordVectors wordVectors = WordVectorSerializer.readWord2VecModel(new File(W2VECPATH));
        
        // Créer le tokenizer qui gère maintenant aussi les embeddings
        this.tokenizer = new Tokenizer(wordVectors);
        
        // Utiliser les embeddings du tokenizer
        pretrainedEmbeddings = tokenizer.getPretrainedEmbeddings();
        
        this.encoder = new Encoder(numLayers, dModel, numHeads, dff, dropoutRate, this.tokenizer);
        this.decoder = new Decoder(numLayers, dModel, numHeads, dff, dropoutRate);
        
        // Calcul du nombre total de paramètres
        long totalParams = encoder.getNumberOfParameters() + decoder.getNumberOfParameters();
        
        this.optimizer = new CustomAdamOptimizer(0.001, dModel, 1000, totalParams); // Initialisation hypothétique
    }
    

    public TransformerModel() throws IOException {
        
        // Charger Word2Vec
        WordVectors wordVectors = WordVectorSerializer.readWord2VecModel(new File(W2VECPATH));
        
        // Créer le tokenizer qui gère maintenant aussi les embeddings
        this.tokenizer = new Tokenizer(wordVectors);
        
        // Utiliser les embeddings du tokenizer
        pretrainedEmbeddings = tokenizer.getPretrainedEmbeddings();
        
        this.encoder = new Encoder(numLayers, dModel, numHeads, dff, dropoutRate, this.tokenizer);
        this.decoder = new Decoder(numLayers, dModel, numHeads, dff, dropoutRate);
        
        // Calcul du nombre total de paramètres
        long totalParams = encoder.getNumberOfParameters() + decoder.getNumberOfParameters();
        
        this.optimizer = new CustomAdamOptimizer(0.001, dModel, 1000, totalParams); // Initialisation hypothétique
    }
    
 
    public void train(DataGenerator dataGenerator) throws IOException {
        for (int epoch = 0; epoch < 10; epoch++) {
            optimizer.setEpoch(epoch);

            while (dataGenerator.hasNextBatch()) {
            	
            	// at start because it keeps gradients loaded for the last loop and we can save the state on the disk if needed
            	cleanGradients();
            	
                Batch batch = dataGenerator.nextBatch();

                List<Integer> targetTokenIds = tokenizer.tokensToIds(tokenizer.tokenize(String.join("", batch.getTarget())));
                List<Integer> dataTokenIds = tokenizer.tokensToIds(tokenizer.tokenize(String.join("", batch.getData())));

                // Créer les masques
                INDArray encoderPaddingMask = createPaddingMask(dataTokenIds);
                INDArray decoderPaddingMask = createPaddingMask(targetTokenIds);
                INDArray lookAheadMask = createLookAheadMask(targetTokenIds.size());

                INDArray encoded = encoder.encode(true, dataTokenIds, encoderPaddingMask);
                INDArray decodedOutput = decoder.decode(true, encoded, encoded, lookAheadMask, decoderPaddingMask);

                List<INDArray> decodedLogits = new ArrayList<>();
                decodedLogits.add(decodedOutput);

                backpropagation(decodedLogits, targetTokenIds);
                addCombinedParameters();
                addCombinedGradients();
                optimizer.update(combinedParameters, combinedGradients);
            }

            dataGenerator.init();
        }

        isTrained = true;
    }
    
    public INDArray createLookAheadMask(int size) {
        // Création d'une matrice où les éléments au-dessus de la diagonale sont 1 (ce qui signifie masqués)
        INDArray mask = Nd4j.ones(size, size);
        INDArray lowerTriangle = Nd4j.tri(size, size, 0); // Crée une matrice triangulaire inférieure
        mask.subi(lowerTriangle); 
        // Appliquer dessous le masquage infini pour softmax
        for (int i = 0; i < size; i++) {
            for (int j = i + 1; j < size; j++) {
                mask.putScalar(i, j, Double.NEGATIVE_INFINITY);
            }
        }        
        return mask;
    }
    

    
    public INDArray createPaddingMask(List<Integer> tokenIds) {
        // Génération d'un masque où chaque emplacement de padding est marqué par 1 (infinité après le masquage)
        long size = tokenIds.size();
        INDArray mask = Nd4j.zeros(size);
        for (int i = 0; i < size; i++) {
            if (tokenIds.get(i) == tokenizer.getPadTokenId()) { 
                mask.putScalar(i, Double.POSITIVE_INFINITY);
            }
        }
        return mask;
    }
    



    
    private void backpropagation(List<INDArray> decodedLogits, List<Integer> targetTokenIds) {
        // Étape 1: Calcul de la perte et des gradients initiaux
        // Cette fonction est hypothétique et devrait retourner la perte et le gradient initial
        Pair<Float, INDArray> lossAndGradients = calculateCrossEntropyLossAndGradient(decodedLogits, targetTokenIds);
        float loss = lossAndGradients.getLeft();
        INDArray initialGradients = lossAndGradients.getRight();
        
        // Afficher la perte pour le monitoring
        System.out.println("Perte: " + loss);

        // Étape 2: Rétropropagation à travers le Décodeur
        // Cela ajuste les poids du décodeur basés sur les gradients calculés
        Map<String, INDArray> decoderGradients = decoder.backward(initialGradients);
        
        // Extraire les gradients pertinents pour l'encodeur à partir de decoderGradients
        Map<String, INDArray> encoderGradients = extractEncoderGradients(decoderGradients);
        

        // Étape 3: Rétropropagation à travers l'Encodeur
        // L'encodeur ajuste ses poids basé sur ses propres calculs de gradients
        // Dans un modèle Transformer, cela pourrait impliquer des gradients venant de la couche d'attention encodeur-décodeur
        // Pour simplifier, nous allons juste appeler backward sur l'encodeur sans passer de gradients spécifiques
        // car dans une implémentation réelle, cela dépendrait des détails spécifiques de votre modèle
        encoder.backward(encoderGradients);

        // Mettre à jour les poids basés sur les gradients calculés, normalement fait par l'optimiseur
        updateModelWeights();
    }


    private Map<String, INDArray> extractEncoderGradients(Map<String, INDArray> decoderGradients) {
        // Créez un nouveau Map pour contenir les gradients spécifiquement pour l'encoder.
        Map<String, INDArray> encoderGradients = new HashMap<>();
        
        // Extrayez les gradients par rapport aux entrées K et V de l'attention encoder-décodeur.
        // Ces gradients sont ceux qui doivent être propagés à travers l'encoder.
        INDArray gradK = decoderGradients.get("inputK");
        INDArray gradV = decoderGradients.get("inputV");
        
        // Ajoutez ces gradients au Map sous des clés représentant leur rôle dans l'encoder.
        // Par exemple, vous pouvez simplement les renommer pour correspondre à la nomenclature attendue par l'encoder.
        encoderGradients.put("gradK", gradK);
        encoderGradients.put("gradV", gradV);
        
        return encoderGradients;
    }




	private void updateModelWeights() {
        // Implémentez cette fonction pour mettre à jour les poids du modèle
        // basé sur les gradients calculés. Normalement, cela est géré par votre optimiseur
    }



	public void addCombinedParameters() {
        
        // Ajoute les paramètres de l'encoder
        combinedParameters.addAll(encoder.getParameters());
        
        // Ajoute les paramètres du decoder
        combinedParameters.addAll(decoder.getParameters());
        
    }

    private void addCombinedGradients() {
        
        // Ajoute les gradients de l'encoder
        combinedGradients.addAll(encoder.getGradients());
        
        // Ajoute les gradients du decoder
        combinedGradients.addAll(decoder.getGradients());
        
    }
    
    public void cleanGradients() {
    	combinedParameters.clear();
    	combinedGradients.clear();
    }




    public String infer(String prompt) {
        if (!isTrained) {
            throw new IllegalStateException("Le modèle doit être entraîné avant l'inférence.");
        }

        List<String> promptTokens = tokenizer.tokenize(prompt);
        List<Integer> promptTokenIds = tokenizer.tokensToIds(promptTokens);

        INDArray encoderPaddingMask = createPaddingMask(promptTokenIds);
        INDArray encodedPrompt = encoder.encode(false, promptTokenIds, encoderPaddingMask);

        List<Integer> outputIds = new ArrayList<>();
        int maxLength = 100; // Définissez une longueur maximale pour la sortie

        for (int i = 0; i < maxLength; i++) {
            List<Integer> currentOutput = new ArrayList<>(promptTokenIds);
            currentOutput.addAll(outputIds);

            INDArray decoderPaddingMask = createPaddingMask(currentOutput);
            INDArray lookAheadMask = createLookAheadMask(currentOutput.size());

            INDArray logits = decoder.decode(false, encodedPrompt, encodedPrompt, lookAheadMask, decoderPaddingMask);

            // Prendre le dernier token prédit
            INDArray lastTokenLogits = logits.get(NDArrayIndex.point(logits.rows() - 1), NDArrayIndex.all());
            int predictedTokenId = Nd4j.argMax(lastTokenLogits).getInt(0);

            outputIds.add(predictedTokenId);

            if (predictedTokenId == tokenizer.getEndTokenId()) {
                break;
            }
        }

        return tokenizer.idsToTokens(outputIds);
    }






    public boolean isTrained() {
        return isTrained;
    }


    
    protected Pair<Float, INDArray> calculateCrossEntropyLossAndGradient(List<INDArray> decodedLogits, List<Integer> targetTokenIds) {
        float loss = 0.0f;
        int N = targetTokenIds.size();
        

        // Assumons que decodedLogits contient une seule INDArray pour l'ensemble de la séquence
        INDArray logits = decodedLogits.get(0); // Obtenez les logits pour l'ensemble de la séquence
        INDArray gradients = Nd4j.zeros(logits.shape()); // Initialiser le gradient de la même forme que les logits

        System.out.println("logits.shape()"  + Arrays.toString(logits.shape()));
        
        for (int i = 0; i < N; i++) {
            int targetId = targetTokenIds.get(i); // L'ID attendu à la position i

            // Extraire les logits pour la position i et toutes les classes (vocabulaire)
            INDArray logitsForPosition = logits.getRow(i); // Assume une forme [vocabSize] pour chaque position
            
            // Utiliser Transforms pour le softmax sur les logits pour la position i
            INDArray softmaxLogits = Transforms.softmax(logitsForPosition, false); 
            
            // Calculer le log softmax spécifiquement pour l'indice de la cible
            float logSoftmaxForTarget = (float) Math.log(softmaxLogits.getDouble(targetId));
            
            // Accumuler la perte négative log softmax pour la cible
            loss += -logSoftmaxForTarget;

            // Calcul du gradient initial : p - y
            INDArray targetOneHot = Nd4j.zeros(logitsForPosition.shape());
            targetOneHot.putScalar(targetId, 1);
            INDArray gradForPosition = softmaxLogits.sub(targetOneHot);
            gradients.putRow(i, gradForPosition);
        }
        
        return Pair.of(loss / N, gradients); // Retourner la moyenne de la perte et les gradients accumulés
    }
    
    
    public void saveState(String filePath) throws IOException {
        try (ObjectOutputStream oos = new ObjectOutputStream(new FileOutputStream(filePath))) {
            // Sauvegarder l'état de l'encodeur et du décodeur
            oos.writeObject(encoder);
            oos.writeObject(decoder);
            
            // Sauvegarder l'état de l'optimiseur
            oos.writeObject(optimizer.getCurrentStep());
            oos.writeObject(optimizer.getEpoch());
            oos.writeObject(optimizer.getLearningRate());
            
            // Sauvegarder les paramètres du modèle
            oos.writeObject(combinedParameters.size());
            for (INDArray param : combinedParameters) {
                oos.writeObject(param);
            }
            
            // Sauvegarder l'état d'entraînement
            oos.writeBoolean(isTrained);
        }
    }

    public void loadState(String filePath) throws IOException, ClassNotFoundException {
        try (ObjectInputStream ois = new ObjectInputStream(new FileInputStream(filePath))) {
        	
        	this.readObject(ois);
        	
            // Charger l'état de l'encodeur et du décodeur
            this.encoder = (Encoder) ois.readObject();
            this.decoder = (Decoder) ois.readObject();
            
            // Charger l'état de l'optimiseur
            int currentStep = (int) ois.readObject();
            int epoch = (int) ois.readObject();
            double learningRate = (double) ois.readObject();
            optimizer.setCurrentStep(currentStep);
            optimizer.setEpoch(epoch);
            optimizer.setLearningRate(learningRate);
            
            // Charger les paramètres du modèle
            int numParams = (int) ois.readObject();
            for (int i = 0; i < numParams; i++) {
                INDArray param = (INDArray) ois.readObject();
                combinedParameters.get(i).assign(param);
            }
            
            // Charger l'état d'entraînement
            this.isTrained = ois.readBoolean();
        }
    }

    
    private void writeObject(ObjectOutputStream oos) throws IOException {
        oos.defaultWriteObject();
        // Vous pouvez sauvegarder le chemin du fichier Word2Vec si nécessaire
        oos.writeObject(W2VECPATH);
    }

    private void readObject(ObjectInputStream ois) throws IOException, ClassNotFoundException {
        ois.defaultReadObject();
        String word2vecPath = (String) ois.readObject();
        // Réinitialiser wordVectors
        this.wordVectors = WordVectorSerializer.loadStaticModel(new File(word2vecPath));
    }



	public void setTrained(boolean isTrained) {
		this.isTrained = isTrained;
	}



	public int getDModel() {
		return dModel;
	}



	public static int getVocabSize() {
		return pretrainedEmbeddings.rows();
	}


	public static INDArray getPretrainedEmbeddings() {
		return pretrainedEmbeddings;
	}


	public List<INDArray> getCombinedParameters() {
		return combinedParameters;
	}


	public List<INDArray> getCombinedGradients() {
		return combinedGradients;
	}


}
