package RN.transformer;

import java.util.List;

public class Batch {
    private List<String> data;
    private List<String> target;

    public Batch(List<String> data, List<String> target) {
        this.data = data;
        this.target = target;
    }

    public List<String> getData() {
        return data;
    }

    public List<String> getTarget() {
        return target;
    }
}
package RN.transformer;

import java.io.IOException;
import java.io.ObjectInputStream;
import java.io.ObjectOutputStream;
import java.io.Serializable;
import java.util.ArrayList;
import java.util.List;

import org.nd4j.linalg.api.buffer.DataType;
import org.nd4j.linalg.api.ndarray.INDArray;
import org.nd4j.linalg.factory.Nd4j;
import org.nd4j.linalg.ops.transforms.Transforms;

public class CustomAdamOptimizer implements Serializable {
    private static final long serialVersionUID = 3031098044411623634L;

    // Hyperparamètres d'Adam
    private float initialLr;
    private float beta1;
    private float beta2;
    private float epsilon;

    // Paramètres de scheduling
    private int warmupSteps;
    private int currentStep;
    private int epoch;
    private float learningRate;

    // États pour chaque paramètre
    private List<AdamState> states;

    // Classe interne pour stocker m et v
    private static class AdamState implements Serializable {
        private static final long serialVersionUID = 1L;
        public INDArray m;
        public INDArray v;

        public AdamState(long[] shape) {
            this.m = Nd4j.zeros(DataType.FLOAT, shape);
            this.v = Nd4j.zeros(DataType.FLOAT, shape);
        }
    }

    /**
     * Constructeur de l'optimiseur Adam.
     *
     * @param initialLr        Taux d'apprentissage initial.
     * @param dmodel           Dimension du modèle (dModel).
     * @param warmupSteps      Nombre de pas de warmup.
     * @param params           Liste des paramètres du modèle.
     */
    public CustomAdamOptimizer(float initialLr, int dmodel, int warmupSteps, List<INDArray> params) {
        this.initialLr = initialLr;
        this.beta1 = 0.9f;
        this.beta2 = 0.999f;
        this.epsilon = 1e-8f;
        this.warmupSteps = warmupSteps;
        this.currentStep = 0;
        this.epoch = 0;
        this.learningRate = initialLr;

        // Initialiser les états Adam pour chaque paramètre
        this.states = new ArrayList<>();
        for (INDArray param : params) {
            this.states.add(new AdamState(param.shape()));
        }
    }

    // Classe interne pour gérer la sérialisation des états Adam
    private void writeObject(ObjectOutputStream oos) throws IOException {
        oos.defaultWriteObject();
        // Sérialiser les états Adam
        oos.writeInt(states.size());
        for (AdamState state : states) {
            oos.writeObject(state.m);
            oos.writeObject(state.v);
        }
    }

    private void readObject(ObjectInputStream ois) throws IOException, ClassNotFoundException {
        ois.defaultReadObject();
        // Désérialiser les états Adam
        int size = ois.readInt();
        this.states = new ArrayList<>();
        for (int i = 0; i < size; i++) {
            INDArray m = (INDArray) ois.readObject();
            INDArray v = (INDArray) ois.readObject();
            AdamState state = new AdamState(m.shape());
            state.m = m;
            state.v = v;
            this.states.add(state);
        }
    }

    // Méthode pour mettre à jour les paramètres
    public void update(List<INDArray> params, List<INDArray> grads) {
        if (params.size() != grads.size()) {
            throw new IllegalArgumentException("La taille de la liste des paramètres et des gradients doit être la même.");
        }
    
        if (params.size() != states.size()) {
            throw new IllegalStateException("Le nombre de paramètres (" + params.size() + ") ne correspond pas au nombre d'états (" + states.size() + "). Assurez-vous que l'optimiseur est initialisé après avoir ajouté tous les paramètres.");
        }
    
        currentStep++;
        // System.out.println("Current Step: " + currentStep);
        // System.out.println("Learning Rate: " + learningRate);
    
        for (int i = 0; i < params.size(); i++) {
            INDArray param = params.get(i);
            INDArray grad = grads.get(i);
    
            AdamState state = states.get(i);
    
            // Afficher les valeurs avant la mise à jour
            // System.out.println("Param " + i + " before update: " + param);
            // System.out.println("Grad " + i + ": " + grad);
    
            // Mise à jour des moments en place
            state.m.muli(beta1).addi(grad.mul(1 - beta1));
            state.v.muli(beta2).addi(grad.mul(grad).mul(1 - beta2));
    
            // Correction de biais
            INDArray mHat = state.m.mul(1.0f / (1.0f - (float) Math.pow(beta1, currentStep)));
            INDArray vHat = state.v.mul(1.0f / (1.0f - (float) Math.pow(beta2, currentStep)));
    
            // Calcul de l'étape de mise à jour
            INDArray step = mHat.mul(learningRate).div(Transforms.sqrt(vHat).add(epsilon));
    
            // Afficher les valeurs intermédiaires
            // System.out.println("mHat " + i + ": " + mHat);
            // System.out.println("vHat " + i + ": " + vHat);
            // System.out.println("Step " + i + ": " + step);
    
            // Mise à jour du paramètre
            param.subi(step);
    
            // Afficher le paramètre après la mise à jour
            //  System.out.println("Param " + i + " after update: " + param);
        }
    
        // Mettre à jour le taux d'apprentissage si nécessaire
        calculateLearningRate();
    }
    
    
    

    // Calcul du taux d'apprentissage avec warmup et décroissance
    float calculateLearningRate() {
        float step = Math.max(1.0f, (float) currentStep);  // Commencer à 1 pour éviter la division par zéro

        // Learning rate de base
        float lr = initialLr;

        // Calculer le facteur de warmup (0 à 1 pendant la période de warmup)
        float warmupFactor = Math.min(1.0f, step / warmupSteps);

        // Calculer le facteur de décroissance (diminue après la période de warmup)
        float decayFactor;
        if (step <= warmupSteps) {
            decayFactor = 1.0f;
        } else {
            // Décroissance en racine carrée inverse après le warmup
            decayFactor = (float) Math.sqrt(warmupSteps / step);
        }

        // Calculer le learning rate final
        this.learningRate = lr * warmupFactor * decayFactor;

        // Appliquer des limites pour éviter des valeurs extrêmes
        this.learningRate = Math.max(this.learningRate, initialLr * 0.1f);   // Pas moins de 10% du lr initial
        this.learningRate = Math.min(this.learningRate, initialLr);         // Pas plus que le lr initial

        // Log des valeurs pour debugging
        if (currentStep % 100 == 0) {
            System.out.printf("Step: %d, Warmup: %.3f, Decay: %.3f, LR: %.6f%n",
                currentStep, warmupFactor, decayFactor, this.learningRate);
        }

        return this.learningRate;
    }

    // Getters et setters
    public void setCurrentStep(int step) {
        this.currentStep = step;
    }

    public int getCurrentStep() {
        return this.currentStep;
    }

    public void setEpoch(int epoch) {
        this.epoch = epoch;
    }

    public int getEpoch() {
        return this.epoch;
    }

    public float getLearningRate() {
        return learningRate;
    }

    public void setLearningRate(float learningRate) {
        this.learningRate = learningRate;
    }
}
package RN.transformer;

import java.io.BufferedReader;
import java.io.FileReader;
import java.io.IOException;
import java.util.ArrayList;
import java.util.List;

public class DataGenerator {
    private BufferedReader dataReader;
    private BufferedReader targetReader;
    protected Tokenizer tokenizer;
    private int batchSize;
    private int maxTokensPerBatch;
	private String targetFilePath;
	private String dataFilePath;

    public DataGenerator(String dataFilePath, String targetFilePath, Tokenizer tokenizer, int batchSize, int maxTokensPerBatch) throws IOException {
        this.dataReader = new BufferedReader(new FileReader(dataFilePath));
        this.targetReader = new BufferedReader(new FileReader(targetFilePath));
        this.targetFilePath = targetFilePath;
        this.dataFilePath = dataFilePath;
        this.tokenizer = tokenizer;
        this.batchSize = batchSize;
        this.maxTokensPerBatch = maxTokensPerBatch;
    }

    public boolean hasNextBatch() throws IOException {
        return dataReader.ready() && targetReader.ready();
    }

    // public Batch nextBatch() throws IOException {
    //     List<String> dataBatch = new ArrayList<>();
    //     List<String> targetBatch = new ArrayList<>();
    //     StringBuilder dataBuffer = new StringBuilder();
    //     StringBuilder targetBuffer = new StringBuilder();

    //     while (dataBatch.size() < batchSize && hasNextBatch()) {
    //         int dataChar;
    //         while ((dataChar = dataReader.read()) != -1) {
    //             dataBuffer.append((char) dataChar);
    //             if (dataBuffer.length() >= maxTokensPerBatch) break;
    //         }

    //         int targetChar;
    //         while ((targetChar = targetReader.read()) != -1) {
    //             targetBuffer.append((char) targetChar);
    //             if (targetBuffer.length() >= maxTokensPerBatch) break;
    //         }

    //         if (dataBuffer.length() > 0 && targetBuffer.length() > 0) {
    //             List<String> dataTokens = tokenizer.tokenize(dataBuffer.toString());
    //             List<String> targetTokens = tokenizer.tokenize(targetBuffer.toString());
    //             dataBatch.add(String.join(" ", dataTokens));
    //             targetBatch.add(String.join(" ", targetTokens));
    //             dataBuffer = new StringBuilder(); // Réinitialiser les buffers pour le prochain segment
    //             targetBuffer = new StringBuilder();
    //         }
    //     }

    //     return new Batch(dataBatch, targetBatch);
    // }

    
    public Batch nextBatch() throws IOException {
        List<String> dataBatch = new ArrayList<>();
        List<String> targetBatch = new ArrayList<>();
    
        for (int i = 0; i < batchSize; i++) {
            String dataLine = dataReader.readLine();
            String targetLine = targetReader.readLine();
    
            if (dataLine == null || targetLine == null) {
                break;
            }
    
            List<String> dataTokens = tokenizer.tokenize(dataLine);
            List<String> targetTokens = tokenizer.tokenize(targetLine);
            dataBatch.add(String.join(" ", dataTokens));
            targetBatch.add(String.join(" ", targetTokens));
        }
    
        return new Batch(dataBatch, targetBatch);
    }
    
    

    public void close() throws IOException {
        dataReader.close();
        targetReader.close();
    }

    public void init() throws IOException {
        this.dataReader = new BufferedReader(new FileReader(dataFilePath));
        this.targetReader = new BufferedReader(new FileReader(targetFilePath));
    }    
}
package RN.transformer;

import java.io.Serializable;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

import org.nd4j.linalg.api.ndarray.INDArray;
import org.nd4j.linalg.factory.Nd4j;

public class Decoder implements Serializable {
    private static final long serialVersionUID = 283129978055526337L;
    List<DecoderLayer> layers;
    private LayerNorm layerNorm;
    private int numLayers;
    protected int dModel;
    private int numHeads;
    private double dropoutRate;
    private LinearProjection linearProjection; // Projection linéaire vers la taille du vocabulaire

    // Cache pour stocker les entrées des couches pendant la passe forward
    private List<INDArray> forwardCache;
    private INDArray lastNormalizedInput; // Stocke l'entrée normalisée après LayerNorm


    public Decoder(int numLayers, int dModel, int numHeads, int dff, double dropoutRate) {
        this.numLayers = numLayers;
        this.dModel = dModel;
        this.numHeads = numHeads;
        this.dropoutRate = dropoutRate;
        this.layers = new ArrayList<>();
        this.layerNorm = new LayerNorm(dModel);
        this.linearProjection = new LinearProjection(dModel, TransformerModel.getVocabSize()); // Initialiser avec la taille du vocabulaire
        this.forwardCache = new ArrayList<>();

        for (int i = 0; i < numLayers; i++) {
            this.layers.add(new DecoderLayer(dModel, numHeads, dff, dropoutRate));
            this.forwardCache.add(null); // Initialiser le cache avec des valeurs nulles
        }
    }

    /**
     * Réinitialise le cache. Appelé avant une nouvelle passe forward.
     */
    public void resetCache() {
        for (int i = 0; i < forwardCache.size(); i++) {
            forwardCache.set(i, null);
        }
    }

    public INDArray decode(boolean isTraining, INDArray encoderOutput, INDArray encodedDecoderInput, INDArray lookAheadMask, INDArray paddingMask) {
        // Réinitialiser le cache avant une nouvelle passe forward
        resetCache();
    
        // Traitement par les couches de décodeur
        for (int i = 0; i < layers.size(); i++) {
            DecoderLayer layer = layers.get(i);
            encodedDecoderInput = layer.forward(isTraining, encodedDecoderInput, encoderOutput, lookAheadMask, paddingMask, forwardCache.get(i));
            forwardCache.set(i, encodedDecoderInput.dup()); // Stocker l'entrée actuelle dans le cache
        }
    
        // Normalisation finale
        encodedDecoderInput = layerNorm.forward(encodedDecoderInput);
        lastNormalizedInput = encodedDecoderInput.dup(); // Stocker l'entrée normalisée
    
        // Projection linéaire vers le vocabulaire
        INDArray logits = linearProjection.project(encodedDecoderInput); // [batchSize, targetSeqLength, vocabSize]
        // System.out.println("Logits shape: " + Arrays.toString(logits.shape()));
    
        return logits;
    }
    
    public Map<String, INDArray> backward(INDArray gradOutput) {
        // Récupérer l'entrée normalisée de la passe forward
        if (lastNormalizedInput == null) {
            throw new IllegalStateException("L'entrée normalisée n'est pas initialisée. Assurez-vous d'effectuer une passe forward avant.");
        }
    
        // Propager le gradient à travers LinearProjection
        Map<String, INDArray> gradLinearProjection = linearProjection.backward(lastNormalizedInput, gradOutput);
    
        // Propager le gradient à travers LayerNorm
        Map<String, INDArray> gradLayerNorm = layerNorm.backward(gradLinearProjection.get("input"));
    
        // Transformer gradLayerNorm en un Map pour correspondre à la signature de DecoderLayer.backward
        Map<String, INDArray> gradMap = new HashMap<>();
        gradMap.put("input", gradLayerNorm.get("input")); // Utiliser "input" comme clé est arbitraire mais doit correspondre à ce que s'attend à recevoir DecoderLayer.backward
    
        // Commencer avec le gradient à la sortie du Decoder
        for (int i = layers.size() - 1; i >= 0; i--) {
            DecoderLayer layer = layers.get(i);
            // Passer le cache correspondant à cette couche
            INDArray layerInput = forwardCache.get(i);
            gradMap = layer.backward(gradMap, layerInput);
        }
        // À ce stade, gradMap contiendrait le gradient à propager à l'Encoder
        // Vous pouvez ensuite extraire le gradient à passer à l'encodeur ou à d'autres parties du modèle si nécessaire.
    
        return gradMap;
    }
    

    // Méthode pour obtenir tous les paramètres du décodeur
    public List<INDArray> getParameters() {
        List<INDArray> params = new ArrayList<>();

        // Collecter les paramètres de chaque couche du décodeur
        for (DecoderLayer layer : layers) {
            params.addAll(layer.getParameters());
        }

        // Collecter les paramètres de la normalisation de couche finale
        params.addAll(layerNorm.getParameters());

        // Collecter les paramètres de la projection linéaire
        params.addAll(linearProjection.getParameters());

        return params;
    }

    // Méthode pour obtenir tous les gradients du décodeur
    public List<INDArray> getGradients() {
        List<INDArray> grads = new ArrayList<>();

        // Collecter les gradients de chaque couche du décodeur
        for (DecoderLayer layer : layers) {
            grads.addAll(layer.getGradients());
        }

        // Collecter les gradients de la normalisation de couche finale
        grads.addAll(layerNorm.getGradients());

        // Collecter les gradients de la projection linéaire
        grads.addAll(linearProjection.getGradients());

        return grads;
    }

    // Méthode pour calculer les gradients basés sur la perte
    public INDArray calculateGradients(double loss) {
        // La logique réelle de calcul des gradients serait beaucoup plus complexe
        // et dépendrait des détails spécifiques de votre implémentation et de votre bibliothèque d'autograd.
        INDArray gradients = Nd4j.rand(1, 100); // Assumer des dimensions hypothétiques pour l'exemple
        return gradients;
    }

    public int getNumberOfParameters() {
        int numParams = 0;

        // Parcourir toutes les couches de décodeur pour compter leurs paramètres
        for (DecoderLayer layer : layers) {
            numParams += layer.getNumberOfParameters();
        }

        // Ajouter les paramètres de la normalisation de couche et de la projection linéaire
        numParams += layerNorm.getNumberOfParameters();
        numParams += linearProjection.getNumberOfParameters();

        return numParams;
    }

    static class DecoderLayer implements Serializable {
        private static final long serialVersionUID = 4450374170745550258L;
        MultiHeadAttention selfAttention;
        MultiHeadAttention encoderDecoderAttention;
        PositionwiseFeedForward feedForward;
        LayerNorm layerNorm1;
        LayerNorm layerNorm2;
        LayerNorm layerNorm3;
        Dropout dropout1;
        Dropout dropout2;
        Dropout dropout3;

        public DecoderLayer(int dModel, int numHeads, int dff, double dropoutRate) {
            this.selfAttention = new MultiHeadAttention(dModel, numHeads);
            this.encoderDecoderAttention = new MultiHeadAttention(dModel, numHeads);
            this.feedForward = new PositionwiseFeedForward(dModel, dff);
            this.layerNorm1 = new LayerNorm(dModel);
            this.layerNorm2 = new LayerNorm(dModel);
            this.layerNorm3 = new LayerNorm(dModel);
            this.dropout1 = new Dropout(dropoutRate);
            this.dropout2 = new Dropout(dropoutRate);
            this.dropout3 = new Dropout(dropoutRate);
        }

        /**
         * Passe forward avec gestion du cache.
         *
         * @param isTraining       Indique si le modèle est en mode entraînement
         * @param x                Entrée actuelle
         * @param encoderOutput    Sortie de l'encodeur
         * @param lookAheadMask    Masque de look-ahead
         * @param paddingMask      Masque de padding
         * @param cachedInput      Entrée mise en cache de la passe forward précédente
         * @return Sortie après cette couche
         */
        public INDArray forward(boolean isTraining, INDArray x, INDArray encoderOutput, INDArray lookAheadMask, INDArray paddingMask, INDArray cachedInput) {
            
            // Vérification des formes
            if (x.rank() != 3 || encoderOutput.rank() != 3) {
                throw new IllegalArgumentException("Les entrées query, key et value doivent être de rang 3.");
            }
            
            INDArray attn1 = selfAttention.forward(x, x, x, lookAheadMask);
            attn1 = dropout1.apply(isTraining, attn1);
            x = layerNorm1.forward(x.add(attn1));

            INDArray attn2 = encoderDecoderAttention.forward(x, encoderOutput, encoderOutput, paddingMask);
            attn2 = dropout2.apply(isTraining, attn2);
            x = layerNorm2.forward(x.add(attn2));

            INDArray ffOutput = feedForward.forward(x);
            ffOutput = dropout3.apply(isTraining, ffOutput);
            return layerNorm3.forward(x.add(ffOutput));
        }

        /**
         * Passe backward avec utilisation du cache.
         *
         * @param gradOutput Gradients provenant de la couche suivante
         * @param cachedInput Entrée mise en cache de la passe forward précédente
         * @return Gradients à propager vers les couches précédentes
         */
        public Map<String, INDArray> backward(Map<String, INDArray> gradOutput, INDArray cachedInput) {
            
            Map<String, INDArray> gradients = new HashMap<>();

        	// Rétropropagation à travers LayerNorm3
            Map<String, INDArray> gradLayerNorm3 = layerNorm3.backward(gradOutput.get("input"));
            gradients.putAll(gradLayerNorm3);

            // Rétropropagation à travers PositionwiseFeedForward
            Map<String, INDArray> gradFeedForward = feedForward.backward(gradLayerNorm3.get("input"));
            gradients.putAll(gradFeedForward);

            // Rétropropagation à travers LayerNorm2
            Map<String, INDArray> gradLayerNorm2 = layerNorm2.backward(gradFeedForward.get("input"));
            gradients.putAll(gradLayerNorm2);

            // Rétropropagation à travers encoderDecoderAttention
            Map<String, INDArray> gradEncoderDecoderAttention = encoderDecoderAttention.backward(gradLayerNorm2.get("input"));
            gradients.putAll(gradEncoderDecoderAttention);

            // Rétropropagation à travers LayerNorm1
            Map<String, INDArray> gradLayerNorm1 = layerNorm1.backward(gradEncoderDecoderAttention.get("input"));
            gradients.putAll(gradLayerNorm1);

            // Rétropropagation à travers selfAttention
            Map<String, INDArray> gradSelfAttention = selfAttention.backward(gradLayerNorm1.get("input"));
            gradients.putAll(gradSelfAttention);

            // Retourner les gradients accumulés pour mise à jour des paramètres
            return gradients;
        }

        public List<INDArray> getParameters() {
            List<INDArray> layerParams = new ArrayList<>();

            layerParams.addAll(selfAttention.getParameters());
            layerParams.addAll(encoderDecoderAttention.getParameters());
            layerParams.addAll(feedForward.getParameters());
            layerParams.addAll(layerNorm1.getParameters());
            layerParams.addAll(layerNorm2.getParameters());
            layerParams.addAll(layerNorm3.getParameters());

            return layerParams;
        }

        public List<INDArray> getGradients() {
            List<INDArray> layerGrads = new ArrayList<>();

            layerGrads.addAll(selfAttention.getGradients());
            layerGrads.addAll(encoderDecoderAttention.getGradients());
            layerGrads.addAll(feedForward.getGradients());
            layerGrads.addAll(layerNorm1.getGradients());
            layerGrads.addAll(layerNorm2.getGradients());
            layerGrads.addAll(layerNorm3.getGradients());

            return layerGrads;
        }

        public long getNumberOfParameters() {
            return selfAttention.getNumberOfParameters() +
                   encoderDecoderAttention.getNumberOfParameters() +
                   feedForward.getNumberOfParameters() +
                   layerNorm1.getNumberOfParameters() +
                   layerNorm2.getNumberOfParameters() +
                   layerNorm3.getNumberOfParameters();
        }

        public long getNumberOfGradients() {
            return selfAttention.getNumberOfGradients() +
                   encoderDecoderAttention.getNumberOfGradients() +
                   feedForward.getNumberOfGradients() +
                   layerNorm1.getNumberOfGradients() +
                   layerNorm2.getNumberOfGradients() +
                   layerNorm3.getNumberOfGradients();
        }
    }
}
package RN.transformer;

import java.io.Serializable;

import org.nd4j.linalg.api.ndarray.INDArray;
import org.nd4j.linalg.factory.Nd4j;

public class Dropout implements Serializable {
    /**
	 * 
	 */
	private static final long serialVersionUID = -61325399079678110L;
	private double rate;
    private INDArray mask;


    public Dropout(double rate) {
        this.rate = rate;
    }

    
    public INDArray apply(boolean isTraining, INDArray input) {
        if(isTraining) {
            // Création du masque d'activation basée sur le taux de dropout
            this.mask = Nd4j.rand(input.shape()).gt(rate);
            // Application du masque aux données d'entrée
            return input.mul(mask);
        } else {
            // Pendant l'inférence, dropout n'est pas appliqué mais les activations sont ajustées
            return input.mul(1.0 - rate);
        }
    }
    
    public INDArray backward(INDArray gradOutput) {
        // Pendant la rétropropagation, simplement passer le gradient à travers le masque
        return gradOutput.mul(mask);
    }    
    
}
package RN.transformer;


import java.io.Serializable;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

import org.nd4j.linalg.api.buffer.DataType;
import org.nd4j.linalg.api.ndarray.INDArray;
import org.nd4j.linalg.factory.Nd4j;
import org.nd4j.linalg.indexing.NDArrayIndex;


/**
 * 	numLayers: Le nombre de couches répétitives dans l'encodeur.
 *	dModel: La dimensionnalité des embeddings de tokens et des sorties de toutes les couches dans le modèle.
 *	numHeads: Le nombre de têtes d'attention dans les mécanismes d'attention multi-têtes.
 *	dff: La dimensionnalité des couches feed-forward internes dans chaque couche d'encodeur.
 *	vocabSize: La taille du vocabulaire, nécessaire pour les embeddings de tokens.
 *	maxSeqLength: La longueur maximale de séquence, utilisée pour les embeddings positionnels.
 */

public class Encoder implements Serializable  {
	
    /**
	 * 
	 */
	private static final long serialVersionUID = -5716799542280937448L;
	private List<EncoderLayer> layers;
    private int dModel;
    private PositionalEncoding positionalEncoding;
    private LayerNorm layerNorm;
    private Tokenizer tokenizer;

    public Encoder() {
	}
    
    public Encoder(int numLayers, int dModel, int numHeads, int dff, double dropoutRate, Tokenizer tokenizer) {
    	this.dModel = dModel;
        this.positionalEncoding = new PositionalEncoding(dModel);
        this.layers = new ArrayList<>();
        this.layerNorm = new LayerNorm(dModel);
        this.tokenizer = tokenizer;
        
        for (int i = 0; i < numLayers; i++) {
            this.layers.add(new EncoderLayer(dModel, numHeads, dff, dropoutRate));
        }
    }



	public List<List<Float>> encode(boolean isTraining, String text) {
        // Tokenization du texte
        List<String> tokens = tokenizer.tokenize(text);
        // Conversion des tokens en IDs
        List<Integer> tokenIds = tokenizer.tokensToIds(tokens);

        // Encodage des IDs de tokens à travers les couches de l'encodeur
        INDArray inputEmbeddings = lookupEmbeddings(Arrays.asList(tokenIds));
        INDArray encoded = forward(isTraining, inputEmbeddings, null);
        
        // Conversion des embeddings encodés en logits
        return convertToLogits(encoded);
    }
    
    public INDArray encode(boolean isTraining, List<List<Integer>> tokenIdsBatch, INDArray paddingMask) {
        int batchSize = tokenIdsBatch.size();
        int seqLength = tokenIdsBatch.get(0).size();
        INDArray inputEmbeddings = Nd4j.zeros(DataType.FLOAT, batchSize, seqLength, dModel);

        for (int i = 0; i < batchSize; i++) {
            for (int j = 0; j < seqLength; j++) {
                int tokenId = tokenIdsBatch.get(i).get(j);
                INDArray tokenEmbedding = TransformerModel.getPretrainedEmbeddings().getRow(tokenId);
                // Utilisez slice pour accéder à la position correcte et assign pour copier les valeurs
                inputEmbeddings.get(NDArrayIndex.point(i), NDArrayIndex.point(j), NDArrayIndex.all())
                            .assign(tokenEmbedding);
            }
        }
        // System.out.println("Embedded input shape: " + Arrays.toString(inputEmbeddings.shape()));

        // Appliquer le MultiHeadAttention et les couches de l'encodeur
        INDArray encoded = forward(isTraining, inputEmbeddings, paddingMask);
        return encoded;
    }
    
    
    public INDArray lookupEmbeddings(List<List<Integer>> tokenIdsBatch) {
        int batchSize = tokenIdsBatch.size();
        int seqLength = tokenIdsBatch.get(0).size();
    
        // Étape 1: Aplatir la liste de listes en un tableau 1D d'entiers (int[])
        int[] flattenedTokenIds = new int[batchSize * seqLength];
        for (int i = 0; i < batchSize; i++) {
            List<Integer> tokenIds = tokenIdsBatch.get(i);
            for (int j = 0; j < seqLength; j++) {
                flattenedTokenIds[i * seqLength + j] = tokenIds.get(j); // Pas besoin de convertir en long
            }
        }
    
        // Étape 2: Récupérer les embeddings correspondants
        // TransformerModel.getPretrainedEmbeddings() doit être de forme [vocabSize, dModel]
        INDArray batchEmbeddings = TransformerModel.getPretrainedEmbeddings().getRows(flattenedTokenIds); // [batchSize * seqLength, dModel]
    
        // Étape 3: Reshaper en [batchSize, seqLength, dModel]
        batchEmbeddings = batchEmbeddings.reshape(batchSize, seqLength, dModel); // Utilisation correcte de dModel
    
        return batchEmbeddings;
    }
    
    
    
    

    private List<List<Float>> convertToLogits(INDArray encoded) {
        // Convertir les embeddings encodés en logits
        List<List<Float>> logits = new ArrayList<>();
        int seqLength = (int) encoded.size(0);
        for (int i = 0; i < seqLength; i++) {
            INDArray row = encoded.getRow(i);
            List<Float> rowList = new ArrayList<>();
            for (int j = 0; j < row.length(); j++) {
                rowList.add(row.getFloat(j)); // Ajouter la valeur de l'élément à la liste des logits
            }
            logits.add(rowList);
        }
        return logits;
    }

    public INDArray forward(boolean isTraining, INDArray x, INDArray paddingMask) {
        
        // Appliquer les embeddings positionnels
        INDArray posEncoding = positionalEncoding.getPositionalEncoding(x.shape()[1]);
        x = x.add(posEncoding);
        // System.out.println("After positional encoding: " + Arrays.toString(x.shape()));
    
        for (EncoderLayer layer : layers) {
            x = layer.forward(isTraining, x, paddingMask);
            // System.out.println("After encoder layer: " + Arrays.toString(x.shape()));
        }
        
        x = layerNorm.forward(x);
        // System.out.println("After layer normalization: " + Arrays.toString(x.shape()));
    
        return x;
    }
    
    

    public void backward(Map<String, INDArray> gradOutput) {
    	
        // Récupérer gradAttentionOutputConcat
        INDArray gradAttentionOutputConcatND = gradOutput.get("gradAttentionOutputConcat"); // [1,6,6,50]

        // Permute les axes pour [batchSize, seqLength, numHeads, depth]
        INDArray gradPermuted = gradAttentionOutputConcatND.permute(0, 2, 1, 3); // [1,6,6,50]
    
        // Reshaper pour concaténer les têtes : [batchSize, seqLength, numHeads * depth]
        INDArray gradOutputForLayerNorm = gradPermuted.reshape(1, 6, 6 * 50); // [1,6,300]
    
        // Backpropagation à travers la normalisation de couche finale
    	Map<String, INDArray> gradientsFromLayerNorm = layerNorm.backward(gradOutputForLayerNorm);

        // Récupération du gradient par rapport aux entrées de LayerNorm qui sera utilisé comme gradient initial pour les couches de l'Encoder
        INDArray gradInput = gradientsFromLayerNorm.get("input");

        // Propagation des gradients à travers chaque couche d'Encoder en ordre inverse
        for (int i = layers.size() - 1; i >= 0; i--) {
            // Chaque couche retourne le gradient par rapport à ses entrées qui est passé à la couche précédente
            gradInput = layers.get(i).backward(gradInput);
            if (gradInput == null) {
                throw new IllegalArgumentException("gradInput est null après backward de la couche " + i);
            }
        }

        // Mettre à jour ou enregistrer les gradients pour gamma et beta si nécessaire
        // Par exemple, si ces paramètres sont appris :
        // updateGammaBeta(gradFromLayerNorm.get("gamma"), gradFromLayerNorm.get("beta"));
    }

    // Méthode hypothétique pour mettre à jour ou enregistrer les gradients de gamma et beta
    private void updateGammaBeta(INDArray gradGamma, INDArray gradBeta) {
        // Mettre à jour ou enregistrer les gradients de gamma et beta
        // Ceci pourrait inclure l'application d'un taux d'apprentissage ou l'enregistrement pour une utilisation dans un pas d'optimisation
    }




    
    // Méthode pour calculer les gradients basés sur la perte
    public INDArray calculateGradients(double loss) {
        // Dans un cas réel, cette méthode impliquerait le calcul du gradient de la perte par rapport à chaque paramètre
        // Pour cet exemple, simuler un gradient comme un INDArray de mêmes dimensions que les paramètres
        INDArray gradients = Nd4j.rand(1, 100); // Assumer les mêmes dimensions hypothétiques que les paramètres
        return gradients;
    }
    
    
    public List<INDArray> getParameters() {
        List<INDArray> params = new ArrayList<>();
        // Collecter les poids et biais de multiHeadAttention et positionwiseFeedForward
        for (EncoderLayer layer : layers) {
            params.addAll(layer.getParameters());
        }

        // Inclure les paramètres de la normalisation de couche finale
        if(layerNorm != null) {
            params.addAll(layerNorm.getParameters());
        }
        
        return params;
    }
    
    public List<INDArray> getGradients() {
        List<INDArray> grads = new ArrayList<>();
        // Collecter les poids et biais de multiHeadAttention et positionwiseFeedForward
        for (EncoderLayer layer : layers) {
        	grads.addAll(layer.getGradients());
        }

        // Inclure les gradients de la normalisation de couche finale
        if(layerNorm != null) {
        	grads.addAll(layerNorm.getGradients());
        }
        
        return grads;
    }
    

    
    public int getNumberOfParameters() {
        int numParams = 0;

        // Parcourir toutes les couches d'encodeur pour compter leurs paramètres
        for (EncoderLayer layer : layers) {
            numParams += layer.getNumberOfParameters();
        }

        // Ajouter les paramètres de la normalisation de couche et des embeddings positionnels
        numParams += layerNorm.getNumberOfParameters();

        return numParams;
    }
    
    
    public int getNumberOfGradients() {
        int numGrads = 0;

        // Parcourir toutes les couches d'encodeur pour compter leurs gradients
        for (EncoderLayer layer : layers) {
        	numGrads += layer.getNumberOfGradients();
        }

        // Ajouter les gradients de la normalisation de couche et des embeddings positionnels
        numGrads += layerNorm.getNumberOfGradients();

        return numGrads;
    }



    static class EncoderLayer implements Serializable {
    	
        /**
		 * 
		 */
		private static final long serialVersionUID = -88886021425567141L;
		
		MultiHeadAttention selfAttention;
        PositionwiseFeedForward feedForward;
        LayerNorm layerNorm1;
        LayerNorm layerNorm2;
        Dropout dropout1;
        Dropout dropout2;

        public EncoderLayer(int dModel, int numHeads, int dff, double dropoutRate) {
        	
            this.selfAttention = new MultiHeadAttention(dModel, numHeads);
            this.feedForward = new PositionwiseFeedForward(dModel, dff);
            this.layerNorm1 = new LayerNorm(dModel);
            this.layerNorm2 = new LayerNorm(dModel);
            this.dropout1 = new Dropout(dropoutRate);
            this.dropout2 = new Dropout(dropoutRate);
        }
        
        public INDArray forward(boolean isTraining, INDArray x, INDArray paddingMask) {
        	
            INDArray attnOutput = selfAttention.forward(x, x, x, paddingMask);
            attnOutput = dropout1.apply(isTraining, attnOutput);
            x = layerNorm1.forward(x.add(attnOutput)); // Add & norm

            INDArray ffOutput = feedForward.forward(x);
            ffOutput = dropout2.apply(isTraining, ffOutput);
            return layerNorm2.forward(x.add(ffOutput)); // Add & norm again
        }
        
        public INDArray backward(INDArray gradOutput) {
        	
            // Backward à travers la deuxième normalisation de couche
            Map<String, INDArray> gradLayerNorm2 = layerNorm2.backward(gradOutput);
            INDArray gradToFeedForward = gradLayerNorm2.get("input");

            // Backward à travers la couche PositionwiseFeedForward
            Map<String, INDArray> gradFeedForward = feedForward.backward(gradToFeedForward);
            INDArray gradToLayerNorm1 = gradFeedForward.get("input");

            // Backward à travers la première normalisation de couche
            Map<String, INDArray> gradLayerNorm1 = layerNorm1.backward(gradToLayerNorm1);
            INDArray gradToSelfAttention = gradLayerNorm1.get("input");

            // Backward à travers SelfAttention
            Map<String, INDArray> gradSelfAttention = selfAttention.backward(gradToSelfAttention);


            return gradSelfAttention.get("input");
        }


        
        public List<INDArray> getParameters() {
            List<INDArray> layerParams = new ArrayList<>();
            
            // Collecter les paramètres des composants de la couche d'encodeur
            layerParams.addAll(selfAttention.getParameters());
            layerParams.addAll(feedForward.getParameters());
            layerParams.addAll(layerNorm1.getParameters());
            layerParams.addAll(layerNorm2.getParameters());

            return layerParams;
        }
        
        public List<INDArray> getGradients() {
            List<INDArray> layerGrads = new ArrayList<>();
            
            // Collecter les paramètres des composants de la couche d'encodeur
            layerGrads.addAll(selfAttention.getGradients());
            layerGrads.addAll(feedForward.getGradients());
            layerGrads.addAll(layerNorm1.getGradients());
            layerGrads.addAll(layerNorm2.getGradients());

            return layerGrads;
        }
        

        public long getNumberOfParameters() {
            return selfAttention.getNumberOfParameters() +
                   feedForward.getNumberOfParameters() +
                   layerNorm1.getNumberOfParameters() +
                   layerNorm2.getNumberOfParameters();
        }
        
        

        public long getNumberOfGradients() {
            return selfAttention.getNumberOfGradients() +
                   feedForward.getNumberOfGradients() +
                   layerNorm1.getNumberOfGradients() +
                   layerNorm2.getNumberOfGradients();
        }



    }
    
    
    
}
package RN.transformer;

import java.io.Serializable;
import java.util.Map;

import org.nd4j.linalg.api.ndarray.INDArray;

class Layer implements Serializable {
    /**
	 * 
	 */
	private static final long serialVersionUID = 6248679804921861953L;
	// Paramètres de la couche (poids, biais, etc.)
    INDArray weights;
    INDArray bias;

    // Méthode pour calculer la passe en avant
    public INDArray forward(INDArray input) {
        // Implémentation spécifique de la couche
        return null;
    }

    // Méthode pour calculer la rétropropagation
    public Map<String, INDArray> backward(INDArray incomingGradient) {
        // Calculer les gradients par rapport aux paramètres de la couche
        // et propager le gradient de la perte à la couche précédente
        return null;
    }
}
package RN.transformer;

import java.io.Serializable;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

import org.nd4j.linalg.api.buffer.DataType;
import org.nd4j.linalg.api.ndarray.INDArray;
import org.nd4j.linalg.factory.Nd4j;
import org.nd4j.linalg.ops.transforms.Transforms;

import RN.utils.NDArrayUtils;

/**
 * Classe représentant une normalisation de couche (LayerNorm).
 * Les tenseurs peuvent avoir n'importe quel rang >= 2, normalisés sur la dernière dimension (dModel).
 */
public class LayerNorm extends Layer implements Serializable {
    private static final long serialVersionUID = 941772045774041840L;
    private INDArray gamma, beta;
    private int dModel;
    private final double epsilon = 1e-6;
    private INDArray inputCache; // Cache pour le forward
    private Map<String, INDArray> gradients = new HashMap<>();

    /**
     * Constructeur de la classe LayerNorm.
     * 
     * @param dModel Dimension du modèle (dModel)
     */
    public LayerNorm(int dModel) {
        this.dModel = dModel;
        // Initialisation de gamma à des uns et de beta à des zéros avec la forme [1, 1, dModel]
        gamma = Nd4j.ones(DataType.FLOAT, 1, 1, dModel); // [1, 1, dModel]
        beta = Nd4j.zeros(DataType.FLOAT, 1, 1, dModel); // [1, 1, dModel]

        // Log des formes pour vérification
        // System.out.println("Initialized gamma shape: " + Arrays.toString(gamma.shape()));
        // System.out.println("Initialized beta shape: " + Arrays.toString(beta.shape()));
    }

    /**
     * Passe forward de la normalisation de couche.
     * 
     * @param x Entrée de forme [batchSize, seqLength, dModel] ou plus
     * @return Sortie normalisée de même forme
     */
    @Override
    public INDArray forward(INDArray x) {
        if (x.isNaN().any() || x.isInfinite().any()) {
            throw new RuntimeException("LayerNorm.forward received NaN or Infinite values in input.");
        }

        this.inputCache = x.dup();

        // Calcul de la moyenne et de la variance sur la dernière dimension (dModel)
        INDArray mean = x.mean(true, x.rank() - 1);     // [batchSize, seqLength, 1]
        INDArray variance = x.var(false, x.rank() - 1);  // [batchSize, seqLength, 1]

        // Ajout de epsilon et calcul de l'écart-type
        INDArray std = Transforms.sqrt(variance.add(epsilon)).reshape(x.shape()[0], x.shape()[1], 1); // [batchSize, seqLength, 1]

        if (mean.isNaN().any() || mean.isInfinite().any()) {
            throw new RuntimeException("NaN or Infinite values encountered in mean calculation.");
        }
        if (std.isNaN().any() || std.isInfinite().any()) {
            throw new RuntimeException("NaN or Infinite values encountered in standard deviation calculation.");
        }

        // Normalisation avec broadcast explicite
        INDArray normalized = x.sub(mean).div(std); // [batchSize, seqLength, dModel] / [batchSize, seqLength, 1]

        // Reshape gamma et beta pour le broadcasting correct
        INDArray gammaBroadcast = gamma.reshape(1, 1, dModel);  // [1, 1, dModel]
        INDArray betaBroadcast = beta.reshape(1, 1, dModel);    // [1, 1, dModel]

        // Mise à l'échelle et décalage
        INDArray output = normalized.mul(gammaBroadcast).add(betaBroadcast); // [batchSize, seqLength, dModel]

        if (output.isNaN().any() || output.isInfinite().any()) {
            throw new RuntimeException("NaN or Infinite values produced by LayerNorm normalization.");
        }

        return output;
    }

    /**
     * Passe backward de la normalisation de couche.
     * 
     * @param gradOutput Gradient provenant de la couche suivante de même forme que l'entrée
     * @return Map contenant les gradients pour les paramètres 'gamma', 'beta' et 'input'
     */
    @Override
    public Map<String, INDArray> backward(INDArray gradOutput) {

        if (gradOutput == null) {
            throw new IllegalArgumentException("gradOutput ne peut pas être null lors de la rétropropagation dans LayerNorm.");
        }

        // Vérifications de base
        if (gradOutput.shape()[gradOutput.rank() - 1] != dModel) {
            throw new IllegalStateException("La dernière dimension de gradOutput doit être égale à dModel.");
        }

        // Récupération des valeurs du forward
        INDArray input = this.inputCache; // [batchSize, seqLength, dModel]
        long batchSize = input.size(0);
        long seqLength = input.size(1);
        // Note: On suppose que les dimensions intermédiaires sont maintenues

        // Recalcul de la moyenne et de la variance comme dans le forward
        INDArray mean = input.mean(true, input.rank() - 1); // [batchSize, seqLength, 1]
        INDArray variance = input.var(false, input.rank() - 1).reshape(batchSize, seqLength, 1); // [batchSize, seqLength, 1]

        INDArray stdInv = Transforms.pow(variance.add(epsilon), -0.5); // [batchSize, seqLength, 1]

        INDArray normalized = input.sub(mean).mul(stdInv); // [batchSize, seqLength, dModel]

        // Calcul des gradients pour gamma et beta
        INDArray gradGamma = gradOutput.mul(normalized).sum(new int[]{0, 1}); // [1, 1, dModel]
        INDArray gradBeta = gradOutput.sum(new int[]{0, 1}); // [1, 1, dModel]

        // Calcul du gradient par rapport à l'entrée
        INDArray gradNormalized = gradOutput.mul(gamma); // [batchSize, seqLength, dModel]

        INDArray gradVariance = gradNormalized.mul(normalized).mul(-0.5).div(Transforms.pow(variance.add(epsilon), 1.5)).sum(2).reshape(batchSize, seqLength, 1); // [batchSize, seqLength, 1]
        INDArray gradMean = gradNormalized.mul(-1).div(Transforms.sqrt(variance.add(epsilon))).sum(2).reshape(batchSize, seqLength, 1)
                            .add( gradVariance.mul(normalized.mul(-2)).sum(2).reshape(batchSize, seqLength, 1)); // [batchSize, seqLength, 1]

        INDArray gradInput = gradNormalized.div(Transforms.sqrt(variance.add(epsilon)))
                            .add( gradVariance.mul(normalized.mul(2)).div(dModel))
                            .add( gradMean.div(dModel)); // [batchSize, seqLength, dModel]

        // Stockage des gradients
        NDArrayUtils.addGradient(gradients,"gamma", gradGamma);
        NDArrayUtils.addGradient(gradients,"beta", gradBeta);
        NDArrayUtils.addGradient(gradients,"input", gradInput);// Gradient à propager vers les couches précédentes

        return gradients;
    }

    /**
     * Obtient les gradients des paramètres.
     * 
     * @return Liste des gradients dans l'ordre [gamma, beta]
     */
    public List<INDArray> getGradients() {
        List<INDArray> list = new ArrayList<>();
        list.add(gradients.get("gamma"));
        list.add(gradients.get("beta"));
        if (list.contains(null)) {
            throw new IllegalArgumentException(" gradients contains null ");
        }
        return list;  
    }

    /**
     * Obtient les paramètres de la normalisation de couche.
     * 
     * @return Liste des paramètres dans l'ordre [gamma, beta]
     */
    public List<INDArray> getParameters() {
        return Arrays.asList(gamma, beta);
    }

    /**
     * Définit (met à jour) les paramètres de la normalisation de couche.
     * 
     * @param newGamma Nouvelles valeurs pour gamma
     * @param newBeta  Nouvelles valeurs pour beta
     */
    public void setParameters(INDArray newGamma, INDArray newBeta) {
        // Vérifier que les nouvelles formes sont correctes
        if (!Arrays.equals(newGamma.shape(), gamma.shape())) {
            throw new IllegalArgumentException("newGamma a une forme incorrecte: " + Arrays.toString(newGamma.shape()));
        }
        if (!Arrays.equals(newBeta.shape(), beta.shape())) {
            throw new IllegalArgumentException("newBeta a une forme incorrecte: " + Arrays.toString(newBeta.shape()));
        }
        this.gamma = newGamma;
        this.beta = newBeta;
    }

    /**
     * Obtient le nombre total de paramètres.
     * 
     * @return Nombre total de paramètres
     */
    public long getNumberOfParameters() {
        return gamma.length() + beta.length();
    }

    /**
     * Obtient le nombre total de gradients.
     * 
     * @return Nombre total de gradients
     */
    public long getNumberOfGradients() {
        return gradients.get("gamma").length() + gradients.get("beta").length();
    }
}
package RN.transformer;

import java.io.Serializable;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

import org.nd4j.linalg.api.ndarray.INDArray;
import org.nd4j.linalg.factory.Nd4j;
import org.nd4j.linalg.ops.transforms.Transforms;

import RN.utils.NDArrayUtils;

import org.nd4j.linalg.api.buffer.DataType;

/**
 * Classe représentant une projection linéaire avec normalisation de couche (LayerNorm).
 * Les tenseurs sont supposés avoir la forme [seqLength, dModel].
 */
public class LinearProjection implements Serializable {
    
    private static final long serialVersionUID = -6601830517666118676L;
    private INDArray weights; // Poids de la projection linéaire [dModel, outputSize]
    private INDArray bias;    // Biais de la projection linéaire [1, outputSize]
    private INDArray gamma;   // Paramètre de scale pour LayerNorm [1, dModel]
    private INDArray beta;    // Paramètre de shift pour LayerNorm [1, dModel]
    private final double epsilon = 1e-7; // Petite constante pour éviter la division par zéro
    
    // Gradients calculés lors de la passe backward
    private Map<String, INDArray> gradients = new HashMap<>();
    
    /**
     * Constructeur de la classe LinearProjection.
     * 
     * @param inputSize  Taille de l'entrée (dModel)
     * @param outputSize Taille de la sortie (par exemple, la taille du vocabulaire)
     */
    public LinearProjection(int inputSize, int outputSize) {
        // Initialisation des poids avec une distribution normale divisée par sqrt(inputSize) pour l'initialisation de He
        this.weights = Nd4j.randn(DataType.FLOAT, inputSize, outputSize).div(Math.sqrt(inputSize));
        // Initialisation des biais à zéro
        this.bias = Nd4j.zeros(DataType.FLOAT,1, outputSize);
        // Initialisation de gamma à un et beta à zéro pour la normalisation de couche
        this.gamma = Nd4j.ones(DataType.FLOAT,1, inputSize); // [1, dModel]
        this.beta = Nd4j.zeros(DataType.FLOAT,1, inputSize); // [1, dModel]
    }

    /**
     * Effectue la projection linéaire.
     * 
     * @param input Entrée de forme [seqLength, dModel]
     * @return Sortie projetée de forme [seqLength, outputSize]
     */
    public INDArray project(INDArray input) {
        if (input.rank() == 3) {
            int batchSize = (int) input.size(0);
            int seqLength = (int) input.size(1);
            int inputDim = (int) input.size(2);

            // Reshaper en [batchSize * seqLength, inputDim]
            INDArray reshapedInput = input.reshape(batchSize * seqLength, inputDim);

            // Multiplication matricielle et ajout du biais
            INDArray projected = reshapedInput.mmul(weights).addiRowVector(bias);

            // Reshaper de retour en [batchSize, seqLength, outputDim]
            return projected.reshape(batchSize, seqLength, weights.size(1));
        } else if (input.rank() == 2) {
            // Multiplication matricielle et ajout du biais
            return input.mmul(weights).addiRowVector(bias);
        } else {
            throw new IllegalArgumentException("Input must be rank 2 or 3.");
        }
    }

    /**
     * Passe forward avec normalisation de couche et projection linéaire.
     * 
     * @param input Entrée de forme [seqLength, dModel]
     * @return Sortie projetée de forme [seqLength, outputSize]
     */
    public INDArray forward(INDArray input) {
        // Calcul de la moyenne et de la variance sur la dimension dModel (axis=1)
        INDArray mean = input.mean(1).reshape(input.rows(), 1); // [seqLength, 1]
        INDArray variance = input.var(false, 1).reshape(input.rows(), 1); // [seqLength, 1]
        INDArray stdInv = Transforms.pow(variance.add(epsilon), -0.5); // [seqLength, 1]
        
        // Normalisation: (input - mean) / sqrt(variance + epsilon)
        INDArray normalized = input.sub(mean).mul(stdInv); // [seqLength, dModel]
        
        // Mise à l'échelle et décalage: normalized * gamma + beta
        INDArray scaled = normalized.mul(gamma).add(beta); // [seqLength, dModel]
        
        // Projection linéaire
        INDArray output = scaled.mmul(weights).addRowVector(bias); // [seqLength, outputSize]
        return output;
    }


    /**
     * Passe backward pour calculer les gradients.
     * 
     * @param input      Entrée originale utilisée dans la passe forward de forme [seqLength, dModel]
     * @param gradOutput Gradient provenant de la couche suivante de forme [seqLength, outputSize]
     * @return Map contenant les gradients pour les paramètres 'weights', 'bias', 'gamma' et 'beta', ainsi que 'input'
     */
    public Map<String, INDArray> backward(INDArray input, INDArray gradOutput) {
        // Vérifier les formes
        if (input.rank() != 3 && input.rank() != 2) {
            throw new IllegalArgumentException("Input must be rank 2 or 3.");
        }
        if (input.size(input.rank() - 1) != weights.size(0)) { // dModel
            throw new IllegalArgumentException("Input size mismatch. Expected " + weights.size(0) + ", got " + input.size(input.rank() - 1));
        }
    
        // Si l'input est de rang 3, reshaper pour le traitement
        boolean reshaped = false;
        int batchSize = 1;
        int seqLength = 1;
        if (input.rank() == 3) {
            batchSize = (int) input.size(0);
            seqLength = (int) input.size(1);
            input = input.reshape(batchSize * seqLength, input.size(2)); // [batchSize * seqLength, dModel]
            gradOutput = gradOutput.reshape(batchSize * seqLength, gradOutput.size(2)); // [batchSize * seqLength, vocabSize]
            reshaped = true;
        }
    
        // Calcul des moyennes et variances
        INDArray mean = input.mean(1).reshape(input.size(0), 1); // [batchSize * seqLength, 1]
        INDArray variance = input.var(false, 1).reshape(input.size(0), 1); // [batchSize * seqLength, 1]
        INDArray stdInv = Transforms.pow(variance.add(epsilon), -0.5); // [batchSize * seqLength, 1]
    
        // Calcul de normalized = (input - mean) / sqrt(var + epsilon)
        INDArray normalized = input.sub(mean).mul(stdInv); // [batchSize * seqLength, dModel]
    
        // Gradients pour la projection linéaire
        INDArray gradScaled = gradOutput.mmul(weights.transpose()); // [batchSize * seqLength, dModel]
    
        // Gradients pour gamma et beta de LayerNorm
        INDArray gradGamma = normalized.mul(gradScaled).sum(0).reshape(1, input.size(1)); // [1, dModel]
        INDArray gradBeta = gradScaled.sum(0).reshape(1, input.size(1)); // [1, dModel]
    
        // Gradients pour la normalisation
        INDArray gradNormalized = gradScaled.mul(gamma); // [batchSize * seqLength, dModel]
        
        // Calcul des gradients pour l'entrée
        INDArray sumGradNormInput = gradNormalized.mul(normalized).sum(1).reshape(input.size(0), 1); // [batchSize * seqLength, 1]
        INDArray gradInput = gradNormalized.mul(stdInv).sub(normalized.mul(sumGradNormInput).mul(stdInv)); // [batchSize * seqLength, dModel]
    
        // Gradients pour les poids et les biais
        INDArray gradWeights = input.transpose().mmul(gradOutput); // [dModel, vocabSize]
        INDArray gradBias = gradOutput.sum(0).reshape(1, gradOutput.size(1)); // [1, vocabSize]
    
        // Si reshaped, remettre les formes d'origine
        if (reshaped) {
            gradInput = gradInput.reshape(batchSize, seqLength, input.size(1)); // [batchSize, seqLength, dModel]
        }
    
        // Stockage des gradients dans la map
        NDArrayUtils.addGradient(gradients, "weights", gradWeights);
        NDArrayUtils.addGradient(gradients, "bias", gradBias);
        NDArrayUtils.addGradient(gradients, "gamma", gradGamma);
        NDArrayUtils.addGradient(gradients, "beta", gradBeta);
        NDArrayUtils.addGradient(gradients, "input", gradInput); // Gradient à propager vers les couches précédentes
    
        return gradients;
    }
     
    


    /**
     * Obtient les gradients des paramètres.
     * 
     * @return Liste des gradients dans l'ordre [weights, bias, gamma, beta]
     */
    public List<INDArray> getGradients() {
        List<INDArray> list = new ArrayList<>();
        list.add(gradients.get("weights"));
        list.add(gradients.get("bias"));
        list.add(gradients.get("gamma"));
        list.add(gradients.get("beta"));
        if (list.contains(null)) {
            throw new IllegalArgumentException(" gradients contains null ");
        }
        return list;    
    }

    /**
     * Obtient les paramètres de la projection linéaire.
     * 
     * @return Liste des paramètres dans l'ordre [weights, bias, gamma, beta]
     */
    public List<INDArray> getParameters() {
        return Arrays.asList(weights, bias, gamma, beta);
    }

    /**
     * Définit (met à jour) les paramètres de la projection linéaire.
     * 
     * @param newWeights Nouvelles valeurs pour les poids
     * @param newBias    Nouvelles valeurs pour les biais
     * @param newGamma   Nouvelles valeurs pour gamma
     * @param newBeta    Nouvelles valeurs pour beta
     */
    public void setParameters(INDArray newWeights, INDArray newBias, INDArray newGamma, INDArray newBeta) {
        this.weights = newWeights;
        this.bias = newBias;
        this.gamma = newGamma;
        this.beta = newBeta;
    }

    /**
     * Obtient le nombre total de paramètres.
     * 
     * @return Nombre total de paramètres
     */
    public long getNumberOfParameters() {
        return weights.length() + bias.length() + gamma.length() + beta.length();
    }

    /**
     * Obtient le nombre total de gradients.
     * 
     * @return Nombre total de gradients
     */
    public long getNumberOfGradients() {
        return gradients.get("weights").length() + gradients.get("bias").length() + gradients.get("gamma").length() + gradients.get("beta").length();
    }
}
package RN.transformer;

import java.io.Serializable;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

import org.nd4j.linalg.api.buffer.DataType;
import org.nd4j.linalg.api.ndarray.INDArray;
import org.nd4j.linalg.factory.Nd4j;
import org.nd4j.linalg.indexing.INDArrayIndex;
import org.nd4j.linalg.indexing.NDArrayIndex;
import org.nd4j.linalg.ops.transforms.Transforms;

import RN.utils.NDArrayUtils;

public class MultiHeadAttention implements Serializable {

    private static final long serialVersionUID = -7153801764592720027L;
    private int dModel;
    private int numHeads;
    private int depth;
    private INDArray inputQ, inputK, inputV; // Cached inputs for backward
    private INDArray Q, K, V; // Intermediate projections
    private INDArray Wq, Wk, Wv, Wo; // Weights for queries, keys, values, and output
    private INDArray attentionWeights; // Cached attention weights for backward
    private INDArray attentionOutput; // [batchSize * seqLength, numHeads * depth]
    private Map<String, INDArray> gradients = new HashMap<>();

    public MultiHeadAttention(int dModel, int numHeads) {

        this.dModel = dModel;
        this.numHeads = numHeads;
        this.depth = dModel / numHeads;

        if (dModel != numHeads * depth) {
            throw new IllegalArgumentException("dModel doit être égal à numHeads * depth. Actuellement, dModel="
                    + dModel + ", numHeads=" + numHeads + ", depth=" + depth);
        }

        // Initialize weights with appropriate normalization
        Wq = Nd4j.randn(DataType.FLOAT, dModel, numHeads * depth).div(Math.sqrt(dModel));
        Wk = Nd4j.randn(DataType.FLOAT, dModel, numHeads * depth).div(Math.sqrt(dModel));
        Wv = Nd4j.randn(DataType.FLOAT, dModel, numHeads * depth).div(Math.sqrt(dModel));
        Wo = Nd4j.randn(DataType.FLOAT, numHeads * depth, dModel).div(Math.sqrt(numHeads * depth));
    }

    /**
     * Forward pass of multi-head attention.
     *
     * @param query Input queries of shape [batchSize, seqLength_q, dModel]
     * @param key   Input keys of shape [batchSize, seqLength_k, dModel]
     * @param value Input values of shape [batchSize, seqLength_k, dModel]
     * @param mask  Padding mask of shape [batchSize, 1, 1, seqLength_k]
     * @return Output of shape [batchSize, seqLength_q, dModel]
     */
    public INDArray forward(INDArray query, INDArray key, INDArray value, INDArray mask) {

        // Stocker les inputs pour la passe backward
        this.inputQ = query.dup(); // [batchSize, seqLength_q, dModel]
        this.inputK = key.dup();   // [batchSize, seqLength_k, dModel]
        this.inputV = value.dup(); // [batchSize, seqLength_k, dModel]

        // Obtention des dimensions
        int batchSize = (int) query.size(0);
        int seqLength_q = (int) query.size(1);
        int seqLength_k = (int) key.size(1);
        int depth = dModel / numHeads;

        // Vérification des dimensions des entrées
        if (query.rank() != 3 || key.rank() != 3 || value.rank() != 3) {
            throw new IllegalArgumentException("Les entrées query, key et value doivent être de rang 3.");
        }
       // System.out.println("Key shape: " + Arrays.toString(key.shape()));
        // System.out.println("Batch Size: " + batchSize);
        // System.out.println("Seq Length Q: " + seqLength_q);
        // System.out.println("Seq Length K: " + seqLength_k);
        // System.out.println("dModel: " + dModel);
 

        // Reshaping des entrées de [batchSize, seqLength, dModel] à [batchSize * seqLength, dModel]
        INDArray query2D = query.reshape(batchSize * seqLength_q, dModel);
        INDArray key2D = key.reshape(batchSize * seqLength_k, dModel); // Utiliser seqLength_k
        INDArray value2D = value.reshape(batchSize * seqLength_k, dModel); // Utiliser seqLength_k

        // System.out.println("Query2D shape: " + Arrays.toString(query2D.shape()));
        // System.out.println("Key2D shape: " + Arrays.toString(key2D.shape()));
        // System.out.println("Value2D shape: " + Arrays.toString(value2D.shape()));

        // Application des transformations linéaires
        this.Q = query2D.mmul(Wq); // [batchSize * seqLength_q, numHeads * depth]
        this.K = key2D.mmul(Wk);    // [batchSize * seqLength_k, numHeads * depth]
        this.V = value2D.mmul(Wv);  // [batchSize * seqLength_k, numHeads * depth]

        // System.out.println("Q shape after mmul: " + Arrays.toString(Q.shape()));
        // System.out.println("K shape after mmul: " + Arrays.toString(K.shape()));
        // System.out.println("V shape after mmul: " + Arrays.toString(V.shape()));

        // Reshaping pour multi-head attention
        this.Q = this.Q.reshape(batchSize, seqLength_q, numHeads, depth); // [batchSize, seqLength_q, numHeads, depth]
        this.K = this.K.reshape(batchSize, seqLength_k, numHeads, depth); // [batchSize, seqLength_k, numHeads, depth]
        this.V = this.V.reshape(batchSize, seqLength_k, numHeads, depth); // [batchSize, seqLength_k, numHeads, depth]

        // System.out.println("Q shape after reshape: " + Arrays.toString(Q.shape()));
        // System.out.println("K shape after reshape: " + Arrays.toString(K.shape()));
        // System.out.println("V shape after reshape: " + Arrays.toString(V.shape()));

        // Transpose pour [batchSize, numHeads, seqLength, depth]
        this.Q = this.Q.permute(0, 2, 1, 3); // [batchSize, numHeads, seqLength_q, depth]
        this.K = this.K.permute(0, 2, 1, 3); // [batchSize, numHeads, seqLength_k, depth]
        this.V = this.V.permute(0, 2, 1, 3); // [batchSize, numHeads, seqLength_k, depth]

        // System.out.println("Q shape after permute: " + Arrays.toString(Q.shape()));
        // System.out.println("K shape after permute: " + Arrays.toString(K.shape()));
        // System.out.println("V shape after permute: " + Arrays.toString(V.shape()));

        // Initialiser un tableau pour stocker les scores
        INDArray scores = Nd4j.create(batchSize, numHeads, seqLength_q, seqLength_k); // [batchSize, numHeads, seqLength_q, seqLength_k]

        // Itérer sur batchSize et numHeads pour effectuer mmul
        for (int b = 0; b < batchSize; b++) {
            for (int h = 0; h < numHeads; h++) {
                // Extraire les matrices [seqLength_q, depth] et [depth, seqLength_k]
                INDArray Q_batch_head = Q.get(NDArrayIndex.point(b), NDArrayIndex.point(h), NDArrayIndex.all(), NDArrayIndex.all());
                INDArray KTransposed_batch_head = K
                        .get(NDArrayIndex.point(b), NDArrayIndex.point(h), NDArrayIndex.all(), NDArrayIndex.all())
                        .transpose();

                // Calculer les scores [seqLength_q, seqLength_k]
                INDArray score = Q_batch_head.mmul(KTransposed_batch_head).div(Math.sqrt(depth));

                // Stocker les scores en utilisant assign
                scores.get(NDArrayIndex.point(b), NDArrayIndex.point(h), NDArrayIndex.all(), NDArrayIndex.all())
                        .assign(score);
            }
        }

        // System.out.println("Scores shape after computation: " + Arrays.toString(scores.shape()));

        // Appliquer le masque si fourni
        if (mask != null) {
            // Assurez-vous que le masque a les mêmes dimensions que scores
            // Généralement, mask a la forme [batchSize, 1, 1, seqLength_k]
            scores = scores.add(mask.mul(-1e9)); // Ajouter un grand nombre négatif aux positions masquées
        }

        // System.out.println("Scores shape after masking: " + Arrays.toString(scores.shape()));

        // Application du softmax sur le dernier axe (axis=3)
        INDArray weights = NDArrayUtils.softmax(scores, 3); // [batchSize, numHeads, seqLength_q, seqLength_k]

        // System.out.println("Weights shape after softmax: " + Arrays.toString(weights.shape()));

        // **Stocker attentionWeights pour la passe backward**
        this.attentionWeights = weights;

        // Calcul de l'attention pondérée
        INDArray attention = Nd4j.create(batchSize, numHeads, seqLength_q, depth); // [batchSize, numHeads, seqLength_q, depth]

        for (int b = 0; b < batchSize; b++) {
            for (int h = 0; h < numHeads; h++) {
                // Extraire les matrices [seqLength_q, seqLength_k] et [seqLength_k, depth]
                INDArray weights_batch_head = weights.get(NDArrayIndex.point(b), NDArrayIndex.point(h),
                        NDArrayIndex.all(), NDArrayIndex.all());
                INDArray V_batch_head = V.get(NDArrayIndex.point(b), NDArrayIndex.point(h), NDArrayIndex.all(),
                        NDArrayIndex.all());

                // Calculer l'attention pondérée [seqLength_q, depth]
                INDArray attention_head = weights_batch_head.mmul(V_batch_head);

                // Stocker l'attention en utilisant assign
                attention.get(NDArrayIndex.point(b), NDArrayIndex.point(h), NDArrayIndex.all(), NDArrayIndex.all())
                        .assign(attention_head);
            }
        }

        // System.out.println("Attention shape after computation: " + Arrays.toString(attention.shape()));

        // Transposition et reshaping pour concaténer les têtes
        // [batchSize, numHeads, seqLength_q, depth] -> [batchSize, seqLength_q, numHeads * depth]
        INDArray attentionConcat = attention.permute(0, 2, 1, 3).reshape(batchSize, seqLength_q, numHeads * depth); // [batchSize, seqLength_q, dModel]

        // System.out.println("AttentionConcat shape: " + Arrays.toString(attentionConcat.shape()));

        // Effectuer la multiplication matricielle avec Wo
        // Reshape attentionConcat de [batchSize, seqLength_q, dModel] à [batchSize * seqLength_q, dModel]
        INDArray attention2D = attentionConcat.reshape(batchSize * seqLength_q, dModel); // [batchSize * seqLength_q, dModel]
        INDArray output2D = attention2D.mmul(Wo); // [batchSize * seqLength_q, dModel]

        // System.out.println("Output2D shape: " + Arrays.toString(output2D.shape()));

        // Reshape le résultat en [batchSize, seqLength_q, dModel]
        INDArray output = output2D.reshape(batchSize, seqLength_q, dModel); // [batchSize, seqLength_q, dModel]

        // System.out.println("Output shape: " + Arrays.toString(output.shape()));

        // **Stocker l'attentionOutput pour la passe backward**
        this.attentionOutput = attentionConcat; // ou `output` selon ce qui est nécessaire pour backward

        return output;
    }


    /**
     * Backward pass of multi-head attention.
     *
     * @param gradOutput Gradient of the loss with respect to the output [batchSize, seqLength, dModel]
     * @return Map of gradients with respect to parameters and inputs
     */
    public Map<String, INDArray> backward(INDArray gradOutput) {
        // Vérifications d'état
        if (this.attentionOutput == null || this.Q == null || this.K == null || this.V == null) {
            throw new IllegalStateException("Les variables nécessaires (attentionOutput, Q, K, V) ne sont pas initialisées. Assurez-vous d'avoir effectué une passe forward avant backward.");
        }

        if (this.inputQ == null || this.inputK == null || this.inputV == null) {
            throw new IllegalStateException("Les inputs (inputQ, inputK, inputV) sont null. Assurez-vous que la passe forward les a correctement initialisés.");
        }

        if (this.attentionWeights == null) {
            throw new IllegalStateException("attentionWeights est null. Assurez-vous que la passe forward a correctement initialisé attentionWeights.");
        }

        // Dimensions
        int batchSize = (int) gradOutput.shape()[0];
        int seqLength = (int) gradOutput.shape()[1];
        int numHeads = this.numHeads;
        int depth = this.depth;
        int dModel = this.dModel; // Assurez-vous que dModel = numHeads * depth

        // Logs de dimensions
        // System.out.println("Backward Pass:");
        // System.out.println("batchSize: " + batchSize);
        // System.out.println("seqLength: " + seqLength);
        // System.out.println("numHeads: " + numHeads);
        // System.out.println("depth: " + depth);
        // System.out.println("dModel: " + dModel);
        // System.out.println("gradOutput shape: " + gradOutput.shapeInfoToString());

        // Step 1: Compute gradients for Wo
        // attentionOutputConcat has shape [batchSize, seqLength, numHeads * depth]
        INDArray attentionOutputConcat = this.attentionOutput.permute(0, 2, 1) // [batchSize, numHeads * depth, seqLength]
                                            .reshape(batchSize * seqLength, numHeads * depth); // [batchSize * seqLength, numHeads * depth]

        // Reshape gradOutput to [batchSize * seqLength, dModel]
        INDArray gradOutputReshaped = gradOutput.reshape(batchSize * seqLength, dModel); // [batchSize * seqLength, dModel]

        // Compute gradWo: [numHeads * depth, dModel]
        INDArray gradWo = attentionOutputConcat.transpose().mmul(gradOutputReshaped); // [numHeads * depth, dModel]
        NDArrayUtils.addGradient(gradients, "Wo", gradWo);
        
        // System.out.println("gradWo shape: " + gradWo.shapeInfoToString());

        // Step 2: Compute gradients for attentionOutputConcat
        // gradAttentionOutputConcat = gradOutputReshaped * Wo^T
        INDArray WoTransposed = this.Wo.transpose(); // [dModel, numHeads * depth]
        INDArray gradAttentionOutputConcatReshaped = gradOutputReshaped.mmul(WoTransposed); // [batchSize * seqLength, numHeads * depth]
        // Reshape back to [batchSize, seqLength, numHeads, depth]
        INDArray gradAttentionOutputConcatND = gradAttentionOutputConcatReshaped.reshape(batchSize, seqLength, numHeads, depth);
        // Permute to [batchSize, numHeads, seqLength, depth]
        gradAttentionOutputConcatND = gradAttentionOutputConcatND.permute(0, 2, 1, 3); // [batchSize, numHeads, seqLength, depth]
        NDArrayUtils.addGradient(gradients, "gradAttentionOutputConcat", gradAttentionOutputConcatND);
        // System.out.println("gradAttentionOutputConcat shape: " + gradAttentionOutputConcatND.shapeInfoToString());

        // Step 3: Compute gradients for attentionWeights and V
        // attentionOutput = attentionWeights.mmul(V)
        // gradAttentionWeights = gradAttentionOutputConcat.mmul(V^T)
        // gradV = attentionWeights^T.mmul(gradAttentionOutputConcat)
        INDArray gradAttentionWeights = Nd4j.create(batchSize, numHeads, seqLength, seqLength); // [batchSize, numHeads, seqLength, seqLength]
        for (int b = 0; b < batchSize; b++) {
            for (int h = 0; h < numHeads; h++) {
                // Extract [seqLength, depth] from gradAttentionOutputConcatND
                INDArray gradAttentionOutputHead = gradAttentionOutputConcatND.get(
                    NDArrayIndex.point(b),
                    NDArrayIndex.point(h),
                    NDArrayIndex.all(),
                    NDArrayIndex.all()
                ); // [seqLength, depth]

                // Extract [depth, seqLength] from V
                INDArray VTransposedHead = this.V.get(
                    NDArrayIndex.point(b),
                    NDArrayIndex.point(h),
                    NDArrayIndex.all(),
                    NDArrayIndex.all()
                ).transpose(); // [depth, seqLength]

                // Perform mmul: [seqLength, depth] mmul [depth, seqLength] = [seqLength, seqLength]
                INDArray gradAttentionWeightsHead = gradAttentionOutputHead.mmul(VTransposedHead).div(Math.sqrt(depth)); // [seqLength, seqLength]

                // Assign to gradAttentionWeights
                gradAttentionWeights.get(
                    NDArrayIndex.point(b),
                    NDArrayIndex.point(h),
                    NDArrayIndex.all(),
                    NDArrayIndex.all()
                ).assign(gradAttentionWeightsHead);
            }
        }
        NDArrayUtils.addGradient(gradients,"gradAttentionWeights", gradAttentionWeights);
        // System.out.println("gradAttentionWeights shape: " + gradAttentionWeights.shapeInfoToString());

        // Compute gradV
        INDArray gradV = Nd4j.create(batchSize, numHeads, seqLength, depth); // [batchSize, numHeads, seqLength, depth]
        for (int b = 0; b < batchSize; b++) {
            for (int h = 0; h < numHeads; h++) {
                // Extract [seqLength, seqLength] from attentionWeights^T
                INDArray attentionWeightsTransposedHead = this.attentionWeights.get(
                    NDArrayIndex.point(b),
                    NDArrayIndex.point(h),
                    NDArrayIndex.all(),
                    NDArrayIndex.all()
                ).transpose(); // [seqLength, seqLength]

                // Extract [seqLength, depth] from gradAttentionOutputConcatND
                INDArray gradAttentionOutputHead = gradAttentionOutputConcatND.get(
                    NDArrayIndex.point(b),
                    NDArrayIndex.point(h),
                    NDArrayIndex.all(),
                    NDArrayIndex.all()
                ); // [seqLength, depth]

                // Perform mmul: [seqLength, seqLength] mmul [seqLength, depth] = [seqLength, depth]
                INDArray gradVHead = attentionWeightsTransposedHead.mmul(gradAttentionOutputHead).div(Math.sqrt(depth)); // [seqLength, depth]

                // Assign to gradV
                gradV.get(
                    NDArrayIndex.point(b),
                    NDArrayIndex.point(h),
                    NDArrayIndex.all(),
                    NDArrayIndex.all()
                ).assign(gradVHead); // [seqLength, depth]
                
            }
        }
        //NDArrayUtils.addGradient(gradients,"gradV", gradV);
        // System.out.println("gradV shape: " + gradV.shapeInfoToString());

        // Step 4: Compute gradients through softmax
        // gradScores = softmaxGrad(attentionWeights, gradAttentionWeights)
        INDArray gradScores = softmaxGrad(this.attentionWeights, gradAttentionWeights); // [batchSize, numHeads, seqLength, seqLength]
        NDArrayUtils.addGradient(gradients,"gradScores", gradScores);
        // System.out.println("gradScores shape: " + gradScores.shapeInfoToString());

        // Step 5: Compute gradients for Q and K
        // scores = Q.mmul(K.transpose(0, 1, 3, 2)) / sqrt(depth)
        // gradQ = gradScores.mmul(K) / sqrt(depth)
        // gradK = gradScores^T.mmul(Q) / sqrt(depth)
        INDArray gradQ = Nd4j.create(batchSize, numHeads, seqLength, depth); // [batchSize, numHeads, seqLength, depth]
        for (int b = 0; b < batchSize; b++) {
            for (int h = 0; h < numHeads; h++) {
                // Extract [seqLength, seqLength] from gradScores
                INDArray gradScoresHead = gradScores.get(
                    NDArrayIndex.point(b),
                    NDArrayIndex.point(h),
                    NDArrayIndex.all(),
                    NDArrayIndex.all()
                ); // [seqLength, seqLength]

                // Extract [seqLength, depth] from K
                INDArray KHead = this.K.get(
                    NDArrayIndex.point(b),
                    NDArrayIndex.point(h),
                    NDArrayIndex.all(),
                    NDArrayIndex.all()
                ); // [seqLength, depth]

                // Perform mmul: [seqLength, seqLength] mmul [seqLength, depth] = [seqLength, depth]
                INDArray gradQHead = gradScoresHead.mmul(KHead); // [seqLength, depth]

                // Assign to gradQ
                gradQ.get(
                    NDArrayIndex.point(b),
                    NDArrayIndex.point(h),
                    NDArrayIndex.all(),
                    NDArrayIndex.all()
                ).assign(gradQHead);
            }
        }
        //NDArrayUtils.addGradient(gradients,"gradQ", gradQ);
        // System.out.println("gradQ shape: " + gradQ.shapeInfoToString());

        // Compute gradK
        INDArray gradK = Nd4j.create(batchSize, numHeads, seqLength, depth); // [batchSize, numHeads, seqLength, depth]
        for (int b = 0; b < batchSize; b++) {
            for (int h = 0; h < numHeads; h++) {
                // Extract [seqLength, seqLength] from gradScores transpose
                INDArray gradScoresTransposedHead = gradScores.get(
                    NDArrayIndex.point(b),
                    NDArrayIndex.point(h),
                    NDArrayIndex.all(),
                    NDArrayIndex.all()
                ).transpose(); // [seqLength, seqLength]

                // Extract [seqLength, depth] from Q
                INDArray QHead = this.Q.get(
                    NDArrayIndex.point(b),
                    NDArrayIndex.point(h),
                    NDArrayIndex.all(),
                    NDArrayIndex.all()
                ); // [seqLength, depth]

                // Perform mmul: [seqLength, seqLength] mmul [seqLength, depth] = [seqLength, depth]
                INDArray gradKHead = gradScoresTransposedHead.mmul(QHead); // [seqLength, depth]

                // Assign to gradK
                gradK.get(
                    NDArrayIndex.point(b),
                    NDArrayIndex.point(h),
                    NDArrayIndex.all(),
                    NDArrayIndex.all()
                ).assign(gradKHead);
            }
        }
        //NDArrayUtils.addGradient(gradients,"gradK", gradK);
        // System.out.println("gradK shape: " + gradK.shapeInfoToString());

        // Step 6: Reshape gradQ and gradK for Wq and Wk gradients
        // Reshape gradQ and gradK from [batchSize, numHeads, seqLength, depth] to [batchSize * seqLength, numHeads * depth]
        INDArray gradQReshaped = gradQ.reshape(batchSize * seqLength, numHeads * depth); // [1, 300]
        INDArray gradKReshaped = gradK.reshape(batchSize * seqLength, numHeads * depth); // [1, 300]

        // Step 7: Compute gradients for Wq, Wk, Wv
        // gradWq = inputQ^T * gradQReshaped [dModel, batchSize * seqLength] mmul [batchSize * seqLength, numHeads * depth] = [dModel, numHeads * depth]
        INDArray inputQReshaped = this.inputQ.reshape(batchSize * seqLength, dModel); // [batchSize * seqLength, dModel]
        INDArray gradWq = inputQReshaped.transpose().mmul(gradQReshaped); // [dModel, numHeads * depth]
        NDArrayUtils.addGradient(gradients,"Wq", gradWq);
        // System.out.println("gradWq shape: " + gradWq.shapeInfoToString());

        // gradWk = inputK^T * gradKReshaped [dModel, batchSize * seqLength] mmul [batchSize * seqLength, numHeads * depth] = [dModel, numHeads * depth]
        INDArray inputKReshaped = this.inputK.reshape(batchSize * seqLength, dModel); // [batchSize * seqLength, dModel]
        INDArray gradWk = inputKReshaped.transpose().mmul(gradKReshaped); // [dModel, numHeads * depth]
        NDArrayUtils.addGradient(gradients,"Wk", gradWk);
        // System.out.println("gradWk shape: " + gradWk.shapeInfoToString());

        // gradWv = inputV^T * gradVReshaped [dModel, batchSize * seqLength] mmul [batchSize * seqLength, numHeads * depth] = [dModel, numHeads * depth]
        // Reshape gradV from [batchSize, numHeads, depth, seqLength] to [batchSize * seqLength, numHeads * depth]
        INDArray gradVReshaped = gradV.permute(0, 1, 3, 2).reshape(batchSize * seqLength, numHeads * depth); // [1, 300]
        INDArray inputVReshaped = this.inputV.reshape(batchSize * seqLength, dModel); // [batchSize * seqLength, dModel]
        INDArray gradWv = inputVReshaped.transpose().mmul(gradVReshaped); // [dModel, numHeads * depth]
        NDArrayUtils.addGradient(gradients,"Wv", gradWv);
        // System.out.println("gradWv shape: " + gradWv.shapeInfoToString());

        // // Step 8: Compute gradients for the inputs (query, key, value)

        // // For gradInputQ
        // INDArray gradQPermuted = gradQ.permute(0, 2, 1, 3); // [batchSize, seqLength, numHeads, depth]
        // gradQReshaped = gradQPermuted.reshape(batchSize * seqLength, numHeads * depth); // [batchSize * seqLength, numHeads * depth]
        // INDArray gradInputQ = gradQReshaped.mmul(this.Wq.transpose()); // [batchSize * seqLength, dModel]
        // gradInputQ = gradInputQ.reshape(batchSize, seqLength, dModel); // [batchSize, seqLength, dModel]
        // gradients.put("gradInputQ", gradInputQ);
        // System.out.println("gradInputQ shape: " + gradInputQ.shapeInfoToString());

        // // For gradInputK
        // INDArray gradKPermuted = gradK.permute(0, 2, 1, 3);
        // gradKReshaped = gradKPermuted.reshape(batchSize * seqLength, numHeads * depth);
        // INDArray gradInputK = gradKReshaped.mmul(this.Wk.transpose());
        // gradInputK = gradInputK.reshape(batchSize, seqLength, dModel);
        // gradients.put("gradInputK", gradInputK);
        // System.out.println("gradInputK shape: " + gradInputK.shapeInfoToString());

        // // For gradInputV
        // INDArray gradVPermuted = gradV.permute(0, 2, 3, 1); // Adjusted permutation for gradV
        // gradVReshaped = gradVPermuted.reshape(batchSize * seqLength, numHeads * depth);
        // INDArray gradInputV = gradVReshaped.mmul(this.Wv.transpose());
        // gradInputV = gradInputV.reshape(batchSize, seqLength, dModel);
        // gradients.put("gradInputV", gradInputV);
        // System.out.println("gradInputV shape: " + gradInputV.shapeInfoToString());

        // Step 7: Compute gradInput as gradOutput * Wo^T
        // Reshape gradOutput to 2D
        INDArray gradOutputReshapedFinal = gradOutput.reshape(batchSize * seqLength, dModel); // [6, 300]

        // Perform mmul with Wo^T
        INDArray gradInputReshaped = gradOutputReshapedFinal.mmul(WoTransposed); // [6, 300]

        // Reshape back to 3D
        INDArray gradInputFinal = gradInputReshaped.reshape(batchSize, seqLength, dModel); // [1, 6, 300]

        NDArrayUtils.addGradient(gradients,"input", gradInputFinal);
        // System.out.println("gradInput shape: " + Arrays.toString(gradInputFinal.shape()));



        // Retourner les gradients des inputs séparément
        return gradients;
    }


    /**
     * Compute the gradient of the softmax function.
     *
     * @param softmaxOutput The output of the softmax function [batchSize, numHeads,
     *                      seqLength, seqLength]
     * @param gradOutput    The gradient w.r.t. the softmax output [batchSize,
     *                      numHeads, seqLength, seqLength]
     * @return The gradient w.r.t. the scores before softmax [batchSize, numHeads,
     *         seqLength, seqLength]
     */
    private INDArray softmaxGrad(INDArray softmaxOutput, INDArray gradOutput) {
        // Calculate the sum over the last axis
        INDArray sum = Nd4j.sum(softmaxOutput.mul(gradOutput), 3).reshape(
                softmaxOutput.shape()[0],
                softmaxOutput.shape()[1],
                softmaxOutput.shape()[2],
                1); // [batchSize, numHeads, seqLength, 1]

        // Calculate gradScores
        INDArray gradScores = softmaxOutput.mul(gradOutput).sub(softmaxOutput.mul(sum)); // [batchSize, numHeads,
                                                                                         // seqLength, seqLength]

        return gradScores;
    }

    public List<INDArray> getParameters() {
        // Return weight matrices as a list of INDArrays
        return Arrays.asList(Wq, Wk, Wv, Wo);
    }

    public List<INDArray> getGradients() {
        // Return gradients as a list of INDArrays
        List<INDArray> list = new ArrayList<>();
        list.add(gradients.get("Wq"));
        list.add(gradients.get("Wk"));
        list.add(gradients.get("Wv"));
        list.add(gradients.get("Wo"));
        if (list.contains(null)) {
            throw new IllegalArgumentException(" gradients contains null ");
        }
        return list;
    }

    public long getNumberOfParameters() {
        return Wq.length() + Wk.length() + Wv.length() + Wo.length();
    }

    public long getNumberOfGradients() {
        return gradients.get("Wq").length() + gradients.get("Wk").length() + gradients.get("Wv").length()
                + gradients.get("Wo").length();
    }

    // Getters and Setters
    public int getdModel() {
        return dModel;
    }

    public void setdModel(int dModel) {
        this.dModel = dModel;
    }

    public int getNumHeads() {
        return numHeads;
    }

    public void setNumHeads(int numHeads) {
        this.numHeads = numHeads;
    }

    public INDArray getWq() {
        return Wq;
    }

    public void setWq(INDArray wq) {
        Wq = wq;
    }

    public INDArray getWk() {
        return Wk;
    }

    public void setWk(INDArray wk) {
        Wk = wk;
    }

    public INDArray getWv() {
        return Wv;
    }

    public void setWv(INDArray wv) {
        Wv = wv;
    }

    public INDArray getWo() {
        return Wo;
    }

    public void setWo(INDArray wo) {
        Wo = wo;
    }
}
package RN.transformer;

import java.io.Serializable;

import org.nd4j.linalg.api.ndarray.INDArray;
import org.nd4j.linalg.factory.Nd4j;
import org.nd4j.linalg.indexing.NDArrayIndex;
import org.nd4j.linalg.ops.transforms.Transforms;

public class PositionalEncoding implements Serializable {
    /**
	 * 
	 */
	private static final long serialVersionUID = 7621854975948659411L;
	private final int dModel; // Dimensionnalité des embeddings
    
    public PositionalEncoding(int dModel) {
        this.dModel = dModel;
    }

    public INDArray getPositionalEncoding(long sequenceLength) {
        INDArray positions = Nd4j.arange(sequenceLength).reshape(sequenceLength, 1);
        INDArray i = Nd4j.arange(dModel).reshape(1, dModel);
        INDArray angleRates = Transforms.pow(i.divi(dModel).muli(-2).divi(2), 10000.0);

        INDArray angles = positions.mmul(angleRates);
        angles.get(NDArrayIndex.all(), NDArrayIndex.interval(0, 2, dModel)).assign(Transforms.sin(angles.get(NDArrayIndex.all(), NDArrayIndex.interval(0, 2, dModel))));
        angles.get(NDArrayIndex.all(), NDArrayIndex.interval(1, 2, dModel)).assign(Transforms.cos(angles.get(NDArrayIndex.all(), NDArrayIndex.interval(1, 2, dModel))));

        return angles;
    }


}


package RN.transformer;

import java.io.Serializable;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashMap;
import java.util.List;
import java.util.Map;

import org.nd4j.linalg.api.ndarray.INDArray;
import org.nd4j.linalg.factory.Nd4j;
import org.nd4j.linalg.ops.transforms.Transforms;

import RN.utils.NDArrayUtils;

/**
 * Classe représentant le réseau Feed-Forward positionnel dans le Transformer.
 * Utilise deux couches linéaires avec une activation ReLU entre elles.
 * Les tenseurs sont supposés avoir la forme [batchSize, seqLength, dModel].
 */
public class PositionwiseFeedForward implements Serializable {
    private static final long serialVersionUID = 4036365276693483563L;
    private INDArray W1, b1, W2, b2;
    private INDArray inputCache, reluCache; // Cache pour le forward
    private Map<String, INDArray> gradients = new HashMap<>();
    private int dModel;
    private int ffSize; // Taille de la couche Feed-Forward
    private double epsilon = 1e-7; // Ajouter epsilon ici pour éviter la division par zéro

    /**
     * Constructeur de la classe PositionwiseFeedForward.
     * 
     * @param modelSize Taille du modèle (dModel)
     * @param ffSize    Taille de la couche Feed-Forward (ffSize)
     */
    public PositionwiseFeedForward(int modelSize, int ffSize) {
        this.dModel = modelSize;
        this.ffSize = ffSize;
        // Initialisation des poids avec une distribution normale et He Initialization
        this.W1 = Nd4j.randn(modelSize, ffSize).div(Math.sqrt(modelSize)); // [dModel, ffSize]
        this.b1 = Nd4j.zeros(1, ffSize); // [1, ffSize]
        this.W2 = Nd4j.randn(ffSize, modelSize).div(Math.sqrt(ffSize)); // [ffSize, dModel]
        this.b2 = Nd4j.zeros(1, modelSize); // [1, dModel]
    }

    /**
     * Passe forward du réseau Feed-Forward positionnel.
     * 
     * @param input Entrée de forme [batchSize, seqLength, dModel]
     * @return Sortie de forme [batchSize, seqLength, dModel]
     */
    public INDArray forward(INDArray input) {
        // Stockage de l'entrée pour la rétropropagation
        this.inputCache = input.dup(); // [batchSize, seqLength, dModel]
        
        long batchSize = input.shape()[0];
        long seqLength = input.shape()[1];
        
        // Reshape input pour la multiplication matricielle
        INDArray inputReshaped = input.reshape(batchSize * seqLength, dModel); // [batchSize * seqLength, dModel]
        
        // Première couche linéaire
        INDArray hidden = inputReshaped.mmul(W1).addiRowVector(b1); // [batchSize * seqLength, ffSize]
        this.reluCache = hidden.dup(); // [batchSize * seqLength, ffSize]
        
        // Activation ReLU
        INDArray reluOutput = Transforms.relu(hidden); // [batchSize * seqLength, ffSize]
        
        // Deuxième couche linéaire
        INDArray output = reluOutput.mmul(W2).addiRowVector(b2); // [batchSize * seqLength, dModel]
        
        // Reshape back to original dimensions
        return output.reshape(batchSize, seqLength, dModel); // [batchSize, seqLength, dModel]
    }

    /**
     * Passe backward pour calculer les gradients.
     * 
     * @param gradOutput Gradient provenant de la couche suivante de forme [batchSize, seqLength, dModel] ou [batchSize * seqLength, dModel]
     * @return Map contenant les gradients pour les paramètres 'W1', 'b1', 'W2', 'b2', et 'input'
     */
    public Map<String, INDArray> backward(INDArray gradOutput) {
        boolean reshaped = false;
        long batchSize = 1;
        long seqLength = 1;
        
        // Vérifier le rang de gradOutput et reshaper si nécessaire
        if (gradOutput.rank() == 3) {
            batchSize = gradOutput.size(0);
            seqLength = gradOutput.size(1);
            gradOutput = gradOutput.reshape(batchSize * seqLength, dModel); // [batchSize * seqLength, dModel]
            reshaped = true;
        } else if (gradOutput.rank() != 2) {
            throw new IllegalArgumentException("gradOutput doit être de rang 2 ou 3.");
        }

        // Reshape inputCache de la même manière que gradOutput
        INDArray inputReshaped = inputCache.reshape(batchSize * seqLength, dModel); // [batchSize * seqLength, dModel]

        // 1. Calcul des gradients par rapport à W2 et b2
        INDArray reluOutput = Transforms.relu(reluCache); // [batchSize * seqLength, ffSize]
        INDArray gradW2 = reluOutput.transpose().mmul(gradOutput); // [ffSize, dModel]
        INDArray gradB2 = gradOutput.sum(0).reshape(1, dModel); // [1, dModel]

        // 2. Propagation du gradient à travers la deuxième couche linéaire
        INDArray gradHidden = gradOutput.mmul(W2.transpose()); // [batchSize * seqLength, ffSize]

        // 3. Application de la dérivée de ReLU
        INDArray reluGrad = reluCache.gt(0).castTo(reluOutput.dataType()); // [batchSize * seqLength, ffSize]
        INDArray gradThroughRelu = gradHidden.mul(reluGrad); // [batchSize * seqLength, ffSize]

        // 4. Calcul des gradients par rapport à W1 et b1
        INDArray gradW1 = inputReshaped.transpose().mmul(gradThroughRelu); // [dModel, ffSize]
        INDArray gradB1 = gradThroughRelu.sum(0).reshape(1, ffSize); // [1, ffSize]
        
        // 5. Calcul du gradient à propager à la couche précédente
        INDArray gradInput = gradThroughRelu.mmul(W1.transpose()); // [batchSize * seqLength, dModel]

        // Remettre la forme d'origine si reshaped
        if (reshaped) {
            gradInput = gradInput.reshape(batchSize, seqLength, dModel); // [batchSize, seqLength, dModel]
        }

        // Stockage des gradients dans la map
        NDArrayUtils.addGradient(gradients, "W1", gradW1);
        NDArrayUtils.addGradient(gradients, "b1", gradB1);
        NDArrayUtils.addGradient(gradients, "W2", gradW2);
        NDArrayUtils.addGradient(gradients, "b2", gradB2);
        NDArrayUtils.addGradient(gradients, "input", gradInput); // Gradient à propager vers les couches précédentes

        return gradients;
    }

    /**
     * Obtient les gradients des paramètres.
     * 
     * @return Liste des gradients dans l'ordre [W1, b1, W2, b2]
     */
    public List<INDArray> getGradients() {
        List<INDArray> list = new ArrayList<>();
        list.add(gradients.get("W1"));
        list.add(gradients.get("b1"));
        list.add(gradients.get("W2"));
        list.add(gradients.get("b2"));
        if (list.contains(null)) {
            throw new IllegalArgumentException(" gradients contains null ");
        }
        return list; 
    }

    /**
     * Obtient les paramètres du réseau Feed-Forward positionnel.
     * 
     * @return Liste des paramètres dans l'ordre [W1, b1, W2, b2]
     */
    public List<INDArray> getParameters() {
        return Arrays.asList(W1, b1, W2, b2);
    }

    /**
     * Définit (met à jour) les paramètres du réseau Feed-Forward positionnel.
     * 
     * @param newW1 Nouvelles valeurs pour W1
     * @param newB1 Nouvelles valeurs pour b1
     * @param newW2 Nouvelles valeurs pour W2
     * @param newB2 Nouvelles valeurs pour b2
     */
    public void setParameters(INDArray newW1, INDArray newB1, INDArray newW2, INDArray newB2) {
        this.W1 = newW1;
        this.b1 = newB1;
        this.W2 = newW2;
        this.b2 = newB2;
    }

    /**
     * Obtient le nombre total de paramètres.
     * 
     * @return Nombre total de paramètres
     */
    public long getNumberOfParameters() {
        return W1.length() + b1.length() + W2.length() + b2.length();
    }

    /**
     * Obtient le nombre total de gradients.
     * 
     * @return Nombre total de gradients
     */
    public long getNumberOfGradients() {
        return gradients.get("W1").length() + gradients.get("b1").length() +
               gradients.get("W2").length() + gradients.get("b2").length();
    }
}
package RN.transformer;


import java.io.Serializable;
import java.util.ArrayList;
import java.util.Collection;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.stream.Collectors;

import org.deeplearning4j.models.embeddings.wordvectors.WordVectors;
import org.nd4j.linalg.api.ndarray.INDArray;
import org.nd4j.linalg.factory.Nd4j;

public class Tokenizer implements Serializable {
    private static final long serialVersionUID = -1691008595018974489L;
    private Map<String, Integer> tokenToId;
    private Map<Integer, String> idToToken;
    private int vocabSize;
    private INDArray pretrainedEmbeddings;

    // Tokens spéciaux
    private static final String PAD_TOKEN = "<PAD>";
    private static final String UNK_TOKEN = "<UNK>";
    private static final String START_TOKEN = "<START>";
    private static final String END_TOKEN = "<END>";
    
    // Les embeddings pour les tokens spéciaux seront initialisés de manière spécifique
    private static final int EMBEDDING_SIZE = 300; // dModel

    public Tokenizer(WordVectors wordVectors) {
        this.tokenToId = new HashMap<>();
        this.idToToken = new HashMap<>();
        
        // Étape 1: Initialiser les tokens spéciaux en premier
        // Cela garantit que leurs IDs sont constants (0, 1, 2, 3)
        initializeSpecialTokens();
        
        // Étape 2: Ajouter tous les mots du vocabulaire Word2Vec
        Collection<String> words = wordVectors.vocab().words();
        for (String word : words) {
            if (!tokenToId.containsKey(word)) {
                addToken(word);
            }
        }
        
        this.vocabSize = tokenToId.size();
        
        // Étape 3: Créer la matrice d'embeddings avec les tokens spéciaux
        initializeEmbeddings(wordVectors);
    }
    
    public Tokenizer(List<String> words) {
        this.tokenToId = new HashMap<>();
        this.idToToken = new HashMap<>();
        
        // Étape 1: Initialiser les tokens spéciaux en premier
        // Cela garantit que leurs IDs sont constants (0, 1, 2, 3)
        initializeSpecialTokens();
        
        // Étape 2: Ajouter tous les mots du vocabulaire Word2Vec
        for (String word : words) {
            if (!tokenToId.containsKey(word)) {
                addToken(word);
            }
        }
        
        this.vocabSize = tokenToId.size();
        
        // Étape 3: Créer la matrice d'embeddings avec les tokens spéciaux
//        initializeEmbeddings(words);
    }

    private void initializeSpecialTokens() {
        // Les tokens spéciaux sont toujours ajoutés dans le même ordre
        addSpecialToken(PAD_TOKEN);    // ID = 0
        addSpecialToken(UNK_TOKEN);    // ID = 1
        addSpecialToken(START_TOKEN);  // ID = 2
        addSpecialToken(END_TOKEN);    // ID = 3
    }

    private void initializeEmbeddings(WordVectors wordVectors) {
        // Créer une nouvelle matrice d'embeddings avec la taille du vocabulaire complet
        pretrainedEmbeddings = Nd4j.zeros(vocabSize, EMBEDDING_SIZE);
        
        // Calculer le vecteur moyen pour l'initialisation des tokens spéciaux
        INDArray meanVector = calculateMeanVector(wordVectors);
        
        // Initialiser les embeddings des tokens spéciaux
        // PAD_TOKEN: vecteur de zéros (déjà initialisé par défaut)
        // UNK_TOKEN: vecteur moyen
        pretrainedEmbeddings.putRow(getUnkTokenId(), meanVector);
        // START_TOKEN: vecteur moyen + bruit gaussien
        pretrainedEmbeddings.putRow(getStartTokenId(), meanVector.add(Nd4j.randn(1, EMBEDDING_SIZE).mul(0.1)));
        // END_TOKEN: vecteur moyen + bruit gaussien
        pretrainedEmbeddings.putRow(getEndTokenId(), meanVector.add(Nd4j.randn(1, EMBEDDING_SIZE).mul(0.1)));
        
        // Copier les embeddings pour tous les autres tokens
        for (Map.Entry<String, Integer> entry : tokenToId.entrySet()) {
            String token = entry.getKey();
            int id = entry.getValue();
            
            // Ignorer les tokens spéciaux déjà traités
            if (isSpecialToken(token)) continue;
            
            if (wordVectors.hasWord(token)) {
                INDArray wordVector = wordVectors.getWordVectorMatrix(token);
                pretrainedEmbeddings.putRow(id, wordVector);
            } else {
                // Utiliser le vecteur moyen pour les mots inconnus
                pretrainedEmbeddings.putRow(id, meanVector);
            }
        }
    }

    private INDArray calculateMeanVector(WordVectors wordVectors) {
        INDArray sum = Nd4j.zeros(EMBEDDING_SIZE);
        int count = 0;
        
        Collection<String> words = wordVectors.vocab().words();
        for (String word : words) {
            sum.addi(wordVectors.getWordVectorMatrix(word));
            count++;
        }
        
        return sum.div(count);
    }

    private boolean isSpecialToken(String token) {
        return token.equals(PAD_TOKEN) || token.equals(UNK_TOKEN) || 
               token.equals(START_TOKEN) || token.equals(END_TOKEN);
    }

    private void addSpecialToken(String token) {
        int id = tokenToId.size();
        tokenToId.put(token, id);
        idToToken.put(id, token);
    }

    private void addToken(String token) {
        if (!tokenToId.containsKey(token)) {
            int id = tokenToId.size();
            tokenToId.put(token, id);
            idToToken.put(id, token);
        }
    }

    public List<String> tokenize(String text) {
        // Cette regex simple sépare les mots et la ponctuation, ce qui est une amélioration par rapport à la séparation par espace.
        // Pour des règles plus complexes, envisagez d'utiliser une librairie de tokenisation spécialisée.
        String[] tokens = text.split("\\s+|(?=\\p{Punct})|(?<=\\p{Punct})");
        List<String> tokenList = new ArrayList<>();
        for (String token : tokens) {
            if (!token.trim().isEmpty()) { // Ignorer les chaînes vides
                tokenList.add(token);
            }
        }
        return tokenList;
    }


    private boolean isPunctuation(String token) {
        // Une vérification simple de la ponctuation basée sur regex; ajustez selon vos besoins
        return token.matches("\\p{Punct}");
    }

    public List<Integer> tokensToIds(List<String> tokens) {
        return tokens.stream()
                .map(token -> tokenToId.getOrDefault(token, tokenToId.get(UNK_TOKEN)))
                .collect(Collectors.toList());
    }

    public String idsToTokens(List<Integer> ids) {
        return ids.stream()
                .map(id -> idToToken.getOrDefault(id, UNK_TOKEN))
                .collect(Collectors.joining(" "));
    }

    // Nouvelles méthodes pour gérer les tokens spéciaux
    public int getPadTokenId() {
        return tokenToId.get(PAD_TOKEN);
    }

    public int getUnkTokenId() {
        return tokenToId.get(UNK_TOKEN);
    }

    public int getStartTokenId() {
        return tokenToId.get(START_TOKEN);
    }

    public int getEndTokenId() {
        return tokenToId.get(END_TOKEN);
    }

    public int getVocabSize() {
        return vocabSize;
    }

    public String getToken(int id) {
        return idToToken.getOrDefault(id, UNK_TOKEN);
    }
    
    public INDArray getPretrainedEmbeddings() {
        return pretrainedEmbeddings;
    }
    
}



package RN.transformer;

import java.io.File;
import java.io.FileInputStream;
import java.io.FileOutputStream;
import java.io.IOException;
import java.io.ObjectInputStream;
import java.io.ObjectOutputStream;
import java.io.Serializable;
import java.util.List;

import org.deeplearning4j.models.embeddings.loader.WordVectorSerializer;
import org.nd4j.linalg.api.ndarray.INDArray;

public class Transformer implements Serializable {
	
    private static final long serialVersionUID = 1L;
    
    private Encoder encoder;
    private Decoder decoder;
    private CustomAdamOptimizer optimizer;
    // Autres attributs du Transformer...

    public Transformer(/* paramètres du constructeur */) {
        // Initialisation du Transformer...
    }

    // Méthodes existantes du Transformer...

    public void saveState(String filePath) throws IOException {
        try (ObjectOutputStream oos = new ObjectOutputStream(new FileOutputStream(filePath))) {
            // Sauvegarder l'état de l'encodeur et du décodeur
            oos.writeObject(encoder);
            oos.writeObject(decoder);
            
            // Sauvegarder l'état de l'optimiseur
            oos.writeObject(optimizer.getCurrentStep());
            oos.writeObject(optimizer.getEpoch());
            oos.writeObject(optimizer.getLearningRate());
            
            // Sauvegarder les paramètres du modèle
            List<INDArray> parameters = getParameters();
            oos.writeObject(parameters.size());
            for (INDArray param : parameters) {
                oos.writeObject(param);
            }
        }
    }

    public void loadState(String filePath) throws IOException, ClassNotFoundException {
        try (ObjectInputStream ois = new ObjectInputStream(new FileInputStream(filePath))) {
            // Charger l'état de l'encodeur et du décodeur
            this.encoder = (Encoder) ois.readObject();
            this.decoder = (Decoder) ois.readObject();
            
            // Charger l'état de l'optimiseur
            int currentStep = (int) ois.readObject();
            int epoch = (int) ois.readObject();
            float learningRate = (float) ois.readObject();
            optimizer.setCurrentStep(currentStep);
            optimizer.setEpoch(epoch);
            optimizer.setLearningRate(learningRate);
            
            // Charger les paramètres du modèle
            int numParams = (int) ois.readObject();
            List<INDArray> parameters = getParameters();
            for (int i = 0; i < numParams; i++) {
                INDArray param = (INDArray) ois.readObject();
                parameters.get(i).assign(param);
            }
        }
    }
    


    private List<INDArray> getParameters() {
        // Méthode pour obtenir tous les paramètres du modèle
        // Combinez les paramètres de l'encodeur et du décodeur
        List<INDArray> params = encoder.getParameters();
        params.addAll(decoder.getParameters());
        return params;
    }
}package RN.transformer;

import java.io.File;
import java.io.FileInputStream;
import java.io.FileOutputStream;
import java.io.IOException;
import java.io.ObjectInputStream;
import java.io.ObjectOutputStream;
import java.io.Serializable;
import java.util.ArrayList;
import java.util.Arrays;
import java.util.HashMap;
import java.util.List;
import java.util.Map;
import java.util.stream.Collectors;

import org.apache.commons.lang3.tuple.Pair;
import org.deeplearning4j.models.embeddings.loader.WordVectorSerializer;
import org.deeplearning4j.models.embeddings.wordvectors.WordVectors;
import org.nd4j.linalg.api.buffer.DataType;
import org.nd4j.linalg.api.ndarray.INDArray;
import org.nd4j.linalg.factory.Nd4j;
import org.nd4j.linalg.indexing.INDArrayIndex;
import org.nd4j.linalg.indexing.NDArrayIndex;
import org.nd4j.linalg.ops.transforms.Transforms;

import RN.utils.NDArrayUtils;

public class TransformerModel  implements Serializable {
	
    /**
	 * 
	 */
	private static final long serialVersionUID = -4799769434788429831L;
	
	private static String W2VECPATH = "pretrained-embeddings/mon_model_word2vec.txt";
	private boolean isTrained = false;
    public Encoder encoder;
    public Decoder decoder;
    public CustomAdamOptimizer optimizer;
    public Tokenizer tokenizer;
    private double dropoutRate = 0.1; // Exemple de taux de dropout fixe
    private transient static WordVectors wordVectors; // Chargé une fois, accessible statiquement
    private int dModel = 300; // dmodel must be divisible by numHeads
    private int numLayers = 6;
    private int numHeads = 6; 
    private int dff = 2048;
    private static INDArray pretrainedEmbeddings = null;
    private List<INDArray> combinedParameters = new ArrayList<>();
    private List<INDArray> combinedGradients = new ArrayList<INDArray>();
    
    static {
        try {
        	wordVectors = WordVectorSerializer.readWord2VecModel(new File(W2VECPATH), true);
        } catch (Exception e) {
            e.printStackTrace();
        }
    }

    /**
     * Méthode par défaut pour le constructeur.
     *
     * @throws IOException en cas d'erreur de chargement des embeddings.
     */
    public TransformerModel() throws IOException {
        this(6, 300, 6, 2048, 0.1);
    }
    
    /**
     * Constructeur principal du modèle Transformer.
     *
     * @param numLayers    Nombre de couches dans l'encodeur et le décodeur.
     * @param dModel       Dimension du modèle.
     * @param numHeads     Nombre de têtes dans l'attention multi-têtes.
     * @param dff          Dimension de la couche Feed-Forward.
     * @param dropoutRate  Taux de dropout.
     */
    public TransformerModel(int numLayers, int dModel, int numHeads, int dff, double dropoutRate) {

    	this.numLayers = numLayers;
    	this.dModel = dModel;
    	this.numHeads = numHeads;
    	this.dff = dff;
    	this.dropoutRate = dropoutRate;

        // Garantit la compatibilité et les performances optimales
        Nd4j.setDefaultDataTypes(DataType.FLOAT, DataType.FLOAT);
    	
        // Charger Word2Vec
        WordVectors wordVectors = WordVectorSerializer.readWord2VecModel(new File(W2VECPATH));
        
        // Créer le tokenizer qui gère maintenant aussi les embeddings
        this.tokenizer = new Tokenizer(wordVectors);
        
        // Utiliser les embeddings du tokenizer
        pretrainedEmbeddings = tokenizer.getPretrainedEmbeddings();
        
        this.encoder = new Encoder(numLayers, dModel, numHeads, dff, dropoutRate, this.tokenizer);
        this.decoder = new Decoder(numLayers, dModel, numHeads, dff, dropoutRate);   

        addCombinedParameters();

        this.optimizer = new CustomAdamOptimizer(0.001f, dModel, 1000, combinedParameters); // Initialisation hypothétique
    }


    public void train(DataGenerator dataGenerator, int epochNum) throws IOException {
        for (int epoch = 0; epoch < epochNum; epoch++) {
            optimizer.setEpoch(epoch);
            System.out.println("Epoch " + (epoch + 1) + " / " + epochNum);
    
            while (dataGenerator.hasNextBatch()) {
                
                // Nettoyer les gradients précédents
                cleanGradients();
                
                Batch batch = dataGenerator.nextBatch();
    
                // Tokeniser chaque phrase dans le batch séparément
                List<List<String>> tokenizedData = batch.getData().stream()
                    .map(sentence -> tokenizer.tokenize(sentence))
                    .collect(Collectors.toList());
    
                List<List<String>> tokenizedTargets = batch.getTarget().stream()
                    .map(sentence -> tokenizer.tokenize(sentence))
                    .collect(Collectors.toList());
    
                // Convertir les tokens en IDs pour chaque phrase du batch
                List<List<Integer>> dataTokenIds = tokenizedData.stream()
                    .map(tokens -> tokenizer.tokensToIds(tokens))
                    .collect(Collectors.toList());
    
                List<List<Integer>> targetTokenIds = tokenizedTargets.stream()
                    .map(tokens -> tokenizer.tokensToIds(tokens))
                    .collect(Collectors.toList());
    
                // Déterminer la longueur maximale dans le batch pour le padding
                int maxDataSeqLength = dataTokenIds.stream().mapToInt(List::size).max().orElse(0);
                int maxTargetSeqLength = targetTokenIds.stream().mapToInt(List::size).max().orElse(0);
    
                // Padder les séquences
                List<List<Integer>> paddedDataTokenIds = padSequences(dataTokenIds, maxDataSeqLength, tokenizer.getPadTokenId());
                List<List<Integer>> paddedTargetTokenIds = padSequences(targetTokenIds, maxTargetSeqLength, tokenizer.getPadTokenId());
    
                // Créer les masques de padding pour le batch
                INDArray encoderPaddingMask = createPaddingMask(paddedDataTokenIds);
                INDArray decoderPaddingMask = createPaddingMask(paddedTargetTokenIds);
                INDArray lookAheadMask = createLookAheadMask(maxTargetSeqLength);
    
                // Encoder les données du batch
                INDArray encoded = encoder.encode(true, paddedDataTokenIds, encoderPaddingMask);
                // System.out.println("Encoded output shape: " + Arrays.toString(encoded.shape()));
    
                // Décoder les données encodées
                INDArray decodedOutput = decoder.decode(true, encoded, encoded, lookAheadMask, encoderPaddingMask); // Passer encoderPaddingMask pour cross-attention
                // System.out.println("Decoded output shape: " + Arrays.toString(decodedOutput.shape()));
    
                // Calculer la perte et les gradients
                List<INDArray> decodedLogits = Arrays.asList(decodedOutput);
                backpropagation(decodedLogits, paddedTargetTokenIds);
    
                // Mettre à jour les paramètres du modèle via l'optimiseur
                optimizer.update(combinedParameters, combinedGradients);
    
                // (Optionnel) Afficher la progression
                // System.out.println("Batch processed.");
            }
    
            // Réinitialiser le générateur de données pour le prochain epoch
            dataGenerator.init();
            System.out.println("Epoch " + (epoch + 1) + " completed.");
        }
    
        isTrained = true;
        System.out.println("Training completed.");
    }
    

    private List<List<Integer>> padSequences(List<List<Integer>> sequences, int maxLength, int padTokenId) {
        return sequences.stream()
            .map(seq -> {
                List<Integer> padded = new ArrayList<>(seq);
                while (padded.size() < maxLength) {
                    padded.add(padTokenId);
                }
                return padded;
            })
            .collect(Collectors.toList());
    }
    

    
    public INDArray createLookAheadMask(int size) {
        // Création d'une matrice où les éléments au-dessus de la diagonale sont 1 (ce qui signifie masqués)
        INDArray mask = Nd4j.ones(size, size);
        INDArray lowerTriangle = Nd4j.tri(size, size, 0); // Crée une matrice triangulaire inférieure
        mask.subi(lowerTriangle); 
        // Appliquer dessous le masquage infini pour softmax
        for (int i = 0; i < size; i++) {
            for (int j = i + 1; j < size; j++) {
                mask.putScalar(i, j, Double.NEGATIVE_INFINITY);
            }
        }        
        return mask;
    }
    

    
    public INDArray createPaddingMask(List<List<Integer>> tokenIdsBatch) {
        // Déterminer le batch size et la longueur maximale de séquence
        int batchSize = tokenIdsBatch.size();
        int seqLength = tokenIdsBatch.get(0).size(); // Supposons que toutes les séquences sont padées à seqLength
    
        // Créer un masque de zéros de la forme [batchSize, 1, 1, seqLength]
        INDArray mask = Nd4j.zeros(DataType.FLOAT, batchSize, 1, 1, seqLength);
    
        for (int i = 0; i < batchSize; i++) {
            for (int j = 0; j < seqLength; j++) {
                if (tokenIdsBatch.get(i).get(j) == tokenizer.getPadTokenId()) { // Supposons que le token <PAD> a un ID spécifique
                    mask.putScalar(new int[]{i, 0, 0, j}, Float.NEGATIVE_INFINITY); // Utiliser -inf pour les positions à ignorer
                } else {
                    mask.putScalar(new int[]{i, 0, 0, j}, 0.0f); // Pas de masque pour les positions valides
                }
            }
        }
    
        return mask;
    }
    
    
    private void backpropagation(List<INDArray> decodedLogits, List<List<Integer>> targetTokenIdsBatch) {
        // Étape 1: Calcul de la perte et des gradients initiaux
        Pair<Float, INDArray> lossAndGradients = calculateCrossEntropyLossAndGradient(decodedLogits, targetTokenIdsBatch);
        float loss = lossAndGradients.getLeft();
        INDArray initialGradients = lossAndGradients.getRight();
    
        // Afficher la perte pour le monitoring
        System.out.println("Perte: " + loss);
    
        // Étape 2: Rétropropagation à travers le Décodeur
        Map<String, INDArray> decoderGradients = decoder.backward(initialGradients);

        // Vérifiez les clés présentes dans decoderGradients
        // System.out.println("Keys in decoderGradients: " + decoderGradients.keySet());
        
        // Extraire les gradients pertinents pour l'encodeur à partir de decoderGradients
        Map<String, INDArray> encoderGradients = extractEncoderGradients(decoderGradients);

        // Étape 3: Rétropropagation à travers l'Encodeur
        encoder.backward(encoderGradients);

        // Ajouter tous les gradients calculés aux `combinedGradients`
        addCombinedGradients();
    
        // Mettre à jour les poids basés sur les gradients calculés, normalement fait par l'optimiseur
        updateModelWeights();
    }
    


    private Map<String, INDArray> extractEncoderGradients(Map<String, INDArray> decoderGradients) {
        // Créez un nouveau Map pour contenir les gradients spécifiquement pour l'encodeur.
        Map<String, INDArray> encoderGradients = new HashMap<>();
        
        // Extrayez les gradients par rapport aux entrées K et V de l'attention encoder-décodeur.
        INDArray gradAttentionOutputConcat = decoderGradients.get("gradAttentionOutputConcat");
        
        
        // Vérifiez que gradAttentionOutputConcat n'est pas null
        if (gradAttentionOutputConcat == null) {
            throw new IllegalStateException("gradAttentionOutputConcat est null. Assurez-vous que MultiHeadAttention.backward retourne correctement ce gradient.");
        }
        
        // Ajoutez gradAttentionOutputConcat au Map des gradients pour l'encodeur
        encoderGradients.put("gradAttentionOutputConcat", gradAttentionOutputConcat);
        
        return encoderGradients;
    }
    


    private void updateModelWeights() {

        // Log pour les tailles
        // System.out.println("Nombre de paramètres : " + combinedParameters.size());
        // System.out.println("Nombre de gradients : " + combinedGradients.size());

        // Vérifiez si les tailles correspondent avant de les passer à l'optimiseur
        if (combinedParameters.size() != combinedGradients.size()) {
            throw new IllegalArgumentException("La taille de la liste des paramètres et des gradients doit être la même.");
        }
    
        // Mettre à jour les poids du modèle via l'optimiseur
        optimizer.update(combinedParameters, combinedGradients);
    }
    



	public void addCombinedParameters() {
        
        // Ajoute les paramètres de l'encoder
        combinedParameters.addAll(encoder.getParameters());
        
        // Ajoute les paramètres du decoder
        combinedParameters.addAll(decoder.getParameters());
        
    }

    private void addCombinedGradients() {
        
        // Ajoute les gradients de l'encoder
        combinedGradients.addAll(encoder.getGradients());
        
        // Ajoute les gradients du decoder
        combinedGradients.addAll(decoder.getGradients());
        
    }
    
    public void cleanGradients() {
    	combinedGradients.clear();
    }

    public String infer(String prompt, int maxLength) {
        if (!isTrained) {
            throw new IllegalStateException("Le modèle doit être entraîné avant l'inférence.");
        }
    
        // Tokenisation du prompt
        List<String> promptTokens = tokenizer.tokenize(prompt);
        List<Integer> promptTokenIds = tokenizer.tokensToIds(promptTokens);
    
        // Ajout du token de début si nécessaire
        promptTokenIds.add(0, tokenizer.getStartTokenId());
    
        // Déterminer la longueur maximale de séquence après ajout du token de début
        int maxSeqLength = promptTokenIds.size();
    
        // Padder la séquence si nécessaire
        List<Integer> paddedPromptTokenIds = padSequencesSingle(promptTokenIds, maxSeqLength, tokenizer.getPadTokenId());
    
        // Créer le masque de padding pour l'encodeur
        INDArray encoderPaddingMask = createPaddingMaskSingle(paddedPromptTokenIds);
    
        // Encoder le prompt (traité comme un batch de taille 1)
        INDArray encodedPrompt = encoder.encode(false, Arrays.asList(paddedPromptTokenIds), encoderPaddingMask);
        // System.out.println("Encoded prompt shape: " + Arrays.toString(encodedPrompt.shape()));
    
        // Initialiser les IDs de sortie avec le token de début
        List<Integer> outputIds = new ArrayList<>();
        outputIds.add(tokenizer.getStartTokenId());
    
        for (int i = 0; i < maxLength; i++) {
            // Créer les masques pour le décodeur
            INDArray lookAheadMask = createLookAheadMask(outputIds.size());
            INDArray crossAttnMask = encoderPaddingMask; // [1,1,1,4}

            // Convertir les IDs de sortie en embeddings pour le décodeur
            INDArray encodedDecoderInput = encoder.lookupEmbeddings(Arrays.asList(outputIds));
            // System.out.println("Encoded decoder input shape: " + Arrays.toString(encodedDecoderInput.shape()));
    
            // Décoder
            INDArray logits = decoder.decode(false, encodedPrompt, encodedDecoderInput, lookAheadMask, crossAttnMask);
            // System.out.println("Logits shape: " + Arrays.toString(logits.shape()));
    
            // Extraction des logits du dernier token généré
            // Correction de l'utilisation de INDArrayIndex
            int lastPosition = (int) logits.shape()[1] - 1; // seqLength - 1
            INDArray lastTokenLogits = logits.get(
                NDArrayIndex.point(0),                     // batch 0
                NDArrayIndex.point(lastPosition),          // dernière position dans seqLength
                NDArrayIndex.all()                         // tous les éléments dans vocabSize
            ).dup(); // [vocabSize]
    
            // Appliquer softmax pour obtenir les probabilités
            INDArray softmaxLogits = Transforms.softmax(lastTokenLogits, false);
    
            // Sélectionner le token avec la plus haute probabilité
            int predictedTokenId = Nd4j.argMax(softmaxLogits, 0).getInt(0);
    
            // Ajouter le token prédit à la séquence de sortie
            outputIds.add(predictedTokenId);
    
            // Vérification du token de fin
            if (predictedTokenId == tokenizer.getEndTokenId()) {
                break;
            }
    
            // (Optionnel) Implémentation d'une stratégie de décodage plus sophistiquée
            // Par exemple, beam search, échantillonnage avec température, etc.
            // Ceci nécessiterait une refonte plus approfondie de la boucle de génération.
        }
    
        // Conversion des IDs en tokens
        // Exclure le token de début
        return tokenizer.idsToTokens(outputIds.subList(1, outputIds.size()));
    }
    
    
    /**
     * Pad une seule séquence à une longueur maximale avec le token de padding.
     *
     * @param sequence      La séquence d'IDs de tokens.
     * @param maxLength     La longueur maximale à atteindre.
     * @param padTokenId    L'ID du token de padding.
     * @return La séquence padée.
     */
    private List<Integer> padSequencesSingle(List<Integer> sequence, int maxLength, int padTokenId) {
        List<Integer> padded = new ArrayList<>(sequence);
        while (padded.size() < maxLength) {
            padded.add(padTokenId);
        }
        return padded;
    }
    
    /**
     * Crée un masque de padding pour une seule séquence.
     *
     * @param tokenIds La liste d'IDs de tokens.
     * @return Un INDArray représentant le masque de padding.
     */
    public INDArray createPaddingMaskSingle(List<Integer> tokenIds) {
        int seqLength = tokenIds.size();
        // Créer un masque de zéros de la forme [1, 1, 1, seqLength]
        INDArray mask = Nd4j.zeros(DataType.FLOAT, 1, 1, 1, seqLength);
    
        for (int j = 0; j < seqLength; j++) {
            if (tokenIds.get(j) == tokenizer.getPadTokenId()) {
                mask.putScalar(new int[]{0, 0, 0, j}, Float.NEGATIVE_INFINITY);
            } else {
                mask.putScalar(new int[]{0, 0, 0, j}, 0.0f);
            }
        }
    
        return mask;
    }
    


    public boolean isTrained() {
        return isTrained;
    }


    protected Pair<Float, INDArray> calculateCrossEntropyLossAndGradient(List<INDArray> decodedLogits, List<List<Integer>> targetTokenIdsBatch) {
        float loss = 0.0f;
        int batchSize = targetTokenIdsBatch.size();
        int maxSeqLength = targetTokenIdsBatch.stream().mapToInt(List::size).max().orElse(0);
        int vocabSize = (int) decodedLogits.get(0).shape()[2];
    
        // Assumons que decodedLogits contient une seule INDArray pour tout le batch
        INDArray logits = decodedLogits.get(0); // [batchSize, targetSeqLength, vocabSize]
        INDArray gradients = Nd4j.zeros(DataType.FLOAT, logits.shape()); // [batchSize, targetSeqLength, vocabSize]
    
        for (int b = 0; b < batchSize; b++) {
            List<Integer> targetTokenIds = targetTokenIdsBatch.get(b);
            for (int i = 0; i < targetTokenIds.size(); i++) {
                int targetId = targetTokenIds.get(i); // L'ID attendu à la position i
    
                // Extraire les logits pour la position i de la séquence b
                INDArray logitsForPosition = logits.get(
                    NDArrayIndex.point(b), 
                    NDArrayIndex.point(i), 
                    NDArrayIndex.all()
                ).dup(); // [vocabSize]
    
                // Appliquer softmax
                INDArray softmaxLogits = Transforms.softmax(logitsForPosition, false); 
    
                // Calculer le log softmax pour l'ID cible
                double prob = softmaxLogits.getDouble(targetId);
                // Éviter les problèmes avec log(0) en ajoutant une petite constante epsilon
                float logSoftmaxForTarget = (float) Math.log(Math.max(prob, 1e-10));
    
                // Accumuler la perte négative log softmax
                loss += -logSoftmaxForTarget;
    
                // Calculer le gradient (p - y)
                INDArray targetOneHot = Nd4j.zeros(DataType.FLOAT, vocabSize);
                targetOneHot.putScalar(targetId, 1.0f);
                INDArray gradForPosition = softmaxLogits.sub(targetOneHot); // [vocabSize]
    
                // Assignation correcte des gradients
                gradients.put(
                    new INDArrayIndex[]{
                        NDArrayIndex.point(b), 
                        NDArrayIndex.point(i), 
                        NDArrayIndex.all()
                    },
                    gradForPosition
                );
            }
        }
    
        // Calculer le nombre total de tokens
        int totalTokens = targetTokenIdsBatch.stream().mapToInt(List::size).sum();
    
        // Moyenne de la perte sur tous les tokens
        loss /= totalTokens;
    
        // Moyenne des gradients sur tous les tokens
        gradients.divi(totalTokens);
    
        return Pair.of(loss, gradients); // Moyenne de la perte et gradients accumulés
    }
    
    
    
    
    
    public void saveState(String filePath) throws IOException {
        try (ObjectOutputStream oos = new ObjectOutputStream(new FileOutputStream(filePath))) {
            
            this.writeObject(oos);
            
            // Sauvegarder l'état de l'encodeur et du décodeur
            oos.writeObject(encoder);
            oos.writeObject(decoder);
            
            // Sauvegarder l'état de l'optimiseur
            oos.writeObject(optimizer.getCurrentStep());
            oos.writeObject(optimizer.getEpoch());
            oos.writeObject(optimizer.getLearningRate());
            
            // Sauvegarder les paramètres du modèle
            oos.writeObject(combinedParameters.size());
            for (INDArray param : combinedParameters) {
                oos.writeObject(param);
            }
            
            // Sauvegarder l'état d'entraînement
            oos.writeBoolean(isTrained);
        }
    }

    public void loadState(String filePath) throws IOException, ClassNotFoundException {
        try (ObjectInputStream ois = new ObjectInputStream(new FileInputStream(filePath))) {
        	
        	this.readObject(ois);
        	
            // Charger l'état de l'encodeur et du décodeur
            this.encoder = (Encoder) ois.readObject();
            this.decoder = (Decoder) ois.readObject();
            
            // Charger l'état de l'optimiseur
            int currentStep = (int) ois.readObject();
            int epoch = (int) ois.readObject();
            float learningRate = (float) ois.readObject();
            optimizer.setCurrentStep(currentStep);
            optimizer.setEpoch(epoch);
            optimizer.setLearningRate(learningRate);
            
            // Charger les paramètres du modèle
            int numParams = (int) ois.readObject();
            for (int i = 0; i < numParams; i++) {
                INDArray param = (INDArray) ois.readObject();
                combinedParameters.get(i).assign(param);
            }
            
            // Charger l'état d'entraînement
            this.isTrained = ois.readBoolean();
        }
    }

    
    private void writeObject(ObjectOutputStream oos) throws IOException {
        // oos.defaultWriteObject();
        // Vous pouvez sauvegarder le chemin du fichier Word2Vec si nécessaire
        oos.writeObject(W2VECPATH);
    }

    private void readObject(ObjectInputStream ois) throws IOException, ClassNotFoundException {
        // ois.defaultReadObject();
        String word2vecPath = (String) ois.readObject();
        // Réinitialiser wordVectors
        this.wordVectors = WordVectorSerializer.loadStaticModel(new File(word2vecPath));
    }



	public void setTrained(boolean isTrained) {
		this.isTrained = isTrained;
	}



	public int getDModel() {
		return dModel;
	}



	public static int getVocabSize() {
		return pretrainedEmbeddings.rows();
	}


    /**
     * Initialise les embeddings pré-entraînés.
     *
     * @param vocabSize Taille du vocabulaire
     * @param dModel    Dimension du modèle
     */
    public static void initializeEmbeddings(int vocabSize, int dModel) {
        // Exemple : initialisation aléatoire des embeddings avec normalisation
        pretrainedEmbeddings = Nd4j.randn(DataType.FLOAT, vocabSize, dModel).divi(Math.sqrt(dModel));
    }

    /**
     * Récupère les embeddings pré-entraînés.
     *
     * @return INDArray contenant les embeddings [vocabSize, dModel]
     */
    public static INDArray getPretrainedEmbeddings() {
        if (pretrainedEmbeddings == null) {
            throw new IllegalStateException("Les embeddings pré-entraînés ne sont pas initialisés. Appelez initializeEmbeddings() d'abord.");
        }
        return pretrainedEmbeddings;
    }


	public List<INDArray> getCombinedParameters() {
		return combinedParameters;
	}


	public List<INDArray> getCombinedGradients() {
		return combinedGradients;
	}







}
